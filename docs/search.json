[
  {
    "objectID": "workshops/ws08.html",
    "href": "workshops/ws08.html",
    "title": "Chapter 8 Workshop",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "workshops/ws08.html#load-and-look-at-the-data",
    "href": "workshops/ws08.html#load-and-look-at-the-data",
    "title": "Chapter 8 Workshop",
    "section": "Load and look at the data",
    "text": "Load and look at the data\nLoad and examine the dataset.\n\n\nCode\ndata(InsectSprays)\nstr(InsectSprays)\n\n\n'data.frame':   72 obs. of  2 variables:\n $ count: num  10 7 20 14 14 12 10 23 17 20 ...\n $ spray: Factor w/ 6 levels \"A\",\"B\",\"C\",\"D\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nCode\nhead(InsectSprays)\n\n\n  count spray\n1    10     A\n2     7     A\n3    20     A\n4    14     A\n5    14     A\n6    12     A\n\n\nYou can read a bit about it here: help(InsectSprays).\nThere are two variables. count is the response variable and represents the number of insects found in a plot, and spray is the predictor variable which indicates the type of insecticide used. Modelling a quantitative response variable with a categorical factor should make you immediately think of ANOVA!\nLet’s give the dataset a shorter name, so we don’t have to write out InsectSprays whenever we want to use it.\n\n\nCode\nIS = InsectSprays\n\n\nCheck the number of replicates (data points) for each group.\n\n\nCode\ntable(IS$spray)\n\n\n\n A  B  C  D  E  F \n12 12 12 12 12 12 \n\n\nThis shows that there are 12 replicates in each group. Because n is the same for each level, we can say that this is a balanced design."
  },
  {
    "objectID": "workshops/ws08.html#plotting-the-data",
    "href": "workshops/ws08.html#plotting-the-data",
    "title": "Chapter 8 Workshop",
    "section": "Plotting the data",
    "text": "Plotting the data\nLet’s plot the data using a boxplot.\n\n\nCode\nIS |&gt; \n  ggplot() +\n  aes(x = count, y = spray) + \n  geom_boxplot()\n\n\n\n\n\nWhat are your first impressions? Does it look like there is much difference among the groups?\nBoxplots are useful, but for small datasets it can be revealing to plot the actual data.\nWe can use a dotplot with violins.\n\n\nCode\ndvp &lt;- IS |&gt; \n  ggplot() +\n  aes(x = count, y = spray) + \n  geom_violin() +\n  geom_jitter(width = 0, height = 0.1, alpha = .5)\n\ndvp\n\n\n\n\n\nDo you think the dot/violinplot is more or less informative than the boxplot?\nAnalysis of variance compares the variation among groups (group means from the overall mean) with the variation within groups (data points from their group means). Recall that, while useful, boxplots do not actually show us the means. So, to help with our visual impression of these data in the context of ANOVA, we’ll now add the means to the dotplot.\n\n\nCode\nmeanIS &lt;- IS |&gt; group_by(spray) |&gt; summarise(count = mean(count))\n\ndvp + \n  geom_point(\n    data = meanIS,\n    colour = \"darkorange\" ,\n    shape = \"|\",\n    size = 8\n    )\n\n\n\n\n\nThough I do like boxplots, I think this is probably a more useful and revealing plot in this case, especially since we’re about to do an ANOVA, which tests for differences among means."
  },
  {
    "objectID": "workshops/ws08.html#fitting-an-anova-model",
    "href": "workshops/ws08.html#fitting-an-anova-model",
    "title": "Chapter 8 Workshop",
    "section": "Fitting an ANOVA model",
    "text": "Fitting an ANOVA model\nAn ANOVA model is fit in R using the function aov(). There is a function anova(), but this instead can be used to produce an ANOVA table from an existing model.\nLet’s do it.\n\n\nCode\naov(count ~ spray, data = IS)\n\n\nCall:\n   aov(formula = count ~ spray, data = IS)\n\nTerms:\n                   spray Residuals\nSum of Squares  2668.833  1015.167\nDeg. of Freedom        5        66\n\nResidual standard error: 3.921902\nEstimated effects may be unbalanced\n\n\nThis shows the among-groups (spray) and within-groups (Residuals) sum of squares, and degrees of freedom. The Residual standard error is the square root of the error variance (\\(\\sigma_\\varepsilon^2\\)). It is the expected (average) absolute deviation from the individual data points to their respective group means. If required, you can remind yourself what these values represent from the lecture slides.\nWe can get the full ANOVA table by commanding a summary() of an aov object. (Note, a very similar output can be achieved by applying the anova() function to an aov object.)\n\n\nCode\naov(count ~ spray, data=IS) |&gt; summary()\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspray        5   2669   533.8    34.7 &lt;2e-16 ***\nResiduals   66   1015    15.4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe full ANOVA table also gives us the Mean Squares (the SS divided by their respective df), the F value (the ratio of the within-to-among-groups Mean Squares), and the P-value.\nThe null hypothesis tested here is that there is no difference between the group means in the population.\n\\[H_0 : \\mu_1 = \\mu_2 = ... = \\mu_g\\]\nThe alternative hypothesis is that at least one of the groups is different to the others.\nRemember that we assume that we have taken a sample of data at random from a population. The P-value is the proportion of equivalent samples in which we would obtain the observed test statistic or greater if the null hypothesis is true (i.e. no difference among groups). In other words, how likely is it that the observed differences among means have occurred just through chance? Here, the F ratio is high and P-value is tiny (&lt; 0.0000000000000002). So, there is extremely strong evidence against the null hypothesis and we can conclude that there is indeed a difference among the group means—plants subjected to different types of spray have, on average, different numbers of insects on them.\nThis, in itself, is not immensely informative. Naturally, we wish to examine which groups (spray types) are different from one another.\nBut, first, we need to examine our assumptions."
  },
  {
    "objectID": "workshops/ws08.html#examining-the-assumptions-of-anova",
    "href": "workshops/ws08.html#examining-the-assumptions-of-anova",
    "title": "Chapter 8 Workshop",
    "section": "Examining the assumptions of ANOVA",
    "text": "Examining the assumptions of ANOVA\nLike for regression, diagnostic plots are the first step for examining how well the assumptions of ANOVA are met by our model. If these assumptions are not met, then the F-ratio might not behave as it should (i.e. follow an F-distribution) with repeated sampling under a true null hypothesis.\nThose assumptions are:\n\nAppropriateness of an additive linear model\nIndependent errors\nNormally distributed errors\nHomogeneity of errors\n\nWe will focus on 3 and 4 because 1 and 2 cannot really be checked at this stage of the analysis.\nLet’s make the plots.\n\n\nCode\nplot(aov(count ~ spray, data=IS))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is cause for concern here. As is often the case with counts, there seems to be some heteroscedasticity in the residuals. That is, the residual variance does not appear constant, but rather increases with the fitted value. In an ANOVA context, this means that the groups with high means have greater within-group variation. Let’s have a look at the standard deviations of the groups.\n\n\nCode\nIS |&gt; group_by(spray) |&gt; summarise(sd = sd(count))\n\n\n# A tibble: 6 × 2\n  spray    sd\n  &lt;fct&gt; &lt;dbl&gt;\n1 A      4.72\n2 B      4.27\n3 C      1.98\n4 D      2.50\n5 E      1.73\n6 F      6.21\n\n\nIndeed, there is a huge difference between the largest and smallest standard deviation (roughly 3.5-fold).\nWe can use a Levene’s test to explicitly test for evidence of heterogeneous variances. The function leveneTest() in the car package does this. Remember, the null hypothesis is that the average absolute deviations are the same in all groups.\n\n\nCode\ncar::leveneTest(aov(count ~ spray, data=IS))\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value   Pr(&gt;F)   \ngroup  5  3.8214 0.004223 **\n      66                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe low P-value here indicates that the null hypothesis is rejected and our suspicions were correct—there are significant differences in the variations among groups.\nThe assumption of normality also seems suspect here, according to the fact that many data do not fall along the dotted line in the Q-Q plot. This is probably part of the same problem.\nWe can try transforming the response variable count and see if that helps."
  },
  {
    "objectID": "workshops/ws08.html#anova-for-transformed-counts",
    "href": "workshops/ws08.html#anova-for-transformed-counts",
    "title": "Chapter 8 Workshop",
    "section": "ANOVA for transformed counts",
    "text": "ANOVA for transformed counts\nWe will try a log(x+1) transformation. We add the constant 1 because there are some (two) zeros in the dataset.\n\n\nCode\ntable(IS$count)\n\n\n\n 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24 26 \n 2  6  4  8  4  7  3  3  1  3  3  2  4  4  2  2  4  1  2  2  1  1  1  2 \n\n\nLet’s test the assumption for the transformed data.\n\n\nCode\nIS |&gt; \n  mutate( \n    lcount  = log(count+1) \n    ) |&gt; \n  aov(formula = lcount ~ spray) |&gt; \n  car::leveneTest()\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  5  1.8821 0.1093\n      66               \n\n\nThe test is now non-significant, but 0.1 is still a fairly small P-value. Looking at the following boxplot, it seems like the log transformation might be too severe—the groups with smaller means now seem to have higher variance than those with larger means.\n\n\nCode\nIS |&gt; \n  mutate( \n    lcount  = log(count+1) \n    ) |&gt; \n  ggplot() + \n  aes(x = lcount, y = spray) |&gt; \n  geom_boxplot()\n\n\n\n\n\nNow try instead a square-root transformation.\n\n\nCode\nIS |&gt; \n  mutate( \n    sqrt_count  = sqrt(count) \n    ) |&gt; \n  aov(formula = sqrt_count ~ spray) |&gt; \n  car::leveneTest()\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  5  0.8836 0.4971\n      66               \n\n\n\n\nCode\nIS |&gt; \n  mutate( \n sqrt_count  = sqrt(count) \n    ) |&gt; \n  ggplot() + \n  aes(x = sqrt_count, y = spray) |&gt; \n  geom_boxplot()\n\n\n\n\n\nThis looks like a better transformation for this model."
  },
  {
    "objectID": "workshops/ws08.html#fitting-an-anova-to-the-transformed-data",
    "href": "workshops/ws08.html#fitting-an-anova-to-the-transformed-data",
    "title": "Chapter 8 Workshop",
    "section": "Fitting an ANOVA to the transformed data",
    "text": "Fitting an ANOVA to the transformed data\nFit the model.\n\n\nCode\nIS &lt;- IS |&gt; mutate( sqrt_count  = sqrt(count) )\n\naov(sqrt_count ~ spray, data=IS) |&gt; summary()\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspray        5  88.44  17.688    44.8 &lt;2e-16 ***\nResiduals   66  26.06   0.395                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow show up the diagnostic plots for this model.\n\n\nCode\naov(sqrt_count ~ spray, data=IS) |&gt; plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMuch better!\nThis clearly has not made a substantial impact on the result of the hypothesis test for a difference among group means, but we can be happier that the inference is correct.\nNow let’s look at the differences among groups."
  },
  {
    "objectID": "workshops/ws08.html#post-hoc-multiple-comparisons",
    "href": "workshops/ws08.html#post-hoc-multiple-comparisons",
    "title": "Chapter 8 Workshop",
    "section": "Post-hoc multiple comparisons",
    "text": "Post-hoc multiple comparisons\nTukey’s Honest Significant Differences (HSD) are a method of adjusting for the fact that, in a post-hoc analysis such as this, there are simultaneously hypothesis tests being done. Across all these tests, the probability of erroneously finding a significant difference (i.e. making a Type I error) is greater than the nominated alpha value (usually 0.05). The HSD method makes each individual test slightly more conservative so that the family-wise error rate across all tests is conserved.\nTukey’s HSD can be implemented in R as follows.\n\n\nCode\naov(sqrt_count ~ spray, data=IS) |&gt; TukeyHSD()\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = sqrt_count ~ spray, data = IS)\n\n$spray\n          diff        lwr        upr     p adj\nB-A  0.1159530 -0.6369601  0.8688661 0.9975245\nC-A -2.5158217 -3.2687349 -1.7629086 0.0000000\nD-A -1.5963245 -2.3492377 -0.8434114 0.0000006\nE-A -1.9512174 -2.7041305 -1.1983042 0.0000000\nF-A  0.2579388 -0.4949744  1.0108519 0.9144964\nC-B -2.6317747 -3.3846879 -1.8788616 0.0000000\nD-B -1.7122775 -2.4651907 -0.9593644 0.0000001\nE-B -2.0671704 -2.8200835 -1.3142572 0.0000000\nF-B  0.1419858 -0.6109274  0.8948989 0.9935788\nD-C  0.9194972  0.1665841  1.6724103 0.0080813\nE-C  0.5646043 -0.1883088  1.3175175 0.2512638\nF-C  2.7737605  2.0208474  3.5266736 0.0000000\nE-D -0.3548928 -1.1078060  0.3980203 0.7366389\nF-D  1.8542633  1.1013502  2.6071764 0.0000000\nF-E  2.2091561  1.4562430  2.9620693 0.0000000\n\n\nThis table above has each possible pairwise comparison of groups. The number of comparisons is \\(g(g – 1)/2 = 15\\) (where \\(g\\) = the number of groups = 6).\nFor each pairwise comparison, the table gives the estimated difference (diff), the lower (lwr) and upper (upr) confidence intervals and the p-value (p adj) which tests whether the difference is statistically significant. The confidence intervals and the p-value are adjusted for the fact that multiple simultaneous comparisons are being made, and so are more conservative than if they were done separately. Out of the 15 comparisons, 10 have very low p-values.\nHere are the intervals shown on a plot.\n\n\nCode\naov(sqrt_count ~ spray, data=IS) |&gt; \n  TukeyHSD() |&gt; \n  plot(las=1)\n\n\n\n\n\nThis plot provides a visual impression of the size and adjusted confidence intervals for each comparison. An interval that includes zero indicates that the difference between those groups is non-significant (i.e. the null hypothesis of no difference is retained). This was the case for B-A, F-A, F-B, E-C, and E-D. From the earlier plots, this makes sense. A-B-F seem to be quite similar, as do E-C-D."
  },
  {
    "objectID": "workshops/ws08.html#load-data",
    "href": "workshops/ws08.html#load-data",
    "title": "Chapter 8 Workshop",
    "section": "Load data",
    "text": "Load data\n\n\nCode\noys &lt;- read_csv(\"https://www.massey.ac.nz/~anhsmith/data/oystergrowth.csv\")\n\nstr(oys)\n\n\nspc_tbl_ [60 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ...1       : num [1:60] 1 2 3 4 5 6 7 8 9 10 ...\n $ Growth     : num [1:60] 5.93 5.48 5.65 6.28 6.31 ...\n $ Salinity   : chr [1:60] \"Low\" \"Low\" \"Low\" \"Low\" ...\n $ Temperature: chr [1:60] \"Low\" \"Low\" \"Low\" \"Low\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ...1 = col_double(),\n  ..   Growth = col_double(),\n  ..   Salinity = col_character(),\n  ..   Temperature = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nThe response variable is Growth.\nThere are two factors:\nFactor A: Temperature, a = 3 levels (High, Medium, and Low) Factor B: Salinity, b = 2 levels (High and Low),\n\n\nCode\ntable(oys$Salinity, oys$Temperature)\n\n\n      \n       High Low Med\n  High   10  10  10\n  Low    10  10  10\n\n\nThe table above shows a count of 10 data points per combination of Temperature and Salinity. The equal number of observations among cells shows the experiment has a crossed and balanced design."
  },
  {
    "objectID": "workshops/ws08.html#ordering-factors",
    "href": "workshops/ws08.html#ordering-factors",
    "title": "Chapter 8 Workshop",
    "section": "Ordering factors",
    "text": "Ordering factors\nThe Temperature variable is ordinal, having a natural order. R does not know this automatically.\n\n\nCode\nfactor(oys$Temperature) |&gt; levels()\n\n\n[1] \"High\" \"Low\"  \"Med\" \n\n\nWe can tell R the correct order of the factors by turning them into ordered factors.\n\n\nCode\noys &lt;- oys |&gt; \n  mutate(\n    Temperature = factor(\n      Temperature, \n      levels = c(\"Low\",\"Med\",\"High\")\n      ),\n    Salinity = factor(\n      Salinity, \n      levels = c(\"Low\",\"High\")\n      )\n    )\n\n\nWe will first examine separately the effects of Temperature and Salinity on Growth, first focusing on Temperature.\n\n\nCode\nggplot(oys) + \n  aes(x = Growth, y = Temperature) + \n  geom_boxplot()\n\n\n\n\n\nCode\nggplot(oys) + \n  aes(x = Growth, y = Temperature) +\n  geom_violin() + \n  geom_jitter(width=0, height=.1, alpha = .4)"
  },
  {
    "objectID": "workshops/ws08.html#one-way-anovas",
    "href": "workshops/ws08.html#one-way-anovas",
    "title": "Chapter 8 Workshop",
    "section": "One-way ANOVAs",
    "text": "One-way ANOVAs\nWhat are your first impressions of these data? Do you think that Temperature has an effect on the Growth of oysters here? Do you think an ANOVA is an appropriate analysis to use? Would you expect the assumptions of ANOVA to be upheld?\nTwo things stand out. Firstly, it does not look like Temperature has a strong effect Growth—the variation within the groups looks to be much larger than the variation among groups. The one-way ANOVA agrees.\n\n\nCode\naov(Growth ~ Temperature, data=oys) |&gt; summary()\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nTemperature  2   2.18   1.088   0.784  0.461\nResiduals   57  79.10   1.388               \n\n\nSecondly, the data seem to have different variances among the three groups. We can use a Levene’s test to confirm this.\n\n\nCode\ncar::leveneTest(y = oys$Growth, group = oys$Temperature)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value    Pr(&gt;F)    \ngroup  2   27.45 4.475e-09 ***\n      57                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow make the same plots for Salinity and Growth.\nThe same general result was obtained for Salinity as it was for Temperature—there was not apparent effect of Salinity on Growth, and the dispersions among groups are significantly different (although only marginally so)."
  },
  {
    "objectID": "workshops/ws08.html#interaction-plots",
    "href": "workshops/ws08.html#interaction-plots",
    "title": "Chapter 8 Workshop",
    "section": "Interaction plots",
    "text": "Interaction plots\nNow we will examine Growth simultaneously in terms of both Temperature and Salinity.\nLet’s make a plot.\n\n\nCode\ng_oys_st &lt;- oys |&gt; \n  ggplot() +\n  aes(x = Temperature, y = Growth, colour = Salinity) + \n  geom_point(\n    alpha = .4,\n    position = position_dodge(width = .3)\n    )\n\ng_oys_st\n\n\n\n\n\nLet’s add the means to the plot above. First, let’s calculate the means of each combination of Temperature and Salinity.\n\n\nCode\nmean_growth &lt;- oys |&gt; \n  group_by(Temperature, Salinity) |&gt; \n  summarise(Growth = mean(Growth))\n\nmean_growth\n\n\n# A tibble: 6 × 3\n# Groups:   Temperature [3]\n  Temperature Salinity Growth\n  &lt;fct&gt;       &lt;fct&gt;     &lt;dbl&gt;\n1 Low         Low        5.86\n2 Low         High       8.12\n3 Med         Low        7.10\n4 Med         High       7.36\n5 High        Low        8.87\n6 High        High       6.04\n\n\nNow add the means as lines.\n\n\nCode\ng_oys_st + \n  geom_line(\n    data = mean_growth,\n    mapping = aes(group = Salinity),\n    position = position_dodge(width = .3)\n    )\n\n\n\n\n\nNote, we can make this plot without pre-calculating the means, by way of the stat_summary() function.\n\n\nCode\ng_oys_st + \n  stat_summary(\n    aes(group = Salinity), \n    fun = mean, \n    geom=\"line\",\n    position = position_dodge(width = .3)\n    )\n\n\n\n\n\nSo, our one-group analysis was unable to tell the real story in this dataset.\nBefore doing a formal two-way ANOVA, try to understand this graph.\nDo you expect to find a significant interaction?\nWhy do you think there was no discernable pattern in the one-factor analyses?\nWill differing dispersions among groups still be a problem?"
  },
  {
    "objectID": "workshops/ws08.html#two-way-anova",
    "href": "workshops/ws08.html#two-way-anova",
    "title": "Chapter 8 Workshop",
    "section": "Two-way ANOVA",
    "text": "Two-way ANOVA\nOK, let’s take a look at an ANOVA model with both factors included, plus the interaction. For this, you can write the full formula: Temperature + Salinity + Temperature:Salinity. In R, Temperature:Salinity is syntax for the interaction between Temperature and Salinity. Alternatively, this formula can be specified using this syntax: Temperature*Salinity. This is equivalent to writing the full formula explicitly. Both factors, and their interaction, are included.\n\n\nCode\naov(Growth ~ Temperature * Salinity, data = oys) |&gt; summary()\n\n\n                     Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTemperature           2   2.18    1.09   4.388 0.0171 *  \nSalinity              1   0.16    0.16   0.660 0.4200    \nTemperature:Salinity  2  65.54   32.77 132.148 &lt;2e-16 ***\nResiduals            54  13.39    0.25                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "workshops/ws08.html#interaction",
    "href": "workshops/ws08.html#interaction",
    "title": "Chapter 8 Workshop",
    "section": "Interaction",
    "text": "Interaction\nFirst, we look at the interaction Temperature:Salinity. It is highly significant and the most important source of explained variation in the model. This was abundantly clear when we plotted Growth according to both factors simultaneously. The mean lines on the plot were not parallel, which is indicative of an interaction.\nTemperature and Salinity have an effect on Growth but they are no independent. The effect of Temperature on Growth depends on Salinity, and the effect of Salinity on Growth depends on Temperature.\nWithout taking Salinity into account, Temperature has no apparent effect. For low Salinity environments, growth increases with increasing Temperature. For high Salinity, the opposite is true: Growth decreases with increasing Temperature.\nThere is an important lesson to be learned here. A variable that is important for explaining variation in the response, but not included in the model, can obfuscate the relationship between the response and the variables that you have included, particularly when interactions are present.\nSo-called ‘latent’ variables cause problems in science. This highlights the importance of knowing all the important causes of variation in a measured response. Two strategies may then be employed. You can hold a variable constant (e.g., examine temperature in medium salinity environments) but this restricts the inference space of your conclusions (they would only be valid for medium salinity). Alternatively, you can include a variable in the design (or measure it if it is not easily manipulated), ensuring that it is accounted for in any analyses.\nAnother potential consequence of an unknown latent variable is excess dispersion in some groups. This resulted in a significant departure from group homogeneity in the Levene’s test for both Salinity and Temperature when the other was left out.\nAnother thing to note is that, when the interaction is significant, we don’t need to worry about the main effects of the two factors."
  },
  {
    "objectID": "workshops/ws08.html#post-hoc-group-comparisons",
    "href": "workshops/ws08.html#post-hoc-group-comparisons",
    "title": "Chapter 8 Workshop",
    "section": "Post-hoc group comparisons",
    "text": "Post-hoc group comparisons\nThe next step in the analysis is to test which levels of the factors differ from each other, using a multiple comparisons procedure, such as Tukey’s Honest Significant Differences. Because the interaction is significant, it is the comparison within the interaction term that will be of most interest. There are six cells (3×2), so the number of comparisons within the interaction term will be six-choose-two:\n\n\nCode\nchoose(6,2)\n\n\n[1] 15\n\n\nRun a Tukey analysis.\n\n\nCode\naov(Growth~Salinity*Temperature, data=oys) |&gt; TukeyHSD()\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Growth ~ Salinity * Temperature, data = oys)\n\n$Salinity\n               diff        lwr       upr     p adj\nHigh-Low -0.1044926 -0.3622801 0.1532949 0.4199781\n\n$Temperature\n              diff         lwr       upr     p adj\nMed-Low  0.2424207 -0.13709813 0.6219396 0.2808614\nHigh-Low 0.4664160  0.08689709 0.8459349 0.0124178\nHigh-Med 0.2239952 -0.15552366 0.6035141 0.3366941\n\n$`Salinity:Temperature`\n                         diff         lwr         upr     p adj\nHigh:Low-Low:Low    2.2550543  1.59707046  2.91303820 0.0000000\nLow:Med-Low:Low     1.2409233  0.58293938  1.89890712 0.0000118\nHigh:Med-Low:Low    1.4989726  0.84098870  2.15695644 0.0000002\nLow:High-Low:Low    3.0072339  2.34925004  3.66521778 0.0000000\nHigh:High-Low:Low   0.1806524 -0.47733150  0.83863624 0.9642520\nLow:Med-High:Low   -1.0141311 -1.67211494 -0.35614720 0.0004194\nHigh:Med-High:Low  -0.7560818 -1.41406562 -0.09809788 0.0155079\nLow:High-High:Low   0.7521796  0.09419571  1.41016345 0.0162888\nHigh:High-High:Low -2.0744020 -2.73238583 -1.41641809 0.0000000\nHigh:Med-Low:Med    0.2580493 -0.39993455  0.91603319 0.8541174\nLow:High-Low:Med    1.7663107  1.10832678  2.42429452 0.0000000\nHigh:High-Low:Med  -1.0602709 -1.71825476 -0.40228702 0.0002074\nLow:High-High:Med   1.5082613  0.85027746  2.16624520 0.0000001\nHigh:High-High:Med -1.3183202 -1.97630408 -0.66033634 0.0000033\nHigh:High-Low:High -2.8265815 -3.48456541 -2.16859767 0.0000000\n\n\nFirst, in the single-factor comparisons (ignoring the other factor), only one comparison is significant—between high and low temperatures. The difference is only 0.46, however, and we know that this comparison does not represent the real effects of temperature on growth. It is marginalised over salinity, whereas we now know that the most important effects of temperature are, in fact, conditional on salinity.\nFor the interaction comparisons, 13 out of 15 are significantly different. You should refer to the interaction plot on the previous page (or on your screen) to make some sense of which of the comparisons are large or small. For example, the first line gives the comparison between the low-salinity-low-temperature group and the high-salinity-low-temperature group (i.e. the two groups to the left of the interaction plot above). It is one of the largest absolute differences and is highly significant. The greatest difference is on the ninth line of the interaction comparisons: low-salinity-high-temperature vs low-salinity-low-temperature (i.e., those at opposite ends of the low-salinity line on the interaction plot).\nWe can also plot the comparisons and corresponding Tukey intervals. The which argument allows you to restrict the output to particular terms (here, the interaction).\n\n\nCode\ntuk &lt;- aov(Growth~Salinity*Temperature, data=oys) |&gt; \n    TukeyHSD(which = \"Salinity:Temperature\")\n\npar(mar=c(5,10,3,1)) # adjust the margins so you can read the labels\nplot(tuk, las = 1)"
  },
  {
    "objectID": "workshops/ws08.html#load-the-data",
    "href": "workshops/ws08.html#load-the-data",
    "title": "Chapter 8 Workshop",
    "section": "Load the data",
    "text": "Load the data\n\n\nCode\ngoat &lt;- read_table(\"https://www.massey.ac.nz/~anhsmith/data/goats.txt\")\n\nstr(goat)\n\n\nspc_tbl_ [40 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Tmt      : chr [1:40] \"standard\" \"standard\" \"standard\" \"standard\" ...\n $ WtGain   : num [1:40] 5 3 8 7 6 4 8 6 7 5 ...\n $ InitialWt: num [1:40] 21 24 21 22 23 26 22 23 24 20 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Tmt = col_character(),\n  ..   WtGain = col_double(),\n  ..   InitialWt = col_double()\n  .. )\n\n\nThe initial weight is included because of commonly observed biological phenomenon known as “regression to the norm”, where lighter animals even the ledger by gaining more weight than the heavier animals. By using an ANCOVA model that incorporates the initial weight as a covariate can therefore give us more power to detect an effect of the treatment."
  },
  {
    "objectID": "workshops/ws08.html#fitting-treatment-only",
    "href": "workshops/ws08.html#fitting-treatment-only",
    "title": "Chapter 8 Workshop",
    "section": "Fitting treatment only",
    "text": "Fitting treatment only\nLet’s start with a boxplot of the response variable by reference to the predictor of interest, treatment.\n\n\nCode\ngoat |&gt; \n  ggplot() + \n  aes(x = WtGain, y = Tmt) + \n  geom_boxplot()\n\n\n\n\n\nThere seems to be a reasonable effect of treatment. This is apparent in an ANOVA fitted without the covariate.\n\n\nCode\naov(WtGain ~ Tmt, data = goat) |&gt; summary()\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nTmt          1   16.9  16.900    4.13 0.0492 *\nResiduals   38  155.5   4.092                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSure enough, effect of the treatment is significant, though it is marginally so. The effect size can be viewed in a linear model summary.\n\n\nCode\nlm(WtGain ~ Tmt, data=goat) |&gt; summary()\n\n\n\nCall:\nlm(formula = WtGain ~ Tmt, data = goat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -3.85  -1.55   0.15   1.15   4.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6.8500     0.4523  15.144   &lt;2e-16 ***\nTmtstandard  -1.3000     0.6397  -2.032   0.0492 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.023 on 38 degrees of freedom\nMultiple R-squared:  0.09803,   Adjusted R-squared:  0.07429 \nF-statistic:  4.13 on 1 and 38 DF,  p-value: 0.04916\n\n\nFrom here, we can see that the estimated difference between the treatments is 1.3 kg—the mean for intensive is 6.85 kg and the mean for standard is 5.55 kg.\n\n\nCode\ngoat |&gt; group_by(Tmt) |&gt; summarise(mean = mean(WtGain))\n\n\n# A tibble: 2 × 2\n  Tmt        mean\n  &lt;chr&gt;     &lt;dbl&gt;\n1 intensive  6.85\n2 standard   5.55\n\n\nThe fitted model explains only 10% of the variation in the data. The average difference between the fitted means and the observations is 2.023, which is greater than the estimated difference in means. So, all in all, there is a significant effect but it is not large. We can estimate a 95% confidence interval for this difference using \\(\\hat d + t_{df,[0.025,0.975]} \\times \\text{SE}(\\hat d)\\).\n\n\nCode\n-1.3 + qt(c(.025,.975), 38) * 0.6397\n\n\n[1] -2.595004947 -0.004995053\n\n\nSo, the confidence interval for the difference is {0.005 , 2.595}."
  },
  {
    "objectID": "workshops/ws08.html#fitting-initial-weight-only",
    "href": "workshops/ws08.html#fitting-initial-weight-only",
    "title": "Chapter 8 Workshop",
    "section": "Fitting Initial Weight only",
    "text": "Fitting Initial Weight only\nNow we will look at the effect of the covariate, InitialWt.\n\n\nCode\ngoat |&gt; \n  ggplot() + \n  aes(x = InitialWt, y = WtGain) +\n  geom_point(alpha = .4)\n\n\n\n\n\nWe can see that there is a relationship between the initial weight of the goats and the gain in weight, in that the goats that were lighter at the start of the experiment gained more weight.\nLook at a linear model using this predictor alone.\n\n\nCode\nlm(WtGain ~ InitialWt, data = goat) |&gt; summary()\n\n\n\nCall:\nlm(formula = WtGain ~ InitialWt, data = goat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.607 -1.206  0.163  1.054  3.871 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.39581    1.85047   7.780 2.22e-09 ***\nInitialWt   -0.35403    0.07906  -4.478 6.68e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.723 on 38 degrees of freedom\nMultiple R-squared:  0.3454,    Adjusted R-squared:  0.3282 \nF-statistic: 20.05 on 1 and 38 DF,  p-value: 6.681e-05\n\n\nFor every extra kilogram of weight at the start, the goat is expected to gain 354 g less over the course of the experiment. The effect is highly significant, and explains 35% of the variation in weight gain. We are not interested in this effect though, per se, because it has been demonstrated many times before. We are most interested in the effect of the treatment."
  },
  {
    "objectID": "workshops/ws08.html#fitting-both-anova",
    "href": "workshops/ws08.html#fitting-both-anova",
    "title": "Chapter 8 Workshop",
    "section": "Fitting both… ANOVA!",
    "text": "Fitting both… ANOVA!\nNow we will look at the effect of the Tmt in the context of the covariate, InitialWt.\n\n\nCode\nggoat &lt;- goat |&gt; \n  ggplot() + \n  aes(x = InitialWt, y = WtGain, colour = Tmt) +\n  geom_point(alpha = .4)\n\nggoat\n\n\n\n\n\nThis shows the effects of both predictors simultaneously. We can envisage two lines on this graph—one through the Intensive points and one through the Standard points. The difference in the heights of these lines is the effect of treatment, the slope of the lines is the effect of the initial weight, and any difference in the slopes of the lines is the interaction between the two predictors.\nNow fit a model including both these predictors simultaneously, including any interaction.\n\n\nCode\naov(WtGain ~ InitialWt * Tmt, data=goat) |&gt; summary()\n\n\n              Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nInitialWt      1  59.55   59.55  22.211 3.6e-05 ***\nTmt            1  16.00   16.00   5.966  0.0196 *  \nInitialWt:Tmt  1   0.34    0.34   0.128  0.7230    \nResiduals     36  96.51    2.68                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe first thing is to check for a significant interaction. There is not, so we can assume that there is no difference in the slope of the lines for Intensive and Standard treatments. Thus, there is no evidence that the effect of the initial weight varies among the two treatments, or that the treatment effect varies according to the initial weight.\nApplying the drop1() function supports that the more parsimonious model is that without the interaction.\n\n\nCode\naov(WtGain ~ InitialWt * Tmt, data = goat) |&gt; drop1()\n\n\nSingle term deletions\n\nModel:\nWtGain ~ InitialWt * Tmt\n              Df Sum of Sq    RSS    AIC\n&lt;none&gt;                     96.514 43.232\nInitialWt:Tmt  1   0.34225 96.857 41.374\n\n\nNow we can confidently fit the model with just the two main effects.\n\n\nCode\naov(WtGain ~ InitialWt + Tmt, data = goat) |&gt; summary()\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nInitialWt    1  59.55   59.55   22.75 2.87e-05 ***\nTmt          1  16.00   16.00    6.11   0.0182 *  \nResiduals   37  96.86    2.62                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI have put the treatment in the model after the initial weight, so we effectively obtain the Type II sum of squares for treatment, i.e., the variation that is attributable to treatment only, after all the variation attributable to initial weight has been accounted for. This provides a more conservative test for our effect of interest. It matters little in this case, however. Review the table for when treatment alone is fit.\n\n\nCode\naov(WtGain ~ Tmt, data = goat) |&gt; summary()\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nTmt          1   16.9  16.900    4.13 0.0492 *\nResiduals   38  155.5   4.092                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe SS for treatment has decreased only slightly from the inclusion of initial weight, from 16.9 to 16.0.\nMore importantly, though, the F-ratio has increased by 50% and the P-value has decreased from 0.05 to 0.02! Just from including another (potentially competing) predictor in the model! Take a moment to consider how this might have happened.\nThe key here is the decrease in the residual SS from 156 to 97. The F-ratio is calculated from the treatment SS divided by the residual SS. Therefore, if you decrease the residual SS, you can increase the value of F. By attributing variation to a covariate, it is no longer considered to be part of the random, unexplained variation. Our ability to detect an effect of the treatment is therefore strengthened.\nThis is also apparent in the linear model summary.\n\n\nCode\nlm(WtGain ~ InitialWt + Tmt, data=goat) |&gt; summary()\n\n\n\nCall:\nlm(formula = WtGain ~ InitialWt + Tmt, data = goat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9716 -1.2419 -0.0338  0.9878  3.2231 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.96661    1.75261   8.540 2.82e-10 ***\nInitialWt   -0.35137    0.07424  -4.733 3.21e-05 ***\nTmtstandard -1.26486    0.51169  -2.472   0.0182 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.618 on 37 degrees of freedom\nMultiple R-squared:  0.4382,    Adjusted R-squared:  0.4078 \nF-statistic: 14.43 on 2 and 37 DF,  p-value: 2.331e-05\n\n\nWhile the estimated effect of the treatment has reduced from a difference of 1.30 to 1.26, the standard error has reduced from 0.64 to 0.51. We can recalculate the confidence interval for the difference (we remove a degree of freedom for the fact that we are also estimating a regression slope).\n\n\nCode\n-1.26 + qt(c(.025,.975),37) * 0.51169\n\n\n[1] -2.2967824 -0.2232176\n\n\nThe interval for the difference {0.223 , 2.297} is quite a bit smaller than the one obtained earlier when fitting treatment alone {0.005 , 2.595}, and no longer brushes so close to zero."
  },
  {
    "objectID": "workshops/ws08.html#plotting-the-model",
    "href": "workshops/ws08.html#plotting-the-model",
    "title": "Chapter 8 Workshop",
    "section": "Plotting the model",
    "text": "Plotting the model\nWe can get ggplot to fit two linear models (one for each treatment) on our scatterplot.\n\n\nCode\nggoat + stat_smooth(method=\"lm\")\n\n\n\n\n\nHowever, notice that the slopes are different. In our model, the slopes are the same, because it doesn’t have an interaction between InitialWt and Tmt. This is where the visreg package is very useful!\n\n\nCode\nlibrary(visreg)\n\nlm(WtGain ~ InitialWt + Tmt, data=goat) |&gt; \n  visreg(xvar = \"InitialWt\", by = \"Tmt\", overlay = TRUE, gg = TRUE)"
  },
  {
    "objectID": "workshops/ws08.html#exercise-8.1",
    "href": "workshops/ws08.html#exercise-8.1",
    "title": "Chapter 8 Workshop",
    "section": "Exercise 8.1",
    "text": "Exercise 8.1\nObtain the main effects and interaction plots.\nOld style Main effects plots:\n\n\nCode\nmod1 &lt;- aov(yield ~ shade + irrigation + inoculum, \n            data=alfalfa)\n\nlibrary(effects)\n\nplot(allEffects(mod1))\n\n\nOld style Interaction effects plots:\n\n\nCode\nmod2 &lt;- aov(\n  yield ~ shade*irrigation*inoculum - shade:irrigation:inoculum, \n  data = alfalfa\n  )\n\n\nlibrary(effects)\n\nmod1 &lt;- lm(yield ~ shade * irrigation, data = alfalfa) \n\neffect('shade:irrigation', \n       mod = mod1) |&gt; \n  plot(multiline = TRUE)\n\n\nCode\nmod2 &lt;- lm(yield ~ shade * inoculum, data = alfalfa) \n\neffect('shade:inoculum',\n       mod=mod2) |&gt; \n  plot(multiline=TRUE)\n\n\nCode\nmod3 &lt;- lm(yield ~ irrigation * inoculum, data = alfalfa) \n\neffect('irrigation:inoculum',\n       mod=mod3) |&gt; \n  plot(multiline=TRUE)\n\n\nggplot2 can produce good main effects and interaction plots but the R codes for this task are not short. For main effects plot-\n\n\nCode\nlibrary(ggplot2)\n\nplot1 &lt;- ggplot(alfalfa) + \n  aes(x = shade, y = yield) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = 1)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = 1)) + \n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"Main effect of shade\")\n\nplot2 &lt;- ggplot(alfalfa) +\n  aes(x = irrigation, y = yield) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = 1)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = 1)) +\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"Main effect of irrigation\")\n\nplot3 &lt;- ggplot(alfalfa) +\n  aes(x = inoculum, y = yield) +\n  stat_summary(fun = mean, geom = \"point\", aes(group = 1)) +\n  stat_summary(fun = mean, geom = \"line\", aes(group = 1)) + \n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"Main effect of inoculum\")\n\nlibrary(patchwork)\nplot1+plot2+plot3\n\n\nFor interaction plot-\n\n\nCode\n#Interactions Plot\nplot4 &lt;- ggplot(alfalfa) +\n  aes(x = shade, y = yield, \n      group = irrigation, colour = irrigation) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"shade*irrigation interaction\")\n\nplot5 &lt;- ggplot(alfalfa) +\n  aes(x = inoculum, y = yield,\n      group = irrigation, colour = irrigation) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"inoculum*irrigation interaction\")\n\nplot6 &lt;- ggplot(alfalfa) +\n  aes(x = shade, y = yield, \n      group = inoculum, colour = inoculum) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_hline(aes(yintercept = mean(yield)), alpha = .7) + \n  ggtitle(\"shade*inoculum interaction\")\n\nplot4 / plot5 / plot6"
  },
  {
    "objectID": "workshops/ws08.html#exercise-8.2",
    "href": "workshops/ws08.html#exercise-8.2",
    "title": "Chapter 8 Workshop",
    "section": "Exercise 8.2",
    "text": "Exercise 8.2\nFit one-way ANOVA models to this dataset.\n\n\nCode\nanova1 &lt;- aov(yield ~ shade, data = alfalfa)\nsummary(anova1)\n\nanova2 &lt;- aov(yield ~ irrigation, data = alfalfa)\nsummary(anova2)\n\nanova3 &lt;- aov(yield ~ inoculum, data = alfalfa)\nsummary(anova3)\n\nplot(TukeyHSD(anova1))\n\n\nCode\nplot(TukeyHSD(anova2))\n\n\nCode\nplot(TukeyHSD(anova3))"
  },
  {
    "objectID": "workshops/ws08.html#exercise-8.3",
    "href": "workshops/ws08.html#exercise-8.3",
    "title": "Chapter 8 Workshop",
    "section": "Exercise 8.3",
    "text": "Exercise 8.3\nFit a three-factor (additive) ANOVA model without interactions.\n\n\nCode\nanova1 &lt;- aov(yield ~ shade + irrigation + inoculum,\n              data = alfalfa)\n\nsummary(anova1)\n\nlibrary(ggfortify)\nautoplot(anova1, 1)\n\n\nFor the base R style four diagnostic plots, use plot(anova1) and set the par.\n\n\nCode\npar(mfrow=c(2,2))\nplot(anova1)"
  },
  {
    "objectID": "workshops/ws08.html#exercise-8.4",
    "href": "workshops/ws08.html#exercise-8.4",
    "title": "Chapter 8 Workshop",
    "section": "Exercise 8.4",
    "text": "Exercise 8.4\nFit the indicator variable regression model of yield ~ inoculum\n\n\nCode\nalfalfa &lt;- alfalfa |&gt; \n  mutate(\n    I.A = as.numeric(inoculum==\"A\"),\n    I.B = as.numeric(inoculum==\"B\"),\n    I.C = as.numeric(inoculum==\"C\"),\n    I.D = as.numeric(inoculum==\"D\")\n  )\n\nindicator.reg &lt;- lm(yield ~ I.A + I.B + I.C + I.D, \n                    data = alfalfa)\n\nsummary(indicator.reg)\n\n\nNote that this regression allows the treatments to be compared with the control."
  },
  {
    "objectID": "workshops/ws08.html#exercise-8.5",
    "href": "workshops/ws08.html#exercise-8.5",
    "title": "Chapter 8 Workshop",
    "section": "Exercise 8.5",
    "text": "Exercise 8.5\nshade is a categorical variable of factor codes but let us (incorrectly) treat it as numerical (and if the actual distances are given, then the distance variable becomes a covariate). Fit ANCOVA of yield on the inoculum factor and shade covariate.\nR:\n\n\nCode\nancova.model &lt;- lm(yield ~ as.numeric(shade) * inoculum, \n                   data = alfalfa)\nsummary(ancova.model)"
  },
  {
    "objectID": "workshops/ws08.html#exercise-8.6",
    "href": "workshops/ws08.html#exercise-8.6",
    "title": "Chapter 8 Workshop",
    "section": "Exercise 8.6",
    "text": "Exercise 8.6\nIn a two-factor experiment, one of the factors was assigned to main plot (main-plot factor), the second factor, called the subplot factor, was assigned into subplots. The dataset https://www.massey.ac.nz/~kgovinda/data/plots.RData gives the experimental set up. Perform the ANOVA for this basic split-plot experiment.\nR:\n\n\nCode\nurl1 &lt;- \"https://www.massey.ac.nz/~anhsmith/data/plots.RData\"\ndownload.file(url = url1, destfile = \"plots.RData\")\nload(\"plots.RData\")\n\nplots\n\n\n\n\nCode\nsp.model &lt;- aov(yield ~ block + A*B + Error(block/A),\n                data=plots)\nsummary(sp.model)\n\n\n\n\nCode\n#Incorrect model\nsummary(aov(yield ~ block + A*B ,\n            data=plots))\n\n\n\nMore R code examples are here"
  },
  {
    "objectID": "workshops/ws08.html#footnotes",
    "href": "workshops/ws08.html#footnotes",
    "title": "Chapter 8 Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nData originally sourced from Beall, G. 1942. The transformation of data from entomological field experiments. Biometrika 32:243.↩︎\nData originally sourced from Beall, G. 1942. The transformation of data from entomological field experiments. Biometrika 32:243.↩︎\nData originally sourced from Beall, G. 1942. The transformation of data from entomological field experiments. Biometrika 32:243.↩︎"
  },
  {
    "objectID": "workshops/ws06.html",
    "href": "workshops/ws06.html",
    "title": "Chapter 6 Workshop",
    "section": "",
    "text": "Code\nlibrary(tidyverse)"
  },
  {
    "objectID": "workshops/ws06.html#load-data",
    "href": "workshops/ws06.html#load-data",
    "title": "Chapter 6 Workshop",
    "section": "Load data",
    "text": "Load data\nLet’s load the data and create a new variable, which is the temperature in Celsius.\n\n\nCode\nch &lt;- read_csv(\"https://www.massey.ac.nz/~anhsmith/data/chirps.csv\") |&gt; \n  mutate(degC = (degF-32)*5/9)\n\n\nWe will examine how well the temperature predicts the frequency of chirping by this insect."
  },
  {
    "objectID": "workshops/ws06.html#plot",
    "href": "workshops/ws06.html#plot",
    "title": "Chapter 6 Workshop",
    "section": "Plot",
    "text": "Plot\nPlot the data, with temperature on the x-axis and chirps on the y-axis.\n\n\nCode\nch |&gt; \n  ggplot() + \n  aes(degC, chirps) +\n  geom_point()\n\n\n\n\n\nAdd a ‘smoother’ line.\n\n\nCode\nch |&gt; \n  ggplot() + \n  aes(degC, chirps) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\nWell… it’s sort-of linear… ish… not really.\n\n\nCode\nch |&gt; \n  ggplot() + \n  aes(degC, chirps) +\n  geom_point() +\n  geom_smooth(method = 'lm')"
  },
  {
    "objectID": "workshops/ws06.html#linear-model",
    "href": "workshops/ws06.html#linear-model",
    "title": "Chapter 6 Workshop",
    "section": "Linear model",
    "text": "Linear model\nNow let’s fit a linear model and print it. (Just typing an object’s name is the same as applying the function print.)\n\nFitting linear model\n\n\nCode\nchm &lt;- lm(chirps ~ degC, data = ch)\n\nchm\n\n\n\nCall:\nlm(formula = chirps ~ degC, data = ch)\n\nCoefficients:\n(Intercept)         degC  \n     6.4725       0.3815  \n\n\nCode\n# or print(chm)\n\n\nThis object contains a lot of information, but just printing it doesn’t show us all of it. Each object in R has a ‘class’, which you can reveal like so.\n\n\nClass\n\n\nCode\nclass(chm)\n\n\n[1] \"lm\"\n\n\nThe model object created by the function lm() has class lm. If you unclass() this object, you will see all the information it contains.\n\n\nCode\nunclass(chm)\n\n\n$coefficients\n(Intercept)        degC \n   6.472457    0.381465 \n\n$residuals\n           1            2            3            4            5            6 \n 1.532589142  1.135313774  0.336540013  0.843865198  0.327989348 -0.127615826 \n           7            8            9           10           11           12 \n 0.237971364  0.031294055  1.001547697 -1.144208684 -1.560086053  0.004139772 \n          13           14           15 \n-0.772011032 -0.386593793 -1.460734975 \n\n$effects\n(Intercept)        degC                                                 \n-64.4980827   5.3185830   0.2895646   0.5875785  -0.0143477  -0.5955397 \n                                                                        \n -0.3578650  -0.2784834   0.3987344  -1.4237522  -1.9256799  -0.2916837 \n                                    \n -1.1143481  -0.6614861  -1.9030762 \n\n$rank\n[1] 2\n\n$fitted.values\n       1        2        3        4        5        6        7        8 \n18.46741 14.86469 19.46346 17.55613 16.77201 15.62762 14.46203 17.06871 \n       9       10       11       12       13       14       15 \n14.39845 17.34421 16.56009 17.19586 16.77201 17.38659 15.86073 \n\n$assign\n[1] 0 1\n\n$qr\n$qr\n   (Intercept)          degC\n1   -3.8729833 -1.033656e+02\n2    0.2581989  1.394252e+01\n3    0.2581989 -4.583652e-01\n4    0.2581989 -9.974999e-02\n5    0.2581989  4.768089e-02\n6    0.2581989  2.628501e-01\n7    0.2581989  4.820038e-01\n8    0.2581989 -8.103759e-03\n9    0.2581989  4.939575e-01\n10   0.2581989 -5.990386e-02\n11   0.2581989  8.752703e-02\n12   0.2581989 -3.201138e-02\n13   0.2581989  4.768089e-02\n14   0.2581989 -6.787296e-02\n15   0.2581989  2.190191e-01\nattr(,\"assign\")\n[1] 0 1\n\n$qraux\n[1] 1.258199 1.406296\n\n$pivot\n[1] 1 2\n\n$tol\n[1] 1e-07\n\n$rank\n[1] 2\n\nattr(,\"class\")\n[1] \"qr\"\n\n$df.residual\n[1] 13\n\n$xlevels\nnamed list()\n\n$call\nlm(formula = chirps ~ degC, data = ch)\n\n$terms\nchirps ~ degC\nattr(,\"variables\")\nlist(chirps, degC)\nattr(,\"factors\")\n       degC\nchirps    0\ndegC      1\nattr(,\"term.labels\")\n[1] \"degC\"\nattr(,\"order\")\n[1] 1\nattr(,\"intercept\")\n[1] 1\nattr(,\"response\")\n[1] 1\nattr(,\".Environment\")\n&lt;environment: R_GlobalEnv&gt;\nattr(,\"predvars\")\nlist(chirps, degC)\nattr(,\"dataClasses\")\n   chirps      degC \n\"numeric\" \"numeric\" \n\n$model\n   chirps     degC\n1    20.0 31.44444\n2    16.0 22.00000\n3    19.8 34.05556\n4    18.4 29.05556\n5    17.1 27.00000\n6    15.5 24.00000\n7    14.7 20.94444\n8    17.1 27.77778\n9    15.4 20.77778\n10   16.2 28.50000\n11   15.0 26.44444\n12   17.2 28.11111\n13   16.0 27.00000\n14   17.0 28.61111\n15   14.4 24.61111\n\n\n\n\nAttributes\nBig compound objects such as a lm are often organised into sections called “attributes”, which you can view with this function.\n\n\nCode\nattributes(chm)\n\n\n$names\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n$class\n[1] \"lm\"\n\n\nThe attributes under the section “names” can be accessed using the dollar ($) notation, and often with specific formulae. You can read what each of these represent in the help file of lm (?lm). Many of them aren’t that useful, but the following ones certainly are.\n\n\nCode\nchm$coefficients\n\n\n(Intercept)        degC \n   6.472457    0.381465 \n\n\nCode\n# or \n\n# coef(chm)\n\n\nThis shows us \\(a\\) and \\(b\\), the sample estimates of the population parameters, \\(\\alpha\\) and \\(\\beta\\). The model we have fit to this dataset is thus \\(Y = 6.47 + 0.38X\\).\nWe can also extract the fitted values and residuals for the model.\n\n\nCode\nchm$fitted.values\n\n\n       1        2        3        4        5        6        7        8 \n18.46741 14.86469 19.46346 17.55613 16.77201 15.62762 14.46203 17.06871 \n       9       10       11       12       13       14       15 \n14.39845 17.34421 16.56009 17.19586 16.77201 17.38659 15.86073 \n\n\nCode\n# or\n\n# fitted(chm)\n\n\n\n\nCode\nresid(chm)\n\n\n           1            2            3            4            5            6 \n 1.532589142  1.135313774  0.336540013  0.843865198  0.327989348 -0.127615826 \n           7            8            9           10           11           12 \n 0.237971364  0.031294055  1.001547697 -1.144208684 -1.560086053  0.004139772 \n          13           14           15 \n-0.772011032 -0.386593793 -1.460734975 \n\n\n\n\nSummary of a linear model\nThe function summary() shows us a useful display of the most important information from the model.\n\n\nCode\nsummary(chm)\n\n\n\nCall:\nlm(formula = chirps ~ degC, data = ch)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.56009 -0.57930  0.03129  0.59020  1.53259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.47246    1.87653   3.449 0.004315 ** \ndegC         0.38146    0.06968   5.475 0.000107 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9715 on 13 degrees of freedom\nMultiple R-squared:  0.6975,    Adjusted R-squared:  0.6742 \nF-statistic: 29.97 on 1 and 13 DF,  p-value: 0.0001067\n\n\nThis presents a summary of the residuals; the estimates, standard errors, and t-tests for the coefficients \\(\\alpha\\) and \\(\\beta\\); the residual standard error; the coefficient of determination; and the F-test. We can see that the p-values for the F-test and the t-test for the regression coefficient (\\(\\beta\\)) are the same: we can conclude that temperature has a highly significant effect on the frequency of chirps, accounting for around 70% of the variation. For every degree increase in temperature, the number of chirps per second is expected to increase by 0.38.\nWe cannot just stop there, however. We need to examine the model and check the assumptions.\nPlot the data again, this time with a linear regression line.\n\n\nCode\nch |&gt; \n  ggplot() + \n  aes(degC, chirps) +\n  geom_point() +\n  geom_abline(\n    intercept = 6.47246,\n    slope = 0.38146\n    )\n\n\n\n\n\nWe can also use the function predict() to extract the fitted values, with confidence or prediction intervals for each data point.\n\n\nCode\npredict(chm, interval=\"confidence\")\n\n\n        fit      lwr      upr\n1  18.46741 17.56955 19.36527\n2  14.86469 13.97481 15.75457\n3  19.46346 18.22919 20.69773\n4  17.55613 16.90760 18.20467\n5  16.77201 16.22807 17.31595\n6  15.62762 14.95122 16.30401\n7  14.46203 13.44151 15.48254\n8  17.06871 16.50254 17.63487\n9  14.39845 13.35659 15.44031\n10 17.34421 16.73758 17.95084\n11 16.56009 16.01692 17.10325\n12 17.19586 16.61319 17.77854\n13 16.77201 16.22807 17.31595\n14 17.38659 16.77226 18.00093\n15 15.86073 15.23503 16.48644\n\n\nThis gives the predictions for each of the original data points, along with and interval in which the mean prediction lies with 95% confidence (i.e., if we took many many samples and fit a regression model to each, 95% of so-constructed confidence intervals will include the true mean of \\(Y\\) for this value of \\(X\\)).\nThis is not to be confused with a “prediction” interval, which is expected to contain 95% of the actual values of \\(Y\\) for this value of \\(X\\), rather than the mean value. They are thus wider than confidence intervals.\n\n\nCode\npredict(chm, interval=\"prediction\")\n\n\n        fit      lwr      upr\n1  18.46741 16.18459 20.75023\n2  14.86469 12.58499 17.14438\n3  19.46346 17.02860 21.89832\n4  17.55613 15.35938 19.75289\n5  16.77201 14.60384 18.94019\n6  15.62762 13.42248 17.83275\n7  14.46203 12.12824 16.79582\n8  17.06871 14.89485 19.24256\n9  14.39845 12.05525 16.74165\n10 17.34421 15.15946 19.52896\n11 16.56009 14.39210 18.72807\n12 17.19586 15.01765 19.37408\n13 16.77201 14.60384 18.94019\n14 17.38659 15.19970 19.57349\n15 15.86073 13.67062 18.05085\n\n\n\n\n\n\n\n\nThe distinction between confidence and prediction intervals\n\n\n\n\nA 95% confidence interval refers to a mean. It is an interval in which the mean of Y, for a given value of X, is expected to lie with 95% confidence.\nA 95% prediction interval is an interval in which 95% of Y values are expected to lie for a given value of X. Prediction intervals are broader than confidence intervals.\n\n\n\n\n\nPredicting for new data\nYou can also use the model to make predictions for new values of \\(X\\). To do this, first create a data frame object with a column of the same name as the \\(X\\) variable used in the model (i.e., degC), and then enter this object as the newdata argument for the predict function.\n\n\nCode\nnewdat &lt;- data.frame(degC = 20:34)\nprednew &lt;- predict(chm, newdata = newdat, interval = \"confidence\")\nprednew\n\n\n        fit      lwr      upr\n1  14.10176 12.95828 15.24524\n2  14.48322 13.46978 15.49666\n3  14.86469 13.97481 15.75457\n4  15.24615 14.47024 16.02206\n5  15.62762 14.95122 16.30401\n6  16.00908 15.41049 16.60767\n7  16.39055 15.83880 16.94230\n8  16.77201 16.22807 17.31595\n9  17.15348 16.57674 17.73022\n10 17.53494 16.89096 18.17892\n11 17.91641 17.18012 18.65269\n12 18.29787 17.45239 19.14335\n13 18.67934 17.71348 19.64520\n14 19.06080 17.96707 20.15454\n15 19.44227 18.21550 20.66903\n\n\nThese correspond to the predicted means and confidence intervals for \\(Y\\), for \\(X\\) values of 20 to 34.\n\n\nPlotting confidence intervals\nWe can automatically plot confidence intervals for a linear fit like so:\n\n\nCode\nch |&gt; \n  ggplot() + \n  aes(degC, chirps) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\nOr with the visreg package.\n\n\nCode\nlibrary(visreg)\n\nvisreg(chm, gg=T)\n\n\n\n\n\nNote that the mean is most certain (i.e., the interval is tighter) around the centre of the data and less so around the extremes.\n\n\nResidual plots\nThe linear regression model looks like it provides a reasonable fit to the data. We should just check the residuals though.\n\n\nCode\nggplot() + \n  aes(chm$fitted.values, chm$residuals) +\n  geom_point() +\n  geom_hline(yintercept = 0)\n\n\n\n\n\nRemember, you want a complete mess in your residual plot—no pattern is good pattern. We generally look for two things: heteroscedasticity and trend. We can see in the plot above that the residuals appear to have quite a constant variance, so no worries about the heteroscedasticity there. However, there is a hint of a trend—there are more points above zero at low and high fitted values, and more below the line at middle fitted values. This is not too severe, but it warrants further consideration.\nUsing the function plot() on an lm object gives four very useful diagnostic graphs. You can read about them in the help file by entering the following.\n\n\nCode\nplot(chm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting R’s diagnostic plots\n\n\n\nThe diagnostic plots are designed to inform the user of any departure from the assumptions of a linear model.\nThe Residuals vs Fitted plot shows the regular residuals, i.e., the difference between the observed and fitted values on the y-axis plotted against the fitted values on the x-axis. Ideally, this would show a complete trend-less mess. Here, there is a little curvature on the plot, which may concern us a little.\nThe Normal Q-Q plot compares the residuals with their expected values if they were normally distributed. When lots of points lie away from this line, then there is evidence that the residuals are non-normally distributed, which violates an assumption of linear models. Here, they look pretty good. A little departure from the line at the very ends is common, and no cause for great concern.\nThe Scale-Location plot shows the square-root of the standardised residuals (i.e. divided by their standard deviation) against the predicted values. This can be used to look for heteroscedasticity—changes in the variance of the residuals for different fitted values—which is also an assumption of linear models. Ideally, there would be no trend in these points and the red line would be perfectly horizontal. It doesn’t look bad here.\nFinally, the Residuals vs Leverage plot shows the standardised residuals against Leverage. The leverage measures the potential influence a point has given the extremeness of its values for the predictor variable. If a point has large leverage and a large residual, then it will have large influence, meaning that it is having a large effect on the estimated parameters. So, this plot is useful for identifying outliers and influential points. Any points that are outside the red dashed lines have high values of Cook’s D reflecting a large influence on the estimates a and b, and are therefore potentially cause for concern. So, ideally, there would be no points outside the dashed lines.\n\n\nThe only problem we can see in the diagnostic plots is in the residuals-vs-fitted one, where there’s a bit of curvature, as identified by the red smoother line. Otherwise, the residuals are fairly normal (Q-Q) and with constant variance (scale-location).\nFrom the plots above, it looks like there may be little effect of temperature on chirps below, say, 23 degrees. Let’s fit this model again, this time removing the three data points below this temperature.\n\n\nCode\nchmsub &lt;- lm(chirps ~ degC, data=ch, subset = degC &gt; 23)\n\nsummary(chmsub)\n\n\n\nCall:\nlm(formula = chirps ~ degC, data = ch, subset = degC &gt; 23)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.05254 -0.60559 -0.05311  0.75837  1.07639 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.86776    2.48526   0.349    0.734    \ndegC         0.57421    0.08821   6.509 6.81e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7998 on 10 degrees of freedom\nMultiple R-squared:  0.8091,    Adjusted R-squared:   0.79 \nF-statistic: 42.37 on 1 and 10 DF,  p-value: 6.815e-05\n\n\nWe now see that if we exclude those points, thus restricting our model to only those times where the temperature is greater than 23°C, we now explain 81% of the variation. The estimated per-degree increase in chirping has gone from 0.38 to 0.57, indicating a much stronger effect.\nLet’s have a look at the diagnostic plots.\n\n\nCode\nplot(chmsub)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residuals seem to be a bit better behaved now. Certainly the trend identified in the previous model has disappeared. The smoother lines might look a little wild at first glance, but this is probably just because the dataset is small. I see no real cause for concern.\nNow plot the new model with confidence intervals.\n\n\nCode\n  ggplot() + \n  aes(degC, chirps) +\n  geom_point(\n    data = ch, \n    aes(colour = degC &gt; 23)\n    ) +\n  geom_smooth(\n    method = \"lm\", \n    data = ch |&gt; filter(degC &gt; 23),\n    colour = 1\n    )\n\n\n\n\n\nThis looks like a much better model overall. Perhaps we can conclude that above approximately 23 degrees, chirping increases linearly with temperature.\nIt is important to note that subsetting the data in this way changes the inference space to which the model applies. The chmsub model should not be used to predict chirps for temperatures less than 23°C (or greater than 34°C, for that matter). A model should only be used within the range of values spanned by the data used to create it. Beyond this range, it will very likely be wrong. Our exclusion of the sub-23° data therefore further restricts the range of values for which this model may be used."
  },
  {
    "objectID": "workshops/ws06.html#robust-regression",
    "href": "workshops/ws06.html#robust-regression",
    "title": "Chapter 6 Workshop",
    "section": "Robust regression",
    "text": "Robust regression\nFit a robust regression using a function in the MASS package.\n\n\nCode\nlibrary(visreg)\nlibrary(car)\nlibrary(MASS)\n\nrlmp &lt;- rlm(prestige ~ education, data = Prestige)\n\n\nPlot using the visreg package.\n\n\nCode\nvisreg(rlmp)\n\n\n\n\n\nNote:\n\nMore R code examples are here"
  },
  {
    "objectID": "workshops/ws04.html",
    "href": "workshops/ws04.html",
    "title": "Chapter 4 Workshop",
    "section": "",
    "text": "Code\nlibrary(tidyverse)"
  },
  {
    "objectID": "workshops/ws04.html#load-the-data",
    "href": "workshops/ws04.html#load-the-data",
    "title": "Chapter 4 Workshop",
    "section": "Load the data",
    "text": "Load the data\nLoad a dataset on elephants.\n\n\nCode\nele &lt;- read_table(\n  \"https://www.massey.ac.nz/~anhsmith/data/elephant.txt\"\n  ) |&gt; \n  rename(\"Height\" = `Shoulder.height`) # renames the variable\n\nstr(ele)\n\n\nspc_tbl_ [312 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Sex   : chr [1:312] \"F\" \"F\" \"F\" \"F\" ...\n $ Age   : num [1:312] 0 0 0 0 0 0 0 0 1 1 ...\n $ Height: num [1:312] 79 83 86 90 91 93 96 102 94 96 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Sex = col_character(),\n  ..   Age = col_double(),\n  ..   Shoulder.height = col_double()\n  .. )\n\n\nThere are three variables:\n\nSex: M or F\nAge: in years\nHeight: shoulder height in cm"
  },
  {
    "objectID": "workshops/ws04.html#explore-the-distribution-of-heights",
    "href": "workshops/ws04.html#explore-the-distribution-of-heights",
    "title": "Chapter 4 Workshop",
    "section": "Explore the distribution of heights",
    "text": "Explore the distribution of heights\nExamine the distribution of the heights.\n\n\nCode\nggplot(ele) + \n  aes(x = Height) +\n  geom_histogram(\n    aes(y = after_stat(density)) # this aes goes here because \n                                 # it only applies to geom_histogram\n    ) + \n  geom_density()\n\n\n\n\n\n\n\n\n\nDoes that look normally distributed to you? Is it symmetrical and bell-shaped?\nWe can test whether these data conform to a normal distribution model with a Shipiro-Wilk test for normality. The null hypothesis is that the data came from a population that is normal.\n\n\nCode\nshapiro.test(ele$Height)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  ele$Height\nW = 0.96785, p-value = 2.043e-06\n\n\nThe very low p-value says that we can reject the null hypothesis, and conclude that these data are very unlikely to have come from a normal distribution.\nThe data have both male and female elephants, so we should probably look at the sexes separately.\n\n\nCode\nggplot(ele) + \n   aes(x = Height) +\n   geom_histogram(\n     aes(y = after_stat(density)) # this aes goes here because \n                                  # it only applies to geom_histogram\n     ) + \n   geom_density() + \n   facet_grid( ~ Sex)\n\n\n\n\n\n\n\n\n\nCode\nggplot(ele) + \n  aes(x = Height, fill = Sex, colour = Sex) +\n  geom_density(alpha = .3)\n\n\n\n\n\n\n\n\n\nIt doesn’t look like there’s a major difference between the sexes in terms of the distribution of heights. Males might be a bit more variable than females (seeing the thicker tails).\nHave a look at the values of age.\n\n\nCode\nggplot(ele) + aes(Age) + geom_bar()\n\n\n\n\n\n\n\n\n\nThere is a wide range of ages, from 0 to 15 years old. Many elephants (49, to be exact) are 0 or 1 year old. Are the data normal if we remove these? We’ll first make a new copy of the data with the baby elephants removed and then repeat our tests for normality.\n\n\nCode\nele2 &lt;- ele |&gt; filter(Age &gt; 1) # make new dataset with babies removed\n\nggplot(ele2) +\n  aes(x = Height) +\n   geom_histogram(\n     aes(y = after_stat(density)) \n                                  \n     ) + \n   geom_density() \n\n\n\n\n\n\n\n\n\nThat looks a bit more normal.\n\n\nCode\nshapiro.test(ele2$Height)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  ele2$Height\nW = 0.99394, p-value = 0.3726\n\n\nOf elephants 2 years or older, there is no evidence that the heights do not come from a normal distribution (W = 1.0, p = 0.37). So, the heights of non-baby elephants appear approximately normally distributed."
  },
  {
    "objectID": "workshops/ws04.html#confidence-interval-for-the-sample",
    "href": "workshops/ws04.html#confidence-interval-for-the-sample",
    "title": "Chapter 4 Workshop",
    "section": "Confidence interval for the sample",
    "text": "Confidence interval for the sample\nLet’s now estimate the population mean (with a 95% confidence interval) of Heights in the dataset with the baby elephants removed.\n\n\nCode\nt.test(ele2$Height)\n\n\n\n    One Sample t-test\n\ndata:  ele2$Height\nt = 101.24, df = 262, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 178.0425 185.1058\nsample estimates:\nmean of x \n 181.5741 \n\n\nThe t.test() function is very flexible. When given a single sample, it will provide the mean and 95% confidence interval. How would you interpret this confidence interval?\nIt also provides a test of whether the mean of the sample is significantly different from zero. If the p-value is less than 0.05, then we can say that there is evidence against the null hypothesis of a mean of zero. Now, in this case, of course this isn’t useful. Elephants can never have zero or negative height, so a mean height of zero is nonsensical.\nNote that we can extract the estimate of the population mean and its confidence interval in the following way.\n\n\nCode\nt.test(ele2$Height)$estimate\n\n\nmean of x \n 181.5741 \n\n\nCode\nt.test(ele2$Height)$conf.int\n\n\n[1] 178.0425 185.1058\nattr(,\"conf.level\")\n[1] 0.95\n\n\nHow was this confidence interval calculated?\nThe standard error (the standard deviation over multiple samples) for a sample mean can be estimated by \\(s / \\sqrt n\\), where \\(s\\) is the sample standard deviation and \\(n\\) is the sample size.\n\n\nCode\nn = length(ele2$Height)\ns = sd(ele2$Height)\n\n( se = s / sqrt(n) )\n\n\n[1] 1.793566\n\n\nOnce you have estimated the standard error, a confidence interval for a sample mean \\(\\bar x\\) is simply\n\\[\n\\bar x \\pm t \\times \\text{se}\n\\]\nThe \\(t\\) value is the 0.975 quantile of the \\(t\\) distribution with the degrees of freedom given by \\(n-1\\).\n\n\nCode\nqt(p = 0.975, df = n - 1)\n\n\n[1] 1.96906\n\n\nSo, the sample mean and confidence interval is:\n\n\nCode\n( xbar &lt;- mean(ele2$Height) )\n\n\n[1] 181.5741\n\n\nCode\n( xbar - qt(p = 0.975, df = n - 1) * se )\n\n\n[1] 178.0425\n\n\nCode\n( xbar + qt(p = 0.975, df = n - 1) * se )\n\n\n[1] 185.1058"
  },
  {
    "objectID": "workshops/ws04.html#t-test-for-difference-of-two-sample-means",
    "href": "workshops/ws04.html#t-test-for-difference-of-two-sample-means",
    "title": "Chapter 4 Workshop",
    "section": "t-test for difference of two sample means",
    "text": "t-test for difference of two sample means\nLet’s say we are interested in testing whether the mean heights of male and female elephants are different. First, we can test for differences in the variances in the two sexes using Levene’s test, which is an important assumption of the t-test. In other words, if the assumption is violated, the results of the test may be misleading.\nLet’s do the Levene’s test.\n\n\nCode\ncar::leveneTest(Height ~ Sex, data = ele2)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  2.6478 0.1049\n      261               \n\n\nA p-value of 0.1 is not huge, but it’s not enough evidence to reject the null hypothesis that the variance of male heights is equal to the variance of female heights (at the 5% level).\nThe default option for a t-test using the t.test() function is not not assume equal variances.\n\n\nCode\nt.test(formula = Height ~ Sex, data = ele2) \n\n\n\n    Welch Two Sample t-test\n\ndata:  Height by Sex\nt = -1.3991, df = 251.38, p-value = 0.163\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -12.111371   2.050723\nsample estimates:\nmean in group F mean in group M \n       179.1259        184.1562 \n\n\nBy this test, there is no strong evidence against the null hypothesis that the mean heights of male and female elephants are equal.\nWelch Test does not assume equal variances. If we wanted to make this assumption (and therefore get more power):\n\n\nCode\nt.test(formula = Height ~ Sex, var.equal = T,  data = ele2) \n\n\n\n    Two Sample t-test\n\ndata:  Height by Sex\nt = -1.4044, df = 261, p-value = 0.1614\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -12.083154   2.022506\nsample estimates:\nmean in group F mean in group M \n       179.1259        184.1562 \n\n\nThe result is essentially the same in this case."
  },
  {
    "objectID": "workshops/ws04.html#one-sample-t-tests",
    "href": "workshops/ws04.html#one-sample-t-tests",
    "title": "Chapter 4 Workshop",
    "section": "One-sample t-tests",
    "text": "One-sample t-tests\nPerform a one-sample t-test to test the hypothesis that the true mean prestige is exactly 50.\n\n\nCode\nlibrary(tidyverse)\nlibrary(car)\ndata(Prestige)\n\n# Alternative hyp: greater or less than 50\nt.test(Prestige$prestige, mu=50)\n\n# Alternative hyp: greater than 50\nt.test(Prestige$prestige, mu=50, alternative=\"greater\")\n\n\nTest whether the true mean prestige score for professionals is 50% more than the true mean prestige score for white collar occupations.\n\n\nCode\nprof.data &lt;- Prestige |&gt; \n  filter(type==\"prof\") |&gt; \n  pull(prestige)\n\nwc.data &lt;- Prestige |&gt; \n  filter(type==\"wc\") |&gt; \n  pull(prestige) \n\nt.test(prof.data, \n       wc.data, \n       mu = 0.5 * mean(wc.data),\n       alternative = 'greater')"
  },
  {
    "objectID": "workshops/ws04.html#skewness",
    "href": "workshops/ws04.html#skewness",
    "title": "Chapter 4 Workshop",
    "section": "Skewness",
    "text": "Skewness\nExplore the skewness in the income variable using a boxplot.\n\n\nCode\nPrestige |&gt; \n  ggplot() +\n  aes(income) + \n  geom_boxplot() \n\n\nCode\n# or\nboxplot(Prestige$income, horizontal = TRUE)"
  },
  {
    "objectID": "workshops/ws04.html#transformation",
    "href": "workshops/ws04.html#transformation",
    "title": "Chapter 4 Workshop",
    "section": "Transformation",
    "text": "Transformation\nFind a suitable power transformation to correct the skewness.\n\n\nCode\nlibrary(lindia)\n\ngg_boxcox(lm(income ~ 1, data = Prestige))"
  },
  {
    "objectID": "workshops/ws04.html#confidence-interval",
    "href": "workshops/ws04.html#confidence-interval",
    "title": "Chapter 4 Workshop",
    "section": "Confidence interval",
    "text": "Confidence interval\nCompute the 95% confidence interval for the true mean income using the raw and log-transformed data.\n\n\nCode\nt.test(Prestige$income)\nt.test(log(Prestige$income))\n\n\nMore R code examples are here"
  },
  {
    "objectID": "workshops/ws02.html",
    "href": "workshops/ws02.html",
    "title": "Chapter 2 Workshop",
    "section": "",
    "text": "Setting up a Quarto project\nIt is a good idea to get into the habit of using Quarto projects, rather than just R scripts. Here is a step-by-step guide to creating a project for your workshops. You don’t have to use projects, but they are very useful.\n\nOpen RStudio. (Optional: click on the little window symbol at the top and select “Console on Right”)\n\n\n\nIf you haven’t already, make a directory on your computer where you want to keep your code for this course.\nMake a new project. Select the “Project” button at the top-right of Rstudio, and select “New Project…”.\n\n\n\nIn the pop-up window:\n\n\nSelect “New Directory”\n\n\n\nSelect “Quarto Project”\n\n\n\nChoose your directory via the “Browse”, and then give the project a name like “161250_workshops”\n\n\n\nFinish by clicking on “Create Project”.\n\nThe project should now be created, and you’ll likely have an open *.qmd file (something like “161250_workshops.qmd”) in the top-right window of Rstudio. We want to make a *.qmd file for this workshop.\n\nRight-click on the qmd tab and select “Rename”, and rename it “workshop2.qmd” or something similar. (Alternatively, just make a new file via the menus: File &gt; New File &gt; Quarto Document.)\n\n\nNow you have a document for your Workshop 2 work. You can:\n\nWrite headings with lines beginning with “#”.\nWrite text in the main part of the document.\nMake a code chunk for your R code using Ctrl-Alt-i. Write R code in the code chunks.\n\nLike so:\n\nThere are lots of tutorials online covering the basics of Quarto, and we’ll discuss them during our own workshops. Here are a couple for starters:\nhttps://quarto.org/docs/get-started/hello/rstudio.html\nhttps://www.youtube.com/watch?v=c654j7aQjcg\nThere are many advantages of Quarto projects. One is that you can put datasets into the project folder, and they’ll be easily accessible within your project, without having to worry about file paths.\nYou can easily open a recent past projects via the “Projects” button on the top-right of Rstudio.\n\n\nDataset Prestige\nAs you work through this workshop, you can copy the code and paste it into a code chunk. Write notes and observations to your self as you go.\nWe will be using a well-known dataset called Prestige from the car R package. This dataset deals with prestige ratings of Canadian occupations. The Prestige dataset has 102 rows and 6 columns. Each row (or ‘observation’) is an occupation.\nThis data frame contains the following columns:\n\neducation - Average education of occupational incumbents, years, in 1971.\nincome - Average income of incumbents, dollars, in 1971.\nwomen - Percentage of incumbents who are women.\nprestige - Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s.\ncensus - Canadian Census occupational code.\ntype - Type of occupation. A factor with levels: bc, Blue Collar; prof, Professional, Managerial, and Technical; wc, White Collar. (includes four missing values).\n\nFirst we’ll load the data. The dataset sits in the car package, so you need to load the car package first.\n\n\nCode\nlibrary(car)\ndata(Prestige)\n\n\n\n\nExercise 2.1\nDraw a bar chart for type. These plots show the count or relative frequency of blue collar (bc), professional (prof), and white collar (wc) professions in the dataset.\n\n\nCode\nlibrary(tidyverse)\n\np &lt;- Prestige |&gt; \n  ggplot() +\n  aes(type) + \n  geom_bar()\n\np\n\n\nOr with plotly (which works for HTML, not for PDF)\n\n\nCode\nlibrary(plotly)\n\nggplotly(p)\n\n\n\n\n\n\nOr with old-style R plot\n\n\nCode\n# or\nlibrary(car)\nbarplot(table(Prestige$type))\n\n\n\n\nExercise 2.2\nDraw a histogram of prestige.\nBelow demonstrates the flexibility of ggplot code. You can specify the data argument by piping it into ggplot, or by putting it as an argument to ggplot or a geom_. Likewise, the mapping or aes information, which determines which variables are used where, can be added as an extra line or specified inside the ggplot or geom_ function.\n\n\nCode\nPrestige |&gt; \n  ggplot() +\n  aes(x = prestige) +\n  geom_histogram()\n\n\nNow, this histogram, where the number of bins has been chosen for us, looks a bit “spiky” to my eye. You can control the number of bins by adding an argument bins = 10.\n\n\nCode\nPrestige |&gt; \n  ggplot() +\n  aes(x = prestige) +\n  geom_histogram(bins=10)\n\n\nggplot is very flexible as to where you put the data and the aes information; all of these methods give the same result.\n\n\nCode\nPrestige |&gt; \n  ggplot() +\n  aes(x = prestige) +\n  geom_histogram(bins=10)\n\n\nCode\nggplot(\n  data = Prestige,\n  mapping = aes(x = prestige)\n  ) +\n  geom_histogram(bins=10)\n\n\nCode\nggplot(Prestige) +\n  aes(x = prestige) +\n  geom_histogram(bins=10) \n\n\nCode\nggplot() +\n  geom_histogram(\n    data = Prestige,\n    mapping = aes(x = prestige),\n    bins = 10\n    )\n\n\nCode\n# or\n# library(plotly)\n# p &lt;- Prestige |&gt; \n#   ggplot() +\n#   aes(prestige) +\n#   geom_histogram(bins=10)\n# \n# ggplotly(p)\n\n# or\n# hist(Prestige$prestige)\n\n\nNow let’s display the prestige scores for each profession as a dot plot.\nNote that I’m including the code-chunk option #| fig-height: 12 at the beginning of my code chunk so that the plot is big enough to show all the professions without overlap.\n\n\nCode\nPrestige |&gt;\n  ggplot() + \n  aes(x = rownames(Prestige), y = prestige) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\nWhat a mess!\nWe can tidy it up by ordering the professions on the plot according to prestige. First, we move the professions from rownames to a variable. Then, we fct_reorder the professions using the prestige scores. Then, the resulting data gets piped into ggplot.\n\n\nCode\nPrestige |&gt; \n  rownames_to_column(var = \"profession\") |&gt; \n  mutate(\n    profession = fct_reorder(profession, prestige)\n    ) |&gt;\n  ggplot() + \n  aes(x = profession, y = prestige, colour = type) +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n\n\nExercise 2.3\nObtain some summary statistics for prestige. There are a few options for this.\n\n\nCode\nsummary(Prestige)\n\n\n   education          income          women           prestige    \n Min.   : 6.380   Min.   :  611   Min.   : 0.000   Min.   :14.80  \n 1st Qu.: 8.445   1st Qu.: 4106   1st Qu.: 3.592   1st Qu.:35.23  \n Median :10.540   Median : 5930   Median :13.600   Median :43.60  \n Mean   :10.738   Mean   : 6798   Mean   :28.979   Mean   :46.83  \n 3rd Qu.:12.648   3rd Qu.: 8187   3rd Qu.:52.203   3rd Qu.:59.27  \n Max.   :15.970   Max.   :25879   Max.   :97.510   Max.   :87.20  \n     census       type   \n Min.   :1113   bc  :44  \n 1st Qu.:3120   prof:31  \n Median :5135   wc  :23  \n Mean   :5402   NA's: 4  \n 3rd Qu.:8312            \n Max.   :9517            \n\n\nCode\nlibrary(psych)\n\ndescribe(Prestige)\n\n\n          vars   n    mean      sd  median trimmed     mad     min      max\neducation    1 102   10.74    2.73   10.54   10.63    3.15    6.38    15.97\nincome       2 102 6797.90 4245.92 5930.50 6161.49 3060.83  611.00 25879.00\nwomen        3 102   28.98   31.72   13.60   24.74   18.73    0.00    97.51\nprestige     4 102   46.83   17.20   43.60   46.20   19.20   14.80    87.20\ncensus       5 102 5401.77 2644.99 5135.00 5393.87 4097.91 1113.00  9517.00\ntype*        6  98    1.79    0.80    2.00    1.74    1.48    1.00     3.00\n             range skew kurtosis     se\neducation     9.59 0.32    -1.03   0.27\nincome    25268.00 2.13     6.29 420.41\nwomen        97.51 0.90    -0.68   3.14\nprestige     72.40 0.33    -0.79   1.70\ncensus     8404.00 0.11    -1.49 261.89\ntype*         2.00 0.40    -1.36   0.08\n\n\nCode\ndescribeBy(education + income + women + prestige ~ type, \n           data = Prestige)\n\n\n\n Descriptive statistics by group \ntype: bc\n          vars  n    mean      sd  median trimmed     mad     min     max\neducation    1 44    8.36    1.16    8.35    8.32    1.14    6.38   10.93\nincome       2 44 5374.14 2004.33 5216.50 5338.56 2275.05 1656.00 8895.00\nwomen        3 44   18.97   26.15    4.72   14.48    7.01    0.00   90.67\nprestige     4 44   35.53   10.02   35.90   35.46   11.34   17.30   54.90\n            range skew kurtosis     se\neducation    4.55 0.34    -0.76   0.18\nincome    7239.00 0.17    -1.00 302.16\nwomen       90.67 1.36     0.51   3.94\nprestige    37.60 0.05    -1.03   1.51\n------------------------------------------------------------ \ntype: prof\n          vars  n     mean      sd  median trimmed     mad     min      max\neducation    1 31    14.08    1.39   14.44   14.16    1.22   11.09    15.97\nincome       2 31 10559.45 5422.82 8865.00 9700.04 3955.58 4614.00 25879.00\nwomen        3 31    25.51   28.37   11.68   21.03   13.86    0.58    96.12\nprestige     4 31    67.85    8.68   68.40   67.34    9.19   53.80    87.20\n             range  skew kurtosis     se\neducation     4.88 -0.47    -0.93   0.25\nincome    21265.00  1.37     1.36 973.97\nwomen        95.54  1.14    -0.04   5.09\nprestige     33.40  0.36    -0.67   1.56\n------------------------------------------------------------ \ntype: wc\n          vars  n    mean      sd  median trimmed     mad     min     max\neducation    1 23   11.02    0.92   11.13   11.03    0.68    9.17   12.79\nincome       2 23 5052.30 1944.32 4741.00 4960.53 2342.51 2448.00 8780.00\nwomen        3 23   52.83   33.11   56.10   53.19   47.77    3.16   97.51\nprestige     4 23   42.24    9.52   41.50   41.61    8.60   26.50   67.50\n            range  skew kurtosis     se\neducation    3.62 -0.20    -0.27   0.19\nincome    6332.00  0.44    -1.18 405.42\nwomen       94.35 -0.10    -1.58   6.90\nprestige    41.00  0.63     0.18   1.98\n\n\n\n\nExercise 2.4\nMake a boxplot of prestige ~ type:\n\n\nCode\nPrestige |&gt; \n  ggplot() +\n  aes(y=prestige, x=type) +\n  geom_boxplot()\n\n\nCode\n# or\n# library(plotly)\n# p &lt;- Prestige |&gt; ggplot() + \n#   aes(y=prestige, x=type) + geom_boxplot()\n# ggplotly(p)\n\n# or\n# library(lattice)\n# bwplot(prestige ~ type, data=Prestige)\n\n# as violin plots\nPrestige |&gt; \n  ggplot() +\n  aes(y=prestige, x=type) +\n  geom_violin()\n\n\nCode\n# Or put it all together\nPrestige |&gt; \n  ggplot() +\n  aes(y=prestige, x=type) +\n  geom_violin() + \n  geom_boxplot(col = 2, alpha = .2) +\n  geom_jitter(alpha = .2, width = .2, height = 0, colour = 4)\n\n\n\n\nExercise 2.5\nObtain the Empirical Cumulative Distribution Function (ECDF) graphs of prestige ~ type:\n\n\nCode\nPrestige |&gt; \n  ggplot() + \n  aes(prestige, colour=type) +\n  stat_ecdf()\n\n\nCode\nPrestige |&gt; \n  ggplot() + \n  aes(prestige) +\n  stat_ecdf() + \n  facet_wrap(~type)\n\n\n\n\nCode\nPrestige |&gt; \n  ggplot() + \n  aes(\n    x = prestige, # these aes settings are used\n    col = type    # by both geoms\n    ) +\n  geom_density(\n    aes(fill = type), # the 'fill' aes goes here because \n    alpha = .2        # geom_rug doesn't use 'fill'\n    ) +\n  geom_rug()\n\n\nWith which plot – the ECDF or the density plot – is it easier to compare the distributions of prestige scores among these groups?\n\n\nExercise 2.6\nObtain the {0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95} quantiles of prestige:\n\n\nCode\npr &lt;- c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99)\n\nPrestige |&gt; \n  summarise(\n    probs = pr,\n    quants = quantile(prestige, pr)\n    )\n\n# or simply\nquantile(Prestige$prestige, pr)\n\n\n\n\nExercise 2.7\nObtain the scatter plot (with and without marginal boxplots) prestige vs. education :\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- Prestige |&gt; \n  ggplot() + \n  aes(x = education, y = prestige) +\n  geom_point() + \n  geom_smooth(col = 2) + \n  geom_smooth(method = \"lm\", se = FALSE)\n\nggMarginal(p1, type=\"boxplot\")\n\n\n\n\nCode\nlibrary(car)\n\nscatterplot(education ~ prestige, data = Prestige)\n\n\nThe later plot will show prediction interval ribbon while the first plot will show the confidence interval ribbon.\n\n\nExercise 2.8\nObtain the bubble or balloon plot prestige vs. education vs. income (income forming the bubble size):\n\n\nCode\nlibrary(ggplot2)\n\nPrestige |&gt; \n  ggplot() + \n  aes(x = education, y = prestige, size = income) +\n  geom_point()\n\n\nCode\n# or\n\nlibrary(plotly)\n\np &lt;- Prestige |&gt;\n  ggplot() + \n  aes(x = education, y = prestige, size = income) +\n  geom_point()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nExercise 2.9\nObtain the contour plot prestige vs. education vs. income :\n\n\nCode\nlibrary(plotly)\n\nplot_ly(type = 'contour', \n        x = Prestige$education, \n        y = Prestige$income, \n        z = Prestige$prestige)\n\n\nTo add axes labels and titles, try-\n\n\nCode\nplot_ly(\n  Prestige,\n  type = 'contour',\n  x = Prestige$education,\n  y = Prestige$income,\n  z = Prestige$prestige\n) |&gt; layout(\n  title = 'Contour Plot of prestige scores',\n  xaxis = list(title = 'education'),\n  yaxis = list(title = 'income')\n)\n\n\nWe can also define our own function for the contour approximation.\n\n\nCode\nlibrary(modelr)\n\n# make a smooth model\ny.m = loess(prestige ~ education * income, data = Prestige)\n\n# make a regular grid of all combinations of education and income\nmygrid &lt;- Prestige |&gt; \n  data_grid(\n    education = seq_range(education, 50),\n    income = seq_range(income, 50)\n  ) |&gt; \n  # add predicted prestige using the smooth model\n  add_predictions(y.m, var = \"predicted prestige\")\n\n# make ggplot contour plot\np &lt;- mygrid |&gt; \n  ggplot() + \n  aes(x = education, y = income, z = `predicted prestige`) +\n  geom_contour()\n\np\n\n\nCode\n# make a plotly version\nlibrary(plotly)\nggplotly(p)\n\n# filled contour ggplot\nmygrid |&gt; \n  ggplot() + \n  aes(x=education, y=income, z=`predicted prestige`) +\n  stat_contour_filled()\n\n\nCode\n# or the older-style lattice graphs \n\nlibrary(lattice)\n\ncontourplot(`predicted prestige` ~ education * income, \n            data = mygrid, \n            cuts = 10, region = TRUE,\n            xlab = \"education \", ylab = \"income \")\n\n\nCode\nwireframe(`predicted prestige` ~ education * income, \n          data = mygrid,  \n          cuts = 10, region = TRUE, \n          xlab = \"education \", ylab = \"income \")\n\n\nCode\nlevelplot(`predicted prestige` ~ education * income, \n          data = mygrid,  \n          cuts = 10, region = TRUE, \n          xlab = \"education \", ylab = \"income \")\n\n\nCode\ncloud(`predicted prestige` ~ income * education, \n      data = mygrid) \n\n\n\n\nExercise 2.10\nObtain an interactive 3-D plot of prestige vs. education vs. income using plotly.\n\n\nCode\nplot_ly(\n  data = Prestige,\n  x = ~education, \n  y = ~income, \n  z = ~prestige) |&gt; \n  add_markers() \n\n\n\n\n\n\n\n\nExercise 2.11\nCreate prestige ~ education | type graphs. That is, prestige ~ education grouped by type as colours and/or panels.\n\n\nCode\nPrestige |&gt; \n  ggplot() + \n  aes(x = education, y = prestige, colour = type) +\n  geom_point() + \n  facet_wrap(~ type)\n\n\nCode\n# or\n# library(plotly)\n#\n# p &lt;- Prestige |&gt; \n#   ggplot() + \n#   aes(x = education, y = prestige, color = type) +\n#   geom_point() + \n#   facet_wrap(~ type)\n# \n# ggplotly(p)\n\n\n\n\nCode\np &lt;- Prestige |&gt; \n  ggplot() + \n  aes(x = education, y = prestige, color = type) +\n  geom_point()\n\np\n\n\nCode\n# OR\n#\n# library(plotly)\n# ggplotly(p)\n\n\nMore graphing examples are here (R code file)."
  },
  {
    "objectID": "workshops/references.html",
    "href": "workshops/references.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nReferences"
  },
  {
    "objectID": "studyguide/index.html",
    "href": "studyguide/index.html",
    "title": "Preface",
    "section": "",
    "text": "“Statistics is the grammar of science.”\n— Karl Pearson\nIn this course, principles and techniques will be considered of collecting, displaying, and analysing data. The Study Guide covers the basic types of sets of numbers, namely single batches, two or more batches, related batches and data in tables. It contains Exploratory Data Analysis (EDA) techniques for displaying data, fitting a model, and considering transformations of variables so that the data better fit the assumptions of the models. The Study Guide also contains some methods for testing hypotheses, namely \\(t\\), \\(F\\) and \\(\\chi^2\\) tests; and some thoughts on the collection data.\nThe essence of Exploratory Data Analysis, or EDA, is to search for clues by whatever graphical or numerical means seem appropriate and even though these methods are often easy to understand, it is useful to have a computer at hand to perform the computations and draw graphs. Methods for EDA have developed very quickly over the past few decades. John Tukey (1977) likened EDA to detective work where data are studied carefully to provide clues. Some of the techniques employed are quite old and date back to the nineteenth century and beyond when researchers collected information in an organised way and displayed it in tables and graphs. Other techniques are of more recent origin and have been developed to more easily discern peculiarities and trends in data and to provide robust methods of fitting models to data, and these methods not being overly sensitive to restrictive assumptions.\nInferential statistics1 has also seen huge progress in the last 50 or so years since we’ve had fast computers. Inference uses probability models to test hypotheses and estimate parameters while quantifying uncertainty. These procedures lie at the heart of all quantitative science, and can be likened to the way a detective evaluates evidence for competing theories. Regression and Analysis of Variance models are covered in the later part of the study guide, where we will use both EDA and inferential statistics. Topics such as time series analysis, nonparametric methods, and experimental design are presented only briefly. It is intended that these sections will give you an idea of how advanced modelling or analysis can be done.\nNote that you are not expected to study all the topics in depth. Some sections of the study guide are intended for revision of the topics you studied in a first year statistics course. Some topics such as experimental design, correspondence analysis etc will not be examined in depth.\nData analysis is best learned by practice—exploring, summarising, plotting, and testing ideas with real data. Reading a textbook or study guide on data analysis is important, but this activity is more like reading a cook book. In the words of John Tukey:"
  },
  {
    "objectID": "studyguide/index.html#footnotes",
    "href": "studyguide/index.html#footnotes",
    "title": "Preface",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInferential statistics is sometimes called “Confirmatory Analysis”, but I’m not a fan of this term. Real scientists acknowledge that there is always uncertainty, and we can never really “confirm” anything!↩︎"
  },
  {
    "objectID": "studyguide/7-multiple.html",
    "href": "studyguide/7-multiple.html",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "",
    "text": "In this chapter, we consider multiple regression and other models in which there are more than one predictor (or \\(X\\)) variable. This extends what we covered in the last chapter where we examined one predictor to multiple predictors. Once again our focus is on finding the estimates of coefficients or parameters in multiple linear regression models by the method of least squares. For this we assume that\nWe will continue to use the data set horsehearts of the weights of horses’ hearts and other related measurements."
  },
  {
    "objectID": "studyguide/7-multiple.html#significance-testing-of-type-i-ss",
    "href": "studyguide/7-multiple.html#significance-testing-of-type-i-ss",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Significance testing of Type I SS",
    "text": "Significance testing of Type I SS\nThe significance of the additional variation explained by a predictor can be tested using a \\(t\\) or \\(F\\) statistic. Consider the simple regression model of WEIGHT on EXTDIA. Suppose we decided to add the explanatory variable OUTERDIA to the model, i.e. regress WEIGHT on two explanatory variables EXTDIA and OUTERDIA. Is this new model a significant improvement on the existing one? For testing the null hypothesis that the true slope coefficient of OUTERDIA in this model is zero, the \\(t\\)-statistic is 1.531 (see output below).\n\n\nCode\ntwovar.model &lt;- lm(WEIGHT~ EXTDIA+OUTERDIA, data=horsehearts)\n\ntwovar.model |&gt; tidy()\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -1.97     0.551      -3.57 0.000885\n2 EXTDIA         0.226    0.0614      3.68 0.000637\n3 OUTERDIA       0.522    0.341       1.53 0.133   \n\n\nThe \\(t\\) and \\(F\\) distributions are related by the equation \\(t^{2} =F\\) when the numerator df is just one for the \\(F\\) statistic. Hence 1.532 = 2.34 is the \\(F\\) value for testing the significance of the additional SSR due to OUTERDIA. In other words, the addition of OUTERDIA to the simple regression model does not result in a significant improvement in the sense that the reduction in residual SS (= 1.247) as measured by the \\(F\\) value of 2.34 is not significant (\\(p\\)-value being 0.133).\n\n\nCode\nonevar.model &lt;- lm(WEIGHT~ EXTDIA, data=horsehearts)\n\ntwovar.model &lt;- lm(WEIGHT~ EXTDIA+OUTERDIA, data=horsehearts)\n\nanova(onevar.model, twovar.model)\n\n\nAnalysis of Variance Table\n\nModel 1: WEIGHT ~ EXTDIA\nModel 2: WEIGHT ~ EXTDIA + OUTERDIA\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     44 24.115                           \n2     43 22.867  1    1.2472 2.3453  0.133\n\n\nAlthough OUTERDIA is correlated with WEIGHT, it also has high correlation with EXTDIA. In other words, the correlation matrix gives us some indication of how many variables might be needed in a multiple regression model, although by itself it cannot tell us what combination of predictor variables is good or best.\n\n\n\nFigure 5: Issues with multiple predictors\n\n\n\n\n\nFigure 6: Effect of multiple predictors on model summaries\n\n\nFigure 5 and Figure 6 summarise the following facts:\n\nWhen there is only one explanatory variable, \\(R^2\\) = SSR/SST equals the square of the correlation coefficient between that variable and the dependent variable. Therefore if only one variable is to be chosen, it should have the highest correlation with the response variable, \\(Y\\).\nWhen variables are added to a model, the regression sum of squares SSR will increase and the residual or error sum of squares SSE will reduce. The opposite is true if variables are dropped from the model. This fact follows from Figure 6.\nThe other side of the coin to the above remark is that as additional variables are added, the Sums of Squares for residuals, SSE, will decrease towards zero as also shown in Figure 6(c).\nThe overlap of circles in suggests that these changes in both SSR and SST will lessen as more variables are added, see Figure 5(b).\nFollowing on from the last two notes, as \\(R^2\\) = SSR/SST, \\(R^2\\) will increase monotonically towards 1 as additional variables are added to the model. (monotonically increasing means that it never decreases although it could remain the same). This is indicated by Figure 6(a). If variables are dropped, then \\(R^2\\) will monotonically decrease.\nAgainst the above trends, the graph of residual mean square in Figure 6(b) reduces to a minimum but may eventually start to increase if enough variables are added. The residual sum of squares SSE decreases as variables are added to the model (see Figure 5(b)). However, the associated df values also decrease so that the residual standard deviation decreases at first and then starts to increase as shown in Figure 6(b). (Note that the residual standard error \\(s_{e}\\) is the square root of the residual mean square\n\\[s_{e}^{2} =\\frac{{\\text {SSE}}}{{\\text {error degrees of freedom}}},\\]\ndenoted as MSE in Figure 5(b)). After a number of variables have been entered, the additional amount of variation explained by them slows down but the degrees of freedom continues to change by 1 for every variable added, resulting in the eventual increase in residual mean square. Note that the graphs in Figure 5 are idealised ones. For some data sets, the behaviour of residual mean square may not be monotone.\nNotice that the above trends will occur even if the variables added are garbage. For example, you could generate a column of random data or a column of birthdays of your friends, and this would improve the \\(R^2\\) but not the adjusted \\(R^2\\). The adjusted \\(R^2\\) makes adjustment for the degrees of freedom for the SSR and SSE, and hence reliable when compared to the unadjusted or multiple \\(R^2\\). The residual mean square error also partly adjusts for the drop in the degrees of freedom for the SSE and hence becomes an important measure. The addition of unimportant variables will not improve the adjusted \\(R^2\\) and the mean square error \\(s_{e}^{2}\\)."
  },
  {
    "objectID": "studyguide/7-multiple.html#other-ss-types",
    "href": "studyguide/7-multiple.html#other-ss-types",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Other SS types",
    "text": "Other SS types\nThe R anova function anova() calculates sequential or Type-I SS values.\nType-II sums of squares is based on the principle of marginality. Type II SS correspond to the R convention in which each variable effect is adjusted for all other appropriate effects.\nType-III sums of squares is the SS added to the regression SS after ALL other predictors including an intercept term. This SS however creates theoretical issues such as violation of marginality principle and we should avoid using this SS type for hypothesis tests.\nThe R package car has the function Anova() to compute the Type II and III sums of squares. Try-\n\n\nCode\nfull.model &lt;- lm(WEIGHT~ ., data=horsehearts)\n\nanova(full.model)\n\nlibrary(car)\n\nAnova(full.model, type=2)\nAnova(full.model, type=3)\n\n\nFor the horsehearts data, a comparison of the Type I and II sums squares is given below:\n\n\nCode\nfull.model &lt;- lm(WEIGHT~ ., data=horsehearts)\n\nanova1 &lt;- full.model |&gt; \n  anova() |&gt; \n  tidy() |&gt; \n  select(term, \"Type I SS\" = sumsq)  \n\nanova2 &lt;- full.model |&gt; \n  Anova(type=2) |&gt; \n  tidy() |&gt; \n  select(term, \"Type II SS\" = sumsq) \n\ntype1and2 &lt;- full_join(anova1, anova2, by=\"term\")\n\ntype1and2\n\n\n\n\n\n\n \n  \n    term \n    Type I SS \n    Type II SS \n  \n \n\n  \n    INNERSYS \n    34.40 \n    0.20 \n  \n  \n    INNERDIA \n    3.53 \n    0.62 \n  \n  \n    OUTERSYS \n    2.76 \n    1.69 \n  \n  \n    OUTERDIA \n    0.13 \n    0.55 \n  \n  \n    EXTSYS \n    0.06 \n    1.79 \n  \n  \n    EXTDIA \n    1.90 \n    1.90 \n  \n  \n    Residuals \n    14.07 \n    14.07 \n  \n\n\n\n\n\nWhen predictor variables are correlated, it is difficult to assess their absolute importance and the importance of a variable can be assessed only relatively. This is not an issue with the most highly correlated predictor in general."
  },
  {
    "objectID": "studyguide/7-multiple.html#best-subsets-selection",
    "href": "studyguide/7-multiple.html#best-subsets-selection",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Best Subsets Selection",
    "text": "Best Subsets Selection\nAn exhaustive screening of all possible regression models (and hence the name best subsets regression) can also be done using software. For example, there are 6 predictor variables in the horses’ hearts data. If we fix the number of predictors as 3, then \\(\\small {\\left(\\begin{array}{c} {6} \\\\ {3} \\end{array}\\right)} = 20\\) regression models are possible. One may select the ‘best’ 3-variable model based on criteria such as AIC, \\(C_{p}\\), \\(R_{adj}^{2}\\) etc. Software must be employed to perform the conventional stepwise regression procedures. Software algorithms give one or more best candidate models fixing the number of variables in each step.\nOn the basis of our analysis on the horses’ hear data, we might decide to recommend the model with predictor variables EXTDIA, EXTSYS, INNERDIA and OUTERSYS. In particular if the model is to be used for describing relationships then we would tend to include more variables. For prediction purposes, however, a simpler feasible model is preferred and in this case we may opt for the smaller model with only INNERDIA and OUTERSYS. See Table 4 produced using the following R codes:\n\n\nCode\nlibrary(leaps)\nlibrary(HH)\nlibrary(kableExtra)\n\nb.model &lt;- regsubsets(WEIGHT ~ ., data = horsehearts) |&gt;\n  summaryHH() \n\nb.model |&gt; \n  kable(digits = 3) |&gt; \n  kable_styling(bootstrap_options = \"basic\", full_width = F)\n\n\n\n\nTable 4: Subset selection\n\n\nmodel\np\nrsq\nrss\nadjr2\ncp\nbic\nstderr\n\n\n\n\nINNERD\n2\n0.658\n19.450\n0.650\n11.923\n-41.677\n0.665\n\n\nINNERD-OUTERS\n3\n0.715\n16.173\n0.702\n4.838\n-46.335\n0.613\n\n\nINNERD-OUTERS-OUTERD\n4\n0.718\n16.043\n0.698\n6.477\n-42.878\n0.618\n\n\nINNERD-OUTERS-EXTS-EXTD\n5\n0.741\n14.739\n0.715\n4.862\n-42.949\n0.600\n\n\nINNERD-OUTERS-OUTERD-EXTS-EXTD\n6\n0.749\n14.272\n0.718\n5.566\n-40.602\n0.597\n\n\nINNERS-INNERD-OUTERS-OUTERD-EXTS-EXTD\n7\n0.753\n14.067\n0.714\n7.000\n-37.437\n0.601\n\n\n\n\n\n\n\n\nSometimes theory may indicate that a certain explanatory variable should be included in the model (e.g. due to small sample size). If this variable is found to make an insignificant contribution to the model, then one should exclude the variable when the model is to be used for prediction but if the model is to be used for explanation purposes only then the variable should be included. Other considerations such as cost and time may also be taken into account. For every method or algorithm, one could find peculiar data sets where it fouls up. The moral – be alert and don’t automatically accept models thrown up by a program. Note there is never one right answer as different methods and different criteria lead to different models.\nVariable selection procedures can be a valuable tool in data analysis, particularly in the early stages of building a model. At the same time, they present certain dangers. There are several reasons for this:\n\nThese procedures automatically snoop though many models and may select ones which, by chance, happen to fit well.\nThese forward or backward stepwise procedures are heuristic (i.e., shortcut) algorithms, which often work very well but which may not always select the best model for a given number of predictors (here best may refer to adjusted \\(R^2\\)-values, or AIC or some other criterion).\nAutomatic procedures cannot take into account special knowledge the analyst may have about the data. Therefore, the model selected may not be the best (or make sense) from a practical point of view.\nMethods are available that shrink coefficients towards zero. The least squares approach minimises the residual sums of squares or RSS without placing any constraint on the coefficients. The shrinkage methods, which place a constraint on the coefficients, work well when there are large numbers of predictors. A ridge regression shrinks the coefficients towards zero but in relation each other. On the other hand, (Least Absolute Selection and Shrinkage Operator) lasso regression shrinks some of coefficients to zero which means these predictors can be dropped. Note that the ridge regression does not completely remove predictors. By shrinking large coefficients, we obtain a model with higher bias but lower variance. This process is known as regularisation in the literature (not covered in this course)."
  },
  {
    "objectID": "studyguide/7-multiple.html#time-series-regression-with-seasonality-components",
    "href": "studyguide/7-multiple.html#time-series-regression-with-seasonality-components",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Time Series Regression with seasonality components",
    "text": "Time Series Regression with seasonality components\nIndicator variables are used to capture the seasonality such as months and quarters. Time related trends can be picked up with the usual regression. The function tslm() from the forecast package is handy to model the response \\(Y\\) using the time variable and the seasonal indicators. Consider the credit card balance series discussed in Chapter 2. The fitted linear model is shown in Table 10 and the forecasts made the model for 48 months ahead are shown in Figure 11.\n\n\nCode\nlibrary(readxl)\n\nurl &lt;- \"http://www.massey.ac.nz/~anhsmith/data/hc12_daily_average_balances.xlsx\"\ndestfile &lt;- \"hc12.xlsx\"\n\ncurl::curl_download(url, destfile)\n\ncredit.balance &lt;- \n  read_excel(destfile, na = \"-\", skip = 4) |&gt; \n  pull(CRCD.MOA20) |&gt; \n  na.omit() |&gt; \n  ts(start=c(2000,7), frequency=12)\n\n\n\n\nCode\nlibrary(forecast)\n\ncbfit &lt;- tslm(credit.balance ~ trend + season)\n\ncbfit |&gt; forecast(h=48) |&gt; autoplot()\n\n\n\n\n\nFigure 11: tidy() output of the tslm() fit\n\n\n\n\n\n\nCode\ncbfit |&gt; tidy()  \n\n\nWarning: The `tidy()` method for objects of class `tslm` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\n\n\n\nTable 10:  tidy() output of the tslm() fit \n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3460.949 \n    122.430 \n    28.269 \n    0.000 \n  \n  \n    trend \n    12.771 \n    0.395 \n    32.358 \n    0.000 \n  \n  \n    season2 \n    -60.206 \n    154.786 \n    -0.389 \n    0.698 \n  \n  \n    season3 \n    -96.978 \n    154.787 \n    -0.627 \n    0.532 \n  \n  \n    season4 \n    -137.532 \n    154.790 \n    -0.889 \n    0.375 \n  \n  \n    season5 \n    -164.651 \n    154.793 \n    -1.064 \n    0.288 \n  \n  \n    season6 \n    -175.292 \n    154.798 \n    -1.132 \n    0.258 \n  \n  \n    season7 \n    -232.007 \n    153.165 \n    -1.515 \n    0.131 \n  \n  \n    season8 \n    -209.839 \n    154.798 \n    -1.356 \n    0.176 \n  \n  \n    season9 \n    -215.827 \n    154.793 \n    -1.394 \n    0.164 \n  \n  \n    season10 \n    -181.642 \n    154.790 \n    -1.173 \n    0.242 \n  \n  \n    season11 \n    -119.718 \n    154.787 \n    -0.773 \n    0.440 \n  \n  \n    season12 \n    14.728 \n    154.786 \n    0.095 \n    0.924 \n  \n\n\n\n\n\n\nTable 10 shows that the seasonal effects are highly significant. Figure 11 shows that the fitted model is not faring well for the year 2020, which was affected by COVID. The forecasts ahead are also untrustworthy.\nNote that the time variable \\(t\\) becomes the predictor in the fitted model but the model is not based on the past or lagged \\(Y\\) data. The smoothing methods discussed below employ such past data."
  },
  {
    "objectID": "studyguide/7-multiple.html#moving-average-smoothing",
    "href": "studyguide/7-multiple.html#moving-average-smoothing",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Moving Average Smoothing",
    "text": "Moving Average Smoothing\nHere we compute the mean of successive smaller sets of numbers of immediate past data. The period or length (also called span) of the moving average is the number of observations (including the present one) used for averaging. The general expression for the moving average \\(M_t\\) at time \\(t\\) is \\[M_t = [X_t + X_{t-1} + ... + X_{t-N+1}] / N\\] where \\(X_t\\) is the observation at time \\(t\\) and \\(N\\) the moving average length. Figure 12 shows the moving average smoothing for the ‘$20 Notes in public hands’ data. It can be noted that the degree of smoothing is directly related to the length of the moving average (i.e., longer the length, greater the smoothing).\n\n\nCode\nNZnotes20 &lt;- read_table(\n  \"http://www.massey.ac.nz/~anhsmith/data/20dollar.txt\") |&gt; \n  pull(value) |&gt; \n  ts(start=1968, frequency=1)\n\n\n\n\nCode\nMA.centred &lt;- ma(NZnotes20, 2, centre = TRUE)\nMA.noncentred &lt;- ma(NZnotes20, 2, centre = FALSE)\n\nlibrary(forecast)\n\nautoplot(NZnotes20) + \n  autolayer(MA.centred, series = \"2 y MA centred\") + \n  autolayer(MA.noncentred, series = \"2 y MA noncentred\", linetype=2)\n\n\n\n\n\nFigure 12: Centred and Non-centred Moving Averages\n\n\n\n\nWhen placing the moving averages against time, placing them in the middle time period is more appropriate. Strictly speaking the moving average must fall at \\(t = 1.5, 2.5, 3.5\\) etc when the period of the moving average is an even number. Hence we need to smooth again the moving average smoothed values to place the moving averages at \\(t = 2, 3, 4\\) etc. Figure 12 also compares the centred moving average smoothing and non-centred moving average smoothing (length 2) for the ‘$20 Notes in public hands’ data. It is easy to see that centring has stopped the moving averages from drifting below the original series and ‘lined’ them with the original data."
  },
  {
    "objectID": "studyguide/7-multiple.html#exponential-smoothing",
    "href": "studyguide/7-multiple.html#exponential-smoothing",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Exponential Smoothing",
    "text": "Exponential Smoothing\nIn moving average smoothing all past observations are given equal weight. In exponential average smoothing, past observations (i.e. as the observations become older) are given exponentially decreasing weights. That is, recent observations are given relatively more weight than the older observations. Hence the exponential smoothing method becomes a representative method to produce a smoothed time series. The average computed using exponentially decreasing weights is known as the Exponentially Weighted Moving average (EWMA). This fitted average is also called level because this method does not allow for trends or seasonality (and everything gets smoothed).\nEWMA smoothing begins by setting \\(S_0\\) to \\(x_1\\) (usually), where \\(S\\) stands for smoothed observation (or EWMA), and \\(x\\) for the observation. The subscript in \\(x\\) refers to the time periods \\(t =1, 2, ... ,n\\). For the second period, \\(S_2 = \\alpha x_2 + (1-\\alpha)x_1\\) and so on. Here the parameter \\(\\alpha\\) is called the smoothing constant, the weight given to the current observation. A general formula is also available to compute the EWMA for any time period \\(t\\). Figure 13 shows the single exponential smoothing on the $20 Notes series for \\(\\alpha=0.5\\). Instead of fixing an \\(\\alpha\\) value such as 0.5, we may leave it to the software to find an optimum value.\n\n\nCode\nsingle.exp &lt;- NZnotes20 |&gt; ses(alpha=0.5) |&gt; fitted()\n\np1 &lt;- autoplot(NZnotes20) + \n  autolayer(single.exp, series =\"alpha=0.5\")\n\nsingle.exp1 &lt;- NZnotes20 |&gt; ses() |&gt; fitted()\n\np2 &lt;- autoplot(NZnotes20) + \n  autolayer(single.exp1, series = \"optimised alpha\")\n\nlibrary(patchwork)\n\np1/p2\n\n\n\n\n\nFigure 13: Single exponential smoothing (fits)\n\n\n\n\nThe rate at which the effect of older observations on the current EWMA will be dampened depends on \\(\\alpha\\), the smoothing constant. Larger the \\(\\alpha\\) value, faster the dampening effect of older observations.\nA naive choice for the initial value for \\(S_0\\) (i.e. at the origin) is \\(x_1\\), the first observation. The other choices include the average of two or more successive observations, estimating using regression methods etc. In this course we will not be concerned with the choice of the initial values very much (and accept the defaults of the R packages)."
  },
  {
    "objectID": "studyguide/7-multiple.html#double-exponential-smoothing",
    "href": "studyguide/7-multiple.html#double-exponential-smoothing",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Double Exponential Smoothing",
    "text": "Double Exponential Smoothing\nSingle exponential smoothing is improved to double exponential smoothing to account for the trend type of variations. This is achieved by introducing a second smoothing constant say \\(\\beta\\). This weighting constant captures linear trends using the successive differences in the fitted EWMAs. The process of double exponential smoothing is conveniently represented by the following two equations.\n\\(S_t = \\alpha X_t+(1-\\alpha)[S_{t-1} + T_{t-1}]\\) (called level equation)\nwhere\n\\(T_t =\\beta [S_t - S_{t-1}] + (1 -\\beta)T_{t-1}\\) (called trend equation).\nThe second equation for the trend EWMA gives a weight of \\(\\beta\\) to the current differences in the EWMAs (i.e. \\(S_t - S_{t-1}\\)) and the balance weight \\((1-\\beta)\\) to the preceding trend EWMA.\nThe main or usual EWMA (i.e. \\(S_t\\)) gives a weight of \\(\\alpha\\) to the current observation and the balance weight \\((1-\\alpha)\\) to the sum of preceding main and trend EWMAs (i.e. \\(S_{t-1} + T_{t-1}\\)). A naive choice for the initial value for \\(T_0\\) (i.e. at the origin) is \\(x_2-x_1\\), the difference between the first and the second observations. The other choices include the average of two or more successive differences, estimating using regression methods etc. In this course we will not be concerned with the choice of the initial values very much. The smoothing constants \\(\\alpha\\) and \\(\\beta\\) are obtained by non-linear optimisation methods (such as the Marquardt algorithm). In this course, we will just accept the R outputs as the optimised fits. Figure 14 shows the double exponential smoothing on the $20 Notes series with optimum \\(\\alpha\\) and \\(\\beta\\) (as determined by the forecast package).\n\n\nCode\ndouble.exp &lt;- NZnotes20 |&gt; holt() |&gt; fitted()\n\nautoplot(NZnotes20) + \n  autolayer(double.exp, series = \"DEWMA-optimised\")\n\n\n\n\n\nFigure 14: Double exponential smoothing"
  },
  {
    "objectID": "studyguide/7-multiple.html#triple-exponential-smoothing",
    "href": "studyguide/7-multiple.html#triple-exponential-smoothing",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Triple Exponential Smoothing",
    "text": "Triple Exponential Smoothing\nThis approach developed by Holt and Winter (hence the name Holts-Winter (HW) smoothing) employs a level equation, a trend equation, and a seasonal equation for smoothing at each time period. Hence three weights, or smoothing parameters are needed.\n\\(S_t = \\alpha(X_t-P_{t-p}) + (1-\\alpha)[S_{t-1}+T_{t-1}]\\) (level equation)\n\\(T_t = \\beta [S_t-S_{t-1}]+ 1-\\beta)T_{t-1}\\) (trend equation)\n\\(P_t = \\phi (X_t-S_t)+(1-\\phi)P_{t-p}\\) (seasonal equation of a given period \\(p\\))\nThe smoothing parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\phi\\) are constants and are usually estimated minimising the MSE. In order to proceed with the triple exponential smoothing, we need at least one complete season’s data to determine initial estimates of the seasonal indices. For estimating the trend components, it is preferable to have at least two complete season’s data.\nThe initial trend is usually estimated using the average differences in the corresponding observations in two adjacent seasons. The estimating initial values for seasonal components, we use the averages rather than differences. Regression methods are also employed for estimating the initial values. In this course, we will not study the estimation methods for initial values in any detail but will accept computations and optimisation reported in the forecast R package. Figure 15 shows the triple exponential smoothing to the outstanding credit card balances series.\n\n\nCode\nlibrary(forecast)\n\ntrp.exp &lt;- credit.balance |&gt; hw() |&gt; fitted()\n\nautoplot(credit.balance) +\n  autolayer(trp.exp, \n            series = \"Holt-Winter- optimised\")\n\n\n\n\n\nFigure 15: Triple exponential smoothing"
  },
  {
    "objectID": "studyguide/7-multiple.html#assessment-of-fit",
    "href": "studyguide/7-multiple.html#assessment-of-fit",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Assessment of Fit:",
    "text": "Assessment of Fit:\nForecast accuracy measures such as MSE are useful for fixing the smoothing parameters such as the moving average length or the EWMA smoothing constant. We may minimise MSE (say) to fix a value for the EWMA smoothing constant. This can be done by trial and error or by nonlinear optimisation methods (such as Marquardt’s procedure).\nBy the term forecasting, we mean projecting the present time series for future time points. For example, assume that we used an uncentered two period moving average to smooth the ‘$20 Notes’ time series. The moving average value (non-centred) for 1969 is 18.05. A naive approach to forecasting will be to use the smoothed value at time \\((t-1)\\) to forecast for time \\(t\\). Hence the forecasted value (or simply forecast) of $20 bills for 1970 would be 18.05 as against the actual observed value of 21.76. In the absence of Year 1970 data, the same value 18.05 would be the forecast for 1971 and so on. MAs are not useful for forecasting in general and hence this average is just employed to fit trends or extract trends when seasonal variation is absent.\nFor EWMA Forecasting, the forecast approach is to add an adjustment for the error that occurred in the last forecast. We again consider ‘$20 Notes’ time series and obtain the EWMA smoothed values for \\(\\alpha = 0.4\\). For the year 1969 (say), the EWMA value is 17.78 as against the actual value 19.41 giving an error of 19.41- 17.78= 2.72. This error is given a weight of 0.4 and added to the 1969 forecast (naive estimate) of 16.69 as \\(16.69+0.4\\times2.72\\) giving a forecast value of 17.78 for 1970. The term ‘adjustment error’ will refer to \\(0.4\\times2.72 = 1.088\\). This forecast value is also obtained as\nForecast for \\(1970 = 0.4\\times19.41+0.6\\times16.69= 17.78\\).\n(from the relationship \\(S_{t+1}=\\alpha x_{t+1}+(1-\\alpha)S_t\\) where the unavailable value \\(X_{t+1}\\) is replaced by the naive estimate \\(X_t\\)). This forecasting approach is also not useful in the presence of trend etc. Hence only a forecast of one time period ahead is usually done. For forecasting two or more time periods ahead, methods such as double and triple exponential smoothing are more useful.\nIf we perform the double exponential forecasting for some \\(m\\) periods ahead from a point at time \\(t\\), the trend part of EWMA, \\(T_t\\), will be added \\(m\\) times to the naive level estimate \\(S_t\\). As shown in Figure 16, the double exponential smoothing approach provides no nonsense forecasts compared to the naive single exponential forecasts in the presence of trends. The fit/forecast quality measures such as the MSE, MAD etc will also be smaller for the double exponential smoothing in the presence of trends.\n\n\nCode\nholt1 &lt;- holt(credit.balance)\nholt2 &lt;- hw(credit.balance)\n\np0 &lt;- forecast::autoplot(window(credit.balance, start=2012)) + \n  xlim(2012, 2025) + ylim(4500,7000) + ylab(\"\")\n\np1 &lt;- p0 + autolayer(holt1, series = \"Double exponential\")\n\np2 &lt;- p0 + autolayer(holt2, series = \"Triple exponential\")\n\nlibrary(patchwork)\n\np1/p2\n\n\n\n\n\nFigure 16: Double and triple exponential forecasts for credit balance data\n\n\n\n\nThe forecast accuracy measures can also be obtained using the accuracy() function in the forecast package. This function also give a few other accuracy measures. For the credit card balances data, we obtain-\n\n\nCode\nbind_cols(\n  Method = c(\n    \"Double exponential smoothing\",\n    \"Triple exponential smoothing\"\n  ),\n  bind_rows(\n    accuracy(holt1)[,c(2,3,5)],\n    accuracy(holt2)[,c(2,3,5)]\n    )\n)\n\n\n# A tibble: 2 × 4\n  Method                        RMSE   MAE  MAPE\n  &lt;chr&gt;                        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Double exponential smoothing  88.5  51.3 0.993\n2 Triple exponential smoothing  72.5  33.5 0.680\n\n\nEvidently the triple exponential smoothing fares well for our credit card balances data."
  },
  {
    "objectID": "studyguide/7-multiple.html#intro-to-autoregressive-modelling",
    "href": "studyguide/7-multiple.html#intro-to-autoregressive-modelling",
    "title": "Chapter 7: Models with Multiple Continuous Predictors",
    "section": "Intro to Autoregressive Modelling",
    "text": "Intro to Autoregressive Modelling\nThe concept of stationarity plays in important role for building time series models. In crude terms, a time series is said to be stationary if the mean and variance do not change over time (alternatively the same probability law applies over time). In fact stationarity is defined in a pure mathematical way but we will not worry about this in this course.\nA white noise series is defined as a series with a constant mean and variance, and the true mean and variance remain the same for all \\(t\\). Normal random data is an example of white noise but the normal assumption is not required for a series to be white noise. You can also intuitively guess that a white noise series is stationary.\nA quick collection of EDA displays can be obtained using a single function ggtsdisplay() or tsdisplay() in R. This display is shown for the white noise series in Figure 17.\n\n\nCode\nset.seed(123)\nwht.noise &lt;- arima.sim(list(order=c(0,0,0)),500)\nggtsdisplay(wht.noise)\n\n\n\n\n\nFigure 17: A summary plots for white noise\n\n\n\n\nTime series modelling is not needed for a series such as this one. The above random series must be distinguished from a series whose autocorrelations are not decaying to zero or becoming significant frequently.\nA drifting random walk series is defined as \\[X_t = \\delta + X_{t-1} + W_t \\]\nwhere \\(\\delta\\) is the constant drift, and \\(W_t\\) is white noise which induces the random walk for the series. The mean function depends on \\(t\\) for this series and hence not stationary. This is not of concern because we can model the drift and make the residuals stationary. The trick is to model the difference \\(X_{t} - X_{t-1}\\) or just use the first lag \\(X_{t-1}\\) as a predictor in the usual regression.\n\n\nCode\nset.seed(123)\nrwd &lt;- arima.sim(list(order=c(0,1,0)),500)\nggtsdisplay(rwd)\n\n\n\n\n\nFigure 18: A summary plots for drifting random walk series\n\n\n\n\nIn Figure 18, it should also be noted that the ACFs decay to zero which is a good thing when compared to the case where ACFs are not decaying to zero.\n\\[X_t=\\beta_0+\\beta_1(\\sin(\\frac {2\\pi}{12} t)+\\beta_1(\\cos(\\frac {2\\pi}{12} t)+\\epsilon_t\\] The above model introduces a 12-period seasonal pattern using \\(\\sin\\) and \\(\\cos\\) functions (which are periodic). The time series EDA plots for this function is obtained below:\n\n\nCode\nt=1:500\nset.seed(123)\nXt=sin(t*2*pi/12)+cos(t*2*pi/12)+rnorm(500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 19: Seasonal series EDA plots\n\n\n\n\nNote the periodical patterns in the EDA plots shown in Figure 19. Analysis of a time series using sine and cosine functions is known as frequency domain approach and is popular in meteorology, chemistry and geophysics. Instead of using trigonometric functions, say if indicator variables are used to model seasonality, we stay within the time domain. The autocovariance function in the time domain is analogous to the spectral density function in the frequency domain.\nConsider the model \\[X_t=\\alpha_1 X_{t-1}+ \\alpha_2 X_{t-2}+ \\dots + \\alpha_p X_{t-p} + \\epsilon_t\\]\nThis model is called the auto-regressive model of order \\(p\\) and called the \\(AR(p)\\) process. Under this model, we assume that the present value depends linearly on the immediate past values as well as a random error. Note that this model is very similar to the multiple regression model where the predictors are just the past values of the series. This \\(AR(p)\\) series is stationary if the variance of the terms are finite. When \\(p = 1\\) (the first-order process), the model is known as a Markov process. The EDA plots for random data from this process is shown in Figure 20. Figure 21 shows the \\(AR(3)\\) process EDA plots. Note that the PACF shows a pattern matching the parameters set ar= c(0.8, -0.7, .3). The last PACF in an \\(AR(p)\\) model accounts the excess autocorrelation at lag \\(p\\) that is not accounted for by an \\(AR(p-1)\\) model.\n\n\nCode\nset.seed(123)\nXt &lt;- arima.sim(list(order=c(1,0,0), ar=.6), n=500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 20: A typical Markov series EDA plots\n\n\n\n\n\n\nCode\nset.seed(123)\nXt &lt;- arima.sim(list(order=c(3,0,0), ar= c(0.8, -0.7, .3)), n=500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 21: A typical AR(p=3) series EDA plots\n\n\n\n\nThe moving average process for errors is defined by the following equation. \\[X_t=\\beta_0 z_{t}+ \\beta_1 z_{t-1}+ \\dots + \\beta_q z_{t-q}\\] Note that \\(X_t\\) is modelled with errors \\(z_1\\), \\(z_2\\),…, whose means are assumed to be zero and constant variance. The \\(\\beta\\)s are coefficients of the model and \\(q\\) is the order. The mean of this \\(MA(q)\\) process is zero but we can always add some mean \\(\\mu\\) which will not affect the properties such as ACFs. The basic idea behind the \\(MA(q)\\) process is that the current value of the response is due to variety of current and past unpredictable random events. It is proved that the moving average process is a stationary process and that the ACFs at lags greater than \\(q\\) are zero. Figure 22 and Figure 23 show the basic EDA plots for the \\(MA(1)\\) and \\(MA(3)\\) processes.\n\n\nCode\nset.seed(123)\nXt &lt;- arima.sim(list(order=c(0,0,1), ma=.6), n=500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 22: EDA plots of a typical MA(1) process\n\n\n\n\n\n\nCode\nset.seed(123)\nXt &lt;- arima.sim(list(order=c(0,0,3), ma=c(.3, .1, -.4)), n=500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 23: EDA plots of a typical MA(3) process\n\n\n\n\nARMA Model\nThe term \\(ARMA(p, q)\\) model refers to the following equation that combines both the \\(AR(p)\\) and \\(MA(q)\\) models.\n\\[X_t=\\alpha_1 X_{t-1}+ \\alpha_2 X_{t-2}+ \\dots + \\alpha_p X_{t-p} + \\beta_o z_{t}+ \\beta_1 z_{t-1}+ \\dots + \\beta_q z_{t-q}\\] It is easy to see that the term \\(\\epsilon_t\\) in the \\(AR(p)\\) model is replaced or expanded with the \\(MA(q)\\) model. You may wonder why to have such a complicated model. In fact the ARMA model requires fewer parameters than using just \\(MA(q)\\) or \\(AR(p)\\) model. \\(ARMA(p, q)\\) model is a stationary model. Figure 24 shows the EDA plots for the simulated series from the \\(ARMA(2,2)\\) process; note the constants fixed under the ar and ma parts of the arima.sim function and compare the ACF and PACF plots.\n\n\nCode\nset.seed(123)\nXt &lt;- arima.sim(list(order=c(2,0,2), ar=c(.5, -.3), ma=c(.3, .1)), n=500)\nggtsdisplay(Xt)\n\n\n\n\n\nFigure 24: EDA plots of a typical MA(3) process\n\n\n\n\nFitting ARMA model Fitting an AR model can be done approximately using the multiple regression approach. If we use the sample mean \\(\\bar{X}\\) to estimate the mean \\(\\mu\\) of the process, the AR model becomes the multiple regression model with lags as predictors. However we cannot take the same approach to fitting ARMA models and we need to employ nonlinear optimisation methods.\nAfter fitting the ARMA model, we perform diagnostics of the fitted model. Here we explore the residuals of the fitted model for randomness and periodicity. In order to avoid over fitting, we will also examine the standard errors of the fitted coefficients. The need for transformations such the logarithm or the square root will also be indicated by the residuals.\nIf the residuals are found to be nonstationary (often the case), we opt for differencing of the series. We have briefly seen that differencing can bring stationarity to a drifting process. Formally, the first difference \\(X_t-X_{t-1}\\) is denoted as \\(\\bigtriangledown X_t\\). If we perform the differencing of the differences, we obtain \\(\\bigtriangledown^2 X_t\\) and so on. In order to bring stationarity to residuals, we may do differencing \\(d\\) times. We then fit the model to \\(\\bigtriangledown^2 X_t\\) instead of \\(X_t\\). This model is known as an autoregressive integrated moving average (ARIMA) model and denoted as \\(ARIMA(p, d, q)\\). The term “integrated” means that the stationary model that was fitted based on the differenced data has to be summed (or “integrated”) to provide a model for the original data.\nThe ARIMA model is further generalised to seasonal ARIMA (SARIMA) model. The AR part for seasons (parameter P), differencing part (D) and the MA part (Q) form part of the \\(SARIMA(p,d,q ,P, D, Q)\\). This topic is covered in higher level courses.\nBuilding good ARIMA models of Box and Jenkins (1976) generally requires a reasonable amount of experience compared to building models to cross-section data. In this course you are expected not to build ARIMA models (no exam questions). But it should not be too hard to recognise the situations such as seasonality in the data series using EDA tools.\nThe R package forecast has a convenient function called auto.arima which can quickly fit an ARIMA model. This is just an initial model which must be improved further. For the credit balance data, we obtain the following output:\n\n\nCode\nauto.arima(credit.balance)\n\n\nSeries: credit.balance \nARIMA(0,2,4)(0,0,2)[12] \n\nCoefficients:\n          ma1      ma2     ma3     ma4    sma1    sma2\n      -0.6271  -0.5941  0.0977  0.1389  0.2602  0.1775\ns.e.   0.0608   0.0729  0.0683  0.0636  0.0623  0.0570\n\nsigma^2 = 5817:  log likelihood = -1581.39\nAIC=3176.78   AICc=3177.2   BIC=3202.1\n\n\nThis package can also generate forecasts easily, see Figure 25. This plot also shows the confidence bands for the forecasts.\n\n\nCode\nfit &lt;- auto.arima(credit.balance)\nforecast::autoplot(forecast(fit,h=24))\n\n\n\n\n\nFigure 25: Forecasts for credit balance series"
  },
  {
    "objectID": "studyguide/5-tabulated.html",
    "href": "studyguide/5-tabulated.html",
    "title": "Chapter 5: Tabulated Counts",
    "section": "",
    "text": "“….the Chi-square statistical test is one of twenty important scientific breakthroughs of the 20th century…”\n— Ian Hacking (1984)"
  },
  {
    "objectID": "studyguide/5-tabulated.html#goodness-of-fit-for-distributions",
    "href": "studyguide/5-tabulated.html#goodness-of-fit-for-distributions",
    "title": "Chapter 5: Tabulated Counts",
    "section": "Goodness of Fit for Distributions",
    "text": "Goodness of Fit for Distributions\nWhen testing for the goodness of fit for a distribution, observations in various non-overlapping ranges are classified into categories and a frequency count is obtained for each category or range. These (observed) frequency counts are compared to the counts which would be obtained if the hypothesised distribution fitted the data exactly. We call these counts the expected counts. In general one assumes that the parameters of the hypothesised distribution equal the estimates obtained from the data. Thus we are effectively just testing whether the shape of the hypothesised distribution is correct.\nFor example if a set of data has a sample mean of 10 and sample standard deviation of 2 and we wish to test whether a Normal distribution is appropriate for these data, we hypothesise a Normal distribution with a mean of 10 and a standard deviation of 2. The following Chi-squared statistic can be used to test whether the hypothesised distribution describes the data reasonably well. The idea is to split the number line into, say, \\(c\\) non-overlapping intervals spanning the range of the data. The observed and expected frequency of data in each interval is then compared, using\n\\[\\chi ^{2} =\\sum _{1}^{c}\\frac{\\left({\\text   {Observed-Expected}}\\right)^2 }{{\\text   {Expected}}}  =\\sum _{1}^{c}\\frac{\\left( O-E\\right)^{2}}{E},\\]\nwhere again we sum over all \\(c\\) categories. For example, for \\(n=100\\) data we would expect 15.87% to be \\(\\leq 8\\), 34.13% to be greater than 8 and \\(\\leq 10\\) , 34.13% to be greater than 10 and \\(\\leq 12\\), and 15.87% to be \\(&gt;12\\). We would compare our observed counts with these expected ones. Note that expected counts need not be integers, and should not be rounded in the calculation of \\(\\chi^2\\), especially if they are small as that can introduce substantial round-off error into \\(\\chi^2\\).\nThe degrees of freedom for this test equals the number of data categories minus one more than the number of parameters estimated \\(c-1-2 = c-3\\) in this case). These tests were covered in your first-year paper. You should revise your notes on them. Note that we have seen in Chapter 4 that it is possible to test for normality using an tests such as Shapiro-Wilk test."
  },
  {
    "objectID": "studyguide/5-tabulated.html#example-porcine-stress-syndrome-pss",
    "href": "studyguide/5-tabulated.html#example-porcine-stress-syndrome-pss",
    "title": "Chapter 5: Tabulated Counts",
    "section": "Example: Porcine Stress Syndrome (PSS)",
    "text": "Example: Porcine Stress Syndrome (PSS)\nPorcine Stress Syndrome (PSS) can result in the development of pale, soft meat in pigs and, under conditions of stress, sudden death. It can therefore result in severe economic loss. It is believed to be controlled by a single gene and its incidence could therefore be reduced by a selective breeding program. In a survey of its incidence, the following results were obtained for four major breeds. (The presence of PSS can be detected by a positive reaction to the breathing of halothane). The observed counts are shown in Table 4.\n\n\nCode\ndpss &lt;- tibble(\n  Breed = c(\"Large White\", \"Hampshire\", \"Landrace(B)\", \"Landrace(S)\"),\n  `Halothane positive` = c(2, 3, 11, 16),\n  `Halothane negative` = c(76, 86, 73, 76)\n) \n\nlibrary(janitor)\n\ndpss_with_totals &lt;- dpss |&gt; \n  adorn_totals(\n    name = c(\"Row total\", \"Column total\"),\n    where = c(\"row\", \"col\")\n    )\n\ndpss_with_totals\n\n\n\n\n\n\nTable 4: Porcine Stress Syndrome (PSS) data\n\n\nBreed\nHalothane positive\nHalothane negative\nColumn total\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nRow total\n32\n311\n343\n\n\n\n\n\n\n\n\nExpected values are calculated assuming independence between the rows and columns. This gives the numbers in parentheses in Figure 1.\n\n\n\nFigure 1: Manual computation of ChSq statistic\n\n\n\n\n\n\n\n\n\n\nFor example the expected value in the (3, 2) cell, the number of British Landrace pigs which gave a negative test for halothane, is \\[E_{3,2} = T_3 \\times T_2 /n = 84 \\times 311/343 = 76.2.\\] The Chi-squared statistic is calculated as before using the formula \\(\\chi^2 = \\sum(O_{ij} - E_{ij})^2/E_{ij}\\). The Chi-square of 16.43 is associated with \\((4-1)\\times (2-1) = 3\\) degrees of freedom, and is therefore significant at the 5% level and at the 0.5% level. (Note that the tabulated value or critical value equals 7.81 at 5% significance level; and 12.8 at the 0.5% level).\nHence, there is strong statistical evidence that there are differences between breeds in the incidence of porcine stress syndrome. In other words, the breed is not independent of the result of the halothane test.\nNotice that the large counts in the second column (halothane negative) lead to large expected values but low contributions to the Chi-squared statistics (0.38, 0.35, 0.13 and 0.66). This is not surprising as each of these terms is divided by a large expected value. On the other hand, the small observations of the first column lead to small expected values but larger contributions to the Chi-squared statistic. Any Chi-squared contribution in excess of four (as explained previously) should be interpreted. In this example it is clear that Landrace (Swedish) pigs are more prone to porcine stress syndrome than are the other three breeds.\nAnother view of the idea of degrees of freedom can be obtained by noting that if the three expected values at the top of Column 2 are calculated (70.7, 80.7 and 76.2) then the remaining expected value can be found by subtraction from the appropriate row or column totals. Thus if the row and column totals are known, then only three numbers inside the contingency table are free before the whole table of numbers is then determined."
  },
  {
    "objectID": "studyguide/5-tabulated.html#example-paturition-in-sows",
    "href": "studyguide/5-tabulated.html#example-paturition-in-sows",
    "title": "Chapter 5: Tabulated Counts",
    "section": "Example: Paturition in Sows",
    "text": "Example: Paturition in Sows\nConsider the data on the effects of parturition in sows shown in Table 5.\n\n\nCode\ndsow &lt;- tibble(\n  Condition = c(\"Starvation\", \"Overlain\", \"Anaemia\", \"Infections\", \"Birth Defect\"),\n  `Control` = c(8, 8, 0, 3, 6),\n  `Injected` = c(10, 9, 12, 7, 2)\n  ) \n\ndsow_with_totals &lt;- dsow |&gt; \n  adorn_totals(\n    name = c(\"Row total\", \"Column total\"),\n    where = c(\"row\", \"col\")\n    )\n\ndsow_with_totals\n\n\n\n\n\n\nTable 5: Parturition in sows\n\n\nCondition\nControl\nInjected\nColumn total\n\n\n\n\nStarvation\n8\n10\n18\n\n\nOverlain\n8\n9\n17\n\n\nAnaemia\n0\n12\n12\n\n\nInfections\n3\n7\n10\n\n\nBirth Defect\n6\n2\n8\n\n\nRow total\n25\n40\n65\n\n\n\n\n\n\n\n\nIn an experiment to consider the effects of parturition in sows, 19 pregnant sows were given intramuscular injections of the drug prostaglandin while another 19 pregnant sows served as controls. Of the piglets born to the control sows, 25 died and were autopsied in the first 3 days and, for the treated sows, 40 died and were autopsied in the first 3 days. The chi square test is shown in Figure 2 and then the R output follows.\n\n\n\nFigure 2: Manual computation of ChSq statistic\n\n\n\n\nCode\n# Make the variable \"Condition\" the row names \n# so that the data frame contains only the \n# two columns and 10 data points \ndsow_dataonly &lt;- dsow |&gt; \n  column_to_rownames(\"Condition\")\n\ndsow_dataonly\n\n\n             Control Injected\nStarvation         8       10\nOverlain           8        9\nAnaemia            0       12\nInfections         3        7\nBirth Defect       6        2\n\n\nCode\ndsow_dataonly |&gt; chisq.test()\n\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  dsow_dataonly\nX-squared = 13.118, df = 4, p-value = 0.01071\n\n\nNotice that there are four cells whose expected values are less than 5 (4.6, 3.8, 3.1 and 4.9) and their contributions to the Chi-squared statistic are (4.62, 0.19, 2.78 and 1.74). Without these contributions, the Chi-squared statistic would not be significant. The conclusion should then be that there is not enough evidence to conclude that the Chi-squared statistic is significant. So there is not sufficient evidence to conclude that the cause of death depends on whether the injection was given.\nIf the Chi-square test involves many small expected counts, then we may obtain the \\(p\\)-value for the chi square statistic by Monte Carlo simulation (i.e. without relying on the Chi-square approximation). This simulation procedure involves random sampling from the set of all possible contingency tables having the same row and column totals. The R function chisq.test() can do this for you.\n\n\nCode\ndsow_dataonly |&gt; chisq.test(simulate.p.value = TRUE)\n\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  dsow_dataonly\nX-squared = 13.118, df = NA, p-value = 0.008496\n\n\nAs mentioned earlier, the Chi-square test is for actual counts and if you scale them you will get different results. For example, if we multiply Parturition counts by 100, and do the test, the conclusions will change.\n\n\nCode\ndsow_dataonly_100 &lt;- dsow_dataonly |&gt; \n  mutate(Control = 100 * Control,\n         Injected = 100 * Injected) \n\ndsow_dataonly_100\n\n\n             Control Injected\nStarvation       800     1000\nOverlain         800      900\nAnaemia            0     1200\nInfections       300      700\nBirth Defect     600      200\n\n\nCode\ndsow_dataonly_100 |&gt; chisq.test()\n\n\n\n    Pearson's Chi-squared test\n\ndata:  dsow_dataonly_100\nX-squared = 1311.8, df = 4, p-value &lt; 2.2e-16\n\n\nWe should also be careful when we combine two or more contingency tables into one. The association seen in a particular table may disappear after amalgamation. Simpson (1951) provided the theory on why this happens for contingency tables data and hence this paradox is known as Simpson’s paradox even though this was observed by others before him. Consider the following R outputs showing the ChiSq test for two separate groups and then the amalgamated group.\nFor Group 1 counts, the Chi-square test is shown below:\n\n\nCode\ngroup1 &lt;- cbind(c(80,30), \n                c(120,80)) \ngroup1\n\n\n     [,1] [,2]\n[1,]   80  120\n[2,]   30   80\n\n\nCode\nchisq.test(group1)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group1\nX-squared = 4.4809, df = 1, p-value = 0.03428\n\n\nFor group 2 counts, the Chi-square test is shown below:\n\n\nCode\ngroup2 &lt;- cbind(c(20,25),\n                c(75,20))\ngroup2\n\n\n     [,1] [,2]\n[1,]   20   75\n[2,]   25   20\n\n\nCode\nchisq.test(group2)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group2\nX-squared = 15.122, df = 1, p-value = 0.0001008\n\n\nFor the amalgamated contingency table, we obtain-\n\n\nCode\nall &lt;- cbind(c(100,55),\n             c(195,100))\nall\n\n\n     [,1] [,2]\n[1,]  100  195\n[2,]   55  100\n\n\nCode\nchisq.test(all)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  all\nX-squared = 0.053808, df = 1, p-value = 0.8166\n\n\nEvidently the association seen in the subgroups is lost after amalgamation.\nThe opposite can also happen. That is, when populations are separated somewhat parallelly by a factor like Gender, the overall association may not filter to the same level of association for the two gender groups."
  },
  {
    "objectID": "studyguide/3-probability.html",
    "href": "studyguide/3-probability.html",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "",
    "text": "“Misunderstanding of probability may be the greatest of all impediments to scientific literacy”\n— Stephen Jay Gould"
  },
  {
    "objectID": "studyguide/3-probability.html#probability-as-a-relative-frequency",
    "href": "studyguide/3-probability.html#probability-as-a-relative-frequency",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Probability as a relative frequency",
    "text": "Probability as a relative frequency\nThe naive or classical definition of probability is just a relative frequency. If \\(n\\) is the total number of equally likely possible outcomes and if \\(m\\) of those \\(n\\) outcomes result in the occurrence of some given event, then the probability of that event is \\(m/n\\).\nFor example, if a fair die is rolled, there are \\(n = 6\\) possible outcomes. Let the event of interest be getting a number 4 or more. So, the probability of this event is 3 out of 6 or \\(p=1/2\\).\nThe sample space or the set of all possible outcomes need not be finite. For example, tossing a coin until the first head appears will result in an infinite sample space.So, the probability can be viewed as a limiting or long run fraction of \\(m/n\\) (i.e. when \\(n \\to \\infty\\)).\nWhen the sample space is finite and outcomes are equally likely, we can assume that classical probability will be the same as empirical probability. Note that to find the probability of a fair coin landing heads it is not necessary to toss the coin repeatedly and observe the proportion of heads. So when the number of trails is reasonably large, empirical probability may be treated as the true value.\nSubjective probability (such as expert opinion) is useful in areas such as decision theory, and its definition does not fit as a relative frequency.\nProbabilities can only be between \\(0\\) and \\(1\\) (impossible to certain)."
  },
  {
    "objectID": "studyguide/3-probability.html#mutually-exclusive-events",
    "href": "studyguide/3-probability.html#mutually-exclusive-events",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Mutually exclusive events",
    "text": "Mutually exclusive events\nA German researcher (Witt 1956) studied the effects of various drugs on the web-building abilities of orb-web spiders (Figure 1).\n\n\n\nFigure 1: Some examples of webs made by spiders dosed with various intoxicants\n\n\nSay he classified the webs into three types:\n\ncomplete, partial, or fail.\n\nHe found the relative frequencies for the three types as follows:\n\n\\(P(W=\\text{complete}) = 0.2\\)\n\\(P(W=\\text{partial}) = 0.3\\)\n\\(P(W=\\text{fail}) = 0.5\\)\n\nThe random variable \\(W\\) has three possible states but any given observation can only belong to one state. You cannot observe a web that is both “partial” and “complete”, or “complete” and “fail”. These values or states are said to be mutually exclusive.\nFor mutually exclusive events,\n\nThe probability of any two events co-occurring is zero.\nThe probability of one event or another event occurring is the sum of the two respective probabilities.\n\nFor example, either a complete web or a partial web being observed is equal to the sum of \\(P(W = \\text{complete})\\) and \\(P(W = \\text {partial})\\). That is,\n\nThe probability of any one event not occurring is the sum of those remaining is \\[P(\\text{not complete})    = P(W = \\text{partial or}~W \\text{ = fail})= 0.3 + 0.5 = 0.8 = 1 – P(W = \\text{complete}) \\]"
  },
  {
    "objectID": "studyguide/3-probability.html#statistical-independence",
    "href": "studyguide/3-probability.html#statistical-independence",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Statistical independence",
    "text": "Statistical independence\nIf events \\(A\\) and \\(B\\) are statistically independent, then \\[P(A \\textbf{ and } B) = P(A) \\times P(B)\\]\nIf the \\(n\\) events \\(A_1, A_2, \\dots, A_n\\) are mutually independent, then: \\[P(A_1 \\textbf{ and } A_2 \\textbf{ and } \\dots \\textbf{ and } A_n) = P(A_1) \\times  P(A_2) \\times \\dots \\times P(A_n)\\]\nThe probability of event A occurring given that event B has already occurred is written in shorthand as \\(P(A|B)\\).\nFor example, the probability of a card you’ve drawn being a 5, given that it is a spade. This is known as a conditional probability.\nNote that the sample space is reduced to that where B has occurred.\nWe say that two events (A and B) are independent if\n\\[P(A | B) = P(A) \\textbf{ and } P(B | A) = P(B)\\]\nThis means that observing event A doesn’t make event B any more or less likely, and vice versa.\nFor any two events \\(A\\) and \\(B\\),\n\\[P(A \\textbf{ and } B ) = P(A|B) \\times P(B)\\] and also\n\\[P(A \\textbf{ and } B ) = P(B|A) \\times P(A)\\]"
  },
  {
    "objectID": "studyguide/3-probability.html#examples",
    "href": "studyguide/3-probability.html#examples",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Examples",
    "text": "Examples\n\nBlood group example\nThere are a large number of genetic-based blood-type systems that have been used for categorising blood. Two of these are the Rh system (Rh+ and Rh–) and the Kell system (K+ and K–). It is found that for any person, their blood type in any one system is independent of their blood type in any other.\nFor Europeans in New Zealand, about 81% are Rh+ and about 8% are K+. So, we can form a Table of counts as shown in Figure 2:\n\n\n\nFigure 2: Categorising blood types\n\n\nIf a European New Zealander is chosen at random, what is the probability that they are (Rh+ and K+) or (Rh– and K–)?\nThe answer is 0.0648 + 0.1748 = 0.2396 (verify).\nSuppose that a murder victim has a bloodstain on him with type (Rh– and K+), presumably from the assailant. What is the probability that a randomly selected person matches this type?\nThe answer is 0.0152 (verify).\n\n\nBeached whales example\nAssume that 0.63 is the probability of a randomly selected beached whale is a male. So the probability of a beached whale being female is \\(1-0.63=0.37\\). Suppose that we also observe whether beached whales are juvenile or adult, and the probability of a whale being juvenile, given that it is female, is 0.8, and the probability it is juvenile, given that it is male, is 0.4.\nLet us denote the random variables and their outcomes as follows:\nSex (S) = either female (\\(f\\)) or male (\\(m\\))\nAge (A) = either juvenile (\\(j\\)) or adult (\\(a\\))\nWe are given\n\\(P(S=m) = 0.63\\)\n\\(P(A=j | S=f) = 0.8\\)\n\\(P(A=j | S=m) = 0.4\\)"
  },
  {
    "objectID": "studyguide/3-probability.html#probability-tree-diagram",
    "href": "studyguide/3-probability.html#probability-tree-diagram",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Probability tree diagram",
    "text": "Probability tree diagram\nEasy to visualise the probabilities with this representation as shown in Figure 3.\n\n\n\nFigure 3: Probability tree diagram\n\n\n\nRules of the Probability Tree\n\nWithin each level, all branches are mutually exclusive events.\nThe tree covers all possibilities (i.e., the entire sample space).\nWe multiply as we move along branches.\nWe add when we move across branches.\n\nWhat is the overall probability that a whale is juvenile? That is, what is \\(P(A = j)\\) (or \\(P(j)\\) for short)?\n\\[P(j) = P(m \\textbf{ and } j) + P(f \\textbf{ and } j)= 0.63\\times 0.4+0.37 \\times 0.8 = 0.548 \\]"
  },
  {
    "objectID": "studyguide/3-probability.html#bayes-rule",
    "href": "studyguide/3-probability.html#bayes-rule",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Bayes rule",
    "text": "Bayes rule\nWhat is the probability of the whale being female given that it is juvenile? i.e., what is \\(P(f | j)\\) ?\n\\(P(f | j)\\) doesn’t appear in the tree anywhere, so we will have to go back to first principles.\nwe know that\n\\[P(f \\textbf{ and }  j)  = P(f | j) × P(j)\\] and\n\\[P(f \\textbf{ and }  j)  = P(j | f) × P(f)\\]\nThus, \\[P(f | j) × P(j) = P(j | f) × P(f)\\]\n\\[P(f | j) = \\frac {P(j | f) × P(f)} {P(j)}\\] This is known as the Bayes rule or theorem.\nFor our question \\[P(f | j) = \\frac {P(j | f) × P(f)} {P(j)}= \\frac{0.8 \\times 0.37}{0.548}= 0.54\\]"
  },
  {
    "objectID": "studyguide/3-probability.html#probability-massdensity-functions",
    "href": "studyguide/3-probability.html#probability-massdensity-functions",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Probability Mass/Density Functions",
    "text": "Probability Mass/Density Functions\nThe above example of the distribution of numbers of eggs of penguins is an odd case, because we’ve simply listed the outcomes and probabilities. More often, we use a known probability distribution where the probabilities of any value of \\(X\\) may be calculated using a Probability Mass Function (PMF, for discrete distributions) or Probability Density Function (PDF, for continuous distributions). A PMF or PDF is a formula that can be used to calculate the probability associated with any value; that is, \\(\\text{P}(X=x)\\), the probability that the random variable \\(X\\) will take a particular value \\(x\\). These probabilities arise from making particular assumptions about how the values arise.\nThe remainder of this chapter will discuss some of the more common probability distributions."
  },
  {
    "objectID": "studyguide/3-probability.html#discrete-probability-distributions",
    "href": "studyguide/3-probability.html#discrete-probability-distributions",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Discrete probability distributions",
    "text": "Discrete probability distributions\nDiscrete probability distributions are those that can produce a limited set of values, usually non-negative integers {0, 1, 2, 3, …}. The two most useful ones are the Binomial and the Poisson.\n\nBinomial distribution\nLet \\(X\\) be the number of heads when two coins are tossed. The possible outcomes are shown in Figure 5.\n\n\n\nFigure 5: Possible outcomes for two coin tosses\n\n\nIn general, when there only two possible outcomes say success or failure (\\(1\\) or \\(0\\); Yes or No; On or Off; Heads or Tails; etc.), we deal with a binary or Bernoulli random variable.\nThe count of the number of successes \\(X\\) out of a fixed total of \\(n\\) independent trials (in the above coin tossing example, there were \\(n = 2\\) trials) follows the binomial distribution.\nIn other words, \\(X \\sim Bin(n, p)\\), where \\(p\\) the probability of a success. The binomial probability mass function \\(P(X=x)\\) or \\(P(x)\\) is given by\n\\[P(x)={n \\choose x}p^{x}(1-p)^{n-x}\\]\nIt is easy to plot these probabilities in R. For \\(n=10\\), \\(p=0.3\\), the binomial probabilities \\(P(x)~~~ x=0,1,2, \\dots, 10\\) are plotted in Figure 6.\n\n\n\n\n\nFigure 6: Binomial probabilities\n\n\n\n\nExample: A microbiologist conducts an experiment to create a recombinant strain of bacteria that is resistant to penicillin. She plates out the bacteria on a plate, and picks out 10 colonies. She knows that the probability of successfully creating a recombinant is 0.15.\nWhat is the probability that if she mixes all 10 colonies in a growth medium with penicillin, something (anything) will grow?\nGiven:\n\\(X \\sim Bin(n = 10, p = 0.15)\\). What is \\(P(x &gt; 0)\\)?\nNote \\(P(x &gt; 0)=1-P(x = 0)\\). So in R, compute this as follows:\n\n\nCode\n1 - dbinom(x=0, size=10, prob=.15)\n\n\n[1] 0.8031256\n\n\nor\n\n\nCode\n1-pbinom(q=0, size=10, prob=.15)\n\n\n[1] 0.8031256\n\n\nNote that function pbinom(k,size=n,prob=p) gives the cumulative probabilities up to and including the quantile \\(k\\) (which is equal to zero in our example).\n\\[P(X\\leq k)=\\sum _{i=0}^{k}{n \\choose x}p^{x}(1-p)^{n-x}\\]\nThe mean and variance of the binomial random variable is given by\n\\[\\mu_X=np~~~~ \\sigma^2_X=np(1-p)\\]\nSo the expected number of recombinant strain of bacteria would be\n\\[\\mu_X=np=10*0.15=1.5\\]\nwith SD\n\\[\\sigma_X=\\sqrt {np(1-p)}=1.129159\\]\n\n\nPoisson distribution\nThe Poisson distribution is used to obtain the probabilities of counts of relatively rare events or outcomes that occur independently in space or time.\nSome Examples:\n\nThe number of snails in a quadrat (\\(1~m^2\\))\nFish counts in a visual transect (25 x 5 m)\nBacterial colonies in 2 litres of milk\n\nThe random variable \\(X\\), the number of occurrences (count), often follows the Poisson distribution whose probability function is given by\n\\[\\Pr(x)= \\frac{\\lambda^x e^{-\\lambda}}{x!}~~~ x=0,1,2,\\dots, \\infty\\]\nThe parameter \\(\\lambda\\) is the mean which is also equal to the variance.\n\\[\\mu_X=\\lambda~~~~ \\sigma^2_X=\\lambda\\]\nMain assumptions:\n\nThe events occur at a constant average rate of \\(\\lambda\\) per unit time or space.\nOccurrences are independent of one another as well as they do not happen at exactly the same unit time or space.\n\nExample: Consider the number of changes that accumulate along a stretch of a neutrally evolving gene over a given period of time.\nThis is a Poisson random variable with a population mean of \\(\\lambda=kt\\), where \\(k\\) is the number of mutations per generation, and \\(t\\) is the time in generations that has elapsed.\nAssume that \\(k = 1\\times10^{-4}\\) and \\(t = 500\\).\nFor \\(\\lambda=kt=0.05\\), the Poisson probabilities are shown in Figure 7.\n\n\n\n\n\nFigure 7: Poisson probabilities\n\n\n\n\nWhat is the probability that at least one mutation has occurred over this period?\n\\(P(x &gt; 0)=1-P(x = 0)\\) is found in R as follows:\n\n\nCode\n1 - dpois(x=0, lambda=0.05)\n\n\n[1] 0.04877058\n\n\nAnother example: The number of parasites in a host is a Poisson random variable with mean 1.5. What is the probability that there will be exactly one parasite in the host? Examine Figure 8 for the answer.\n\n\n\n\n\nFigure 8: Parasites distribution"
  },
  {
    "objectID": "studyguide/3-probability.html#continuous-probability-distributions",
    "href": "studyguide/3-probability.html#continuous-probability-distributions",
    "title": "Chapter 3: Probability Concepts and Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nA discrete random variable takes values which are simply points on a real line. In other words, there is an inherent discontinuity in the values a discrete random variable can take.\nIf a random variable, \\(X\\), can take any value (i.e., not just integers) in some interval of the real line, it is called a continuous random variable. Examples include variables such as height, weight, length, percentage protein etc.\nFor a discrete random variable \\(X\\), the associated probabilities \\(P(X=x)\\) are also just points or masses, and hence the probability function \\(P(x)\\) is also called as the probability mass function (pmf).\nFor continuous random variables, probabilities can be computed when the variable falls in an interval such as \\(5\\) to \\(15\\), but not when it takes a fixed value such as \\(10\\) (which is equal to zero).\nThe probability density function (pdf) gives the probabilities as an area under the curve.\nFor example, consider a random proportion \\((X)\\) between \\(0\\) and \\(1\\). Here \\(X\\) follows a (standard) continuous uniform distribution whose (probability) density function \\(f(x)\\) is defined as follows:\n\\[f(x)=\\begin{cases}{1}~~~\\mathrm {for} \\ 0\\leq x\\leq 1,\\\\[9pt]0~~~\\mathrm {for} \\ x&lt;0\\ \\mathrm {or} \\ x&gt;1\\end{cases}\\]\nThis constant density function is very simple; see Figure 9.\n\n\nCode\ndata.frame(x = c(-.5, 1.5)) |&gt; \n  ggplot() + \n  aes(x) +\n  stat_function(\n    fun = dunif, \n    n = 10001, \n    args = list(min = 0, max = 1)\n    ) +\n  ylab(\"f(x)\") +\n  theme_minimal()\n\n\n\n\n\nFigure 9: Constant density between 0 and 1\n\n\n\n\nThe cumulative distribution function, CDF, \\(F(x)\\) gives the left tail area or probability up to \\(x\\). This is probability is found as\n\\[F_{X}(x)=\\int _{-\\infty }^{x}f_{X}(t)\\,dt\\] The relationship between the density function \\(f(x)\\) and the distribution function \\(F(x)\\) is given by the Fundamental Theorem of Calculus.\n\\[f(x)={dF(x) \\over dx}\\]\nThe total area under the pdf curve is \\(1\\). The probability of obtaining a value between two points (\\(a\\) and \\(b\\)) is the area under the pdf curve between those two points. This probability is given by \\(F(b)-F(a)\\).\nFor the uniform distribution \\(U(0,1)\\), \\(f(x)=1\\). So,\n\\[F_{X}(x)=\\int _{-\\infty }^{x}\\,dt=x\\]\nFor example, the probability of a randomly drawn fraction from the interval \\([0,1]\\) to fall below \\(x=0.5\\) is 50%.\nThe probability of a random fraction falling between \\(a=0.2\\) and \\(b=0.8\\) is\n\\[F(b)-F(a)=0.8-0.2=0.6\\]\n\nNormal distribution\nThe Gaussian or normal distribution is parameterised in terms of the mean \\(\\mu\\) and the variance \\(\\sigma ^{2}\\) and its density function is given by\n\\[f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}\\] If \\(X \\sim N(\\mu, \\sigma)\\), its Z-score is calculated from \\(X\\) by subtracting \\(\\mu\\) and dividing by the standard deviation \\(\\sigma\\).\n\\[Z={\\frac {X-\\mu }{\\sigma }}\\] The random variable \\(Z \\sim N(0,1)\\). Its pdf is given by\n\\[f(z)={\\frac {1}{ {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}z^{2}}\\]\nWe often deal with the Standard normal because the symmetric bell shape of the normal distribution remains the same for all \\(\\mu\\) and \\(\\sigma\\); see Figure 10.\n\n\n\n\n\nFigure 10: Standard Normal Density\n\n\n\n\nExample: The weight of an individual of Amphibola crenata (snail) is normally distributed with a mean of \\(40g\\) and variance of \\(20g^2\\) as shown in Figure 11.\n\n\nCode\ndfs &lt;- tibble(\n  x=seq(20, 60, length=1000), \n  `f(x)` = dnorm(x, mean=40, sd=sqrt(20))\n  )\n\nps &lt;- ggplot(dfs) + aes(x = x, y = `f(x)`) + \n  geom_area(fill=\"gray\") +\n  geom_vline(xintercept=40) \n\nps\n\n\n\n\n\nFigure 11: Distribution of snail weight\n\n\n\n\nWhat is the probability of getting a snail that weighs between \\(35g\\) and \\(50g\\)? In R, the function pnorm() gives the CDF.\n\n\nCode\npnorm(50, mean=40, sd=sqrt(20)) - \n  pnorm(35, mean=40, sd=sqrt(20)) \n\n\n[1] 0.8555501\n\n\nFigure 12 illustrates the computation.\n\n\nCode\nps + \n  geom_area(data = dfs |&gt; filter(x &lt; 50 & x &gt; 35),\n            fill=\"coral1\", alpha=.5)\n\n\n\n\n\nFigure 12: Probability of snail weight being between 35 g and 50 g\n\n\n\n\nWhat is the probability of getting a snail that weighs below \\(35g\\) or over \\(50g\\)?\n\n\nCode\npnorm(35, mean=40, sd=sqrt(20)) + \n  pnorm(50, mean=40, sd=sqrt(20), lower.tail=FALSE) \n\n\n[1] 0.1444499\n\n\nUnder standard normal, the areas under the pdf curve are shown in Figure 13 for various situations.\n\n\n\n\n\nFigure 13: Standard normal coverage\n\n\n\n\n\n\nSkewed distributions\nThere are many probability distributions which are asymmetrical. A few of them are briefed here.\nLog-normal\nA random variable \\(X\\) is log-normally distributed, when \\(log_e(X)\\) follows normal. Alternatively, if \\(X\\) follows normal, then \\(e^X\\) follows log-normal.\nThe log-normal density function is shown below:\n\\[f(x)={\\frac {1}{x\\sigma {\\sqrt {2\\pi }}}}\\ \\exp \\left(-{\\frac {\\left(\\ln x-\\mu \\right)^{2}}{2\\sigma ^{2}}}\\right),~~ x&gt;0\\] While the R function dnorm() gives the normal density, the function dlnorm() gives the log-normal density.\nThe standard log-normal pdf and CDF are plotted in Figure 14\n\n\n\n\n\nFigure 14: Log-normal pdf and CDF\n\n\n\n\nThe mean & variance of the lognormal random variables are:\n\\[\\mu_X=e^{\\left(\\mu +{\\frac {\\sigma ^{2}}{2}}\\right)}~~~~\\sigma_X^2=(e^{\\sigma ^{2}}-1) e^{(2\\mu +\\sigma ^{2})}\\]\nWeibull\nThe probability function of the Weibull distribution is given below:\n\\[f(t;\\eta,\\beta) =\n\\begin{cases}\n\\frac{\\beta}{\\eta}\\left(\\frac{x}{\\eta}\\right)^{\\beta-1}e^{-(x/\\eta)^{\\beta}}  ~~x\\geq0 ,\\\\\n0~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  x&lt;0,\n\\end{cases}\\]\n\\(\\beta~(&gt; 0)\\) is the called the shape parameter and \\(\\eta~(&gt; 0)\\) is the called scale parameter, and different values of \\(\\beta\\) will result in different distributional shapes; see Figure 15.\n\n\n\n\n\nFigure 15: Weibull distributional shapes\n\n\n\n\nThe Weibull distribution becomes the exponential distribution for \\(\\beta=1\\).\nThe scale parameter \\(\\eta\\) is called the characteristic life because \\(\\eta\\) becomes the quantile with slightly less than two-thirds of the population (63.21%) below it irrespective of the shape \\(\\beta\\) when this distribution is used to model lifetimes.\nGamma\nThe probability function of the gamma distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\beta\\) is given below:\n\\[\\displaystyle {\\begin{aligned}f(x)={\\frac {\\beta ^{\\alpha }x^{\\alpha -1}e^{-\\beta x}}{\\Gamma (\\alpha )}}\\quad {\\text{ for }}x&gt;0\\quad \\alpha ,\\beta &gt;0,\\\\[6pt]\\end{aligned}}\\] where \\(\\displaystyle \\Gamma (\\alpha)=\\int _{0}^{\\infty }x^{\\alpha-1}e^{-x}\\,dx.\\)\nFigure 16 shows the plot of gamma density for various shape and scale parameters.\n\n\n\n\n\nFigure 16: Gamma distributional shapes\n\n\n\n\nLognormal, Weibull and gamma distributions are useful to model right skewed data, particularly lifetimes data, but we need large amount of data, say over 300, to discriminate the three fitted distributions.\nBeta distribution\nThe beta distribution is bounded on the interval \\([0, 1]\\) and parameterised by two positive shape parameters, say \\(\\alpha\\) and \\(\\beta\\).\nThe probability function of the Weibull distribution is given below:\n\\[\\begin{aligned}f(x;\\alpha ,\\beta ) ={\\frac {x^{\\alpha -1}(1-x)^{\\beta -1}}{\\displaystyle \\int _{0}^{1}u^{\\alpha -1}(1-u)^{\\beta -1}\\,du}}={\\frac {1}{\\mathrm {B} (\\alpha ,\\beta )}}x^{\\alpha -1}(1-x)^{\\beta -1}\\end{aligned} \\] where \\(\\mathrm {B} (\\alpha ,\\beta )=\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}\\). Figure 17 shows the plot of beta density for various shape parameter combinations.\n\n\n\n\n\nFigure 17: Beta distributional shapes\n\n\n\n\nWhen \\(\\alpha=\\beta=1\\), the beta distribution becomes the continuous uniform distribution. This distribution is useful to model component fractions such as percentage of protein in milk powder, and is also widely used in Bayesian analysis.\n\n\nEmpirical distributions\nData from a normal (or any other distribution) may not look very much like the expected theoretical shape (‘bell-curve’ for normal) unless there is very large amount of it. Figure 18 is for a random sample of 200 from the standard normal distribution. Compare the departures from the expected theoretical shape. Recall that we covered the ECDF curves in the last Chapter. The CDF curve is just theoretical one but fitted with data. More on this later.\n\n\n\n\n\nFigure 18: Normal(0,1) data (n=200)\n\n\n\n\nSmall sample effect For small samples, we do not expect many outliers but the ‘ideal’ shape may not be seen; see Figure 19.\n\n\n\n\n\nFigure 19: Normal(0,1) data (n=50)\n\n\n\n\n\n\nImportance of the normal distribution\nQuestion: Why do we use the Normal distribution so much? Answer: Because:\n\nMany physical, biological, engineering and economic quantities do seem to follow a Normal distribution (sometimes after a transformation of the data to an appropriate scale). An example is that the logarithm of the ‘waiting time’ between earthquakes in Wellington is normally distributed. (The use of the logarithm may worry you, but it is natural for the subject area. Note that the magnitude of earthquakes is also measured on a logarithmic scale, namely the familiar Richter scale.)\nData that are approximately Normal seem to turn up very frequently in practice. In particular this may occur because\nThe Normal distribution is often what you get (for mathematical reasons) when you add together (or average) lots of different things. This result is known as the central limit theorem (CLT). For example, test scores are calculated by adding together marks from different questions, or share market movements are the result of many individuals and firms making private decisions. There is another theory which establishes that the sampling distributions of quantiles of continuous distributions of are approximately normal for large sample sizes.\nWe shall see in this course that residuals play a large part. These are, for example, the ‘errors’ left over after fitting a regression line to data. Quite often, we expect residuals to follow a Normal distribution.\n\n\n\nNormal data in practice\nIn practice, data from a ‘Normal distribution’ may not look very much like the ‘bell-curve’ unless there is very large amount of it. This statement has practical implications but first let us check whether the statement is true. We can do this by simulating batches of data from a Normal distribution (using software) and noting the variations across the data sets. Moreover, because the Normal distribution and its bell shaped curve are so important and ubiquitous, it is instructive to apply EDA methods to Normal data, for we would certainly like EDA methods to work well for that type of curve.\nFor the first batch, we simulated 1000 numbers from a Normal distribution with mean 80 and standard deviation of 12. The numbers could represent, for example, the diastolic blood pressure of 1000 adult New Zealanders. Figure 20 shows the histogram and the boxplot of the random normal data. Even though the typical bell-shaped pattern (theoretical normal curve shown in dotted lines) is evident, the histogram is not perfectly symmetric as the numbers were chosen at random so that by chance more fall into some classifications than would be expected. Also there are a few unusual values at the both extremes (see the boxplot). In practice if we didn’t know that the data came from a Normal distribution we might investigate the three highest points to see if they were valid points or outliers. The extreme edges of a data often comprise very atypical values we would not tend to place too much weight on the conflicting evidence they give about symmetry. The mean and median of this simulated batch are somewhat closer but the median is slightly off-centred from the quartiles.Well, so far so good. Now what if we have smaller batches?\n\n\n\n\n\nFigure 20: Simulated normal data, n=100 and n=1000\n\n\n\n\nFor a normal distribution,\n68% of the values fall within 1 standard deviation of the mean\n95% of the values fall within 2 standard deviation of the mean\n99% of the values fall within 3 standard deviation of the mean.\n\nAs a special case, 50% of the values will fall within 0.6745 standard deviations of the mean. For symmetric distributions, mean equals the median. Hence the fourths (quartiles) of random data obtained drawn from theoretical normal population will be\nLower hinge = median - \\(0.6745\\times S~(\\simeq\\)lower quartile),\nand\nUpper hinge = median + \\(0.6745 \\times S~(\\simeq\\) upper quartile),\nwhere \\(S\\) is the standard deviation. Thus F-spread (or \\(\\simeq\\) the IQR) \\(\\simeq\\) 1.35\\(\\times S\\). Hence\ninner fences= median \\(\\pm\\) 2 F-spread = median \\(\\pm\\) 2.7\\(\\times S\\)\nThe probability that an observation falls between these fences is 0.993, or the probability of falling outside these fences is 0.007. Thus in our simulated batch of size 1000, we expect about 7 possible outliers but none for the simulated batch size 100. Outer fences are defined as being 2 steps (3\\(\\times\\)F-spread) away from the fourths so that\nouter fences= median \\(\\pm\\) 3.5\\(\\times\\)F-spread = median \\(\\pm\\) 4.72\\(\\times S\\)\nThe probability of a point falling outside of this range is the extremely small amount of 0.000002. It is not surprising, then, that none of the values in our simulated batches fell outside of the outer fences. If they did, we would class them as probable outliers. Of course, with distributions other than the Gaussian one, and particularly for asymmetric distributions, the probability of a value falling outside these fences is higher.\n\n\nAssessment of Normality\nThe shapes of the ECDF curves shown in Figure 21 are typical of the empirical cumulative distributions for data from Normal populations. This curve was drawn using 1000 and 100 simulated random normal data. It could perhaps be described as an S-shape on its side (or lazy or stretched out S). The curve is flatter at either end which reflects the small amounts of relative frequency in the tails of the Normal distribution. The theoretical Cumulative Distribution Function (CDF) curve for the normal distribution is also shown in Figure 21 as a smooth curve. For smaller samples from a Normal population, the ECDF curve will vary about the theoretical curve, and, of course, the variation is likely to be larger the smaller the sample; see Figure 21(B).\n\n\n\n\n\nFigure 21: Simulated normal data, n=100 and n=1000\n\n\n\n\n\n\nNormal quantile-quantile plots\nTo visually check whether a given set of data are normally distributed, one can create a normal quantile-quantile (Q-Q) plot. In this plot the quantiles of the sample are plotted against the theoretical quantiles of a standard normal distribution N(0,1). If the data, \\(Y_{i}\\), are normally distributed N(\\(\\mu, \\sigma\\)) then \\(Z_{i} = \\left(Y_{i} -\\mu \\right)/\\sigma\\) has a standard normal distribution. If we plot the quantiles of the data against corresponding quantiles of the standard normal distribution, the points should roughly lie on the straight line: \\[Y_{i} =\\mu +\\sigma Z_{i}\\] that is, a line with intercept \\(\\mu\\), and slope \\(\\sigma\\).\nWe should note that the mean and standard deviation do not appear in the formula \\(Y_{i} =\\mu +\\sigma Z_{i}\\) by coincidence. Rather, the mean and standard deviation are intimately connected to the normal distribution: if the data had not been normal then the line with these parameters would not pass through the data.\n\n\n\n\n\nFigure 22: Normal Quantile-Quantile Plot for Checking Normality\n\n\n\n\nFigure 22 shows the normal Q-Q plot for 50 random values from N(80,12). Note that the points in the middle largely plot on a line. It is natural that the tail part of the distribution shows more variation, and hence we will ignore such departures.\nThe normal Q-Q plot is presented as a normal probability plot in some software where the theoretical quantiles are replaced by the associated normal probabilities. Such normal probability plots show the sample data in the \\(X\\)-axis and the normal probabilities in the \\(Y\\)-axis. In both plots, we mainly look whether or not the points plot roughly on a straight line.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/rangitikei.RData\",\n  destfile = \"rangitikei.RData\")\n\nload(\"rangitikei.RData\")\n\np1 &lt;- ggplot(rangitikei) +\n  aes(sample = people) + \n  stat_qq() + stat_qq_line()\n\np2 &lt;- ggplot(rangitikei) + aes(y=people, x=\"\") +\n  geom_boxplot() +\n  xlab(\"\") + \n  coord_flip()\n\ngridExtra::grid.arrange(p1, p2, ncol=1) \n\n\n\n\n\nFigure 23: Normal Quantile-Quantile Plot for Checking Normality\n\n\n\n\nFigure 23 shows the normal Q-Q plot for the number of people who made use of a recreational facility (rangitikei). This plot clearly reveals that the data are not normal. The plotted points show a curved but asymmetric pattern. This pattern matches with the right skewness seen in the data (see the boxplot given in Figure 23.\nNote that normality and symmetry are not the same. If the true distribution of the data is symmetric such as uniform, rectangular or triangular distributions, then the normal Q-Q plot will show a symmetric pattern for the plotted points but they will not plot on a straight line. As an example, the normal Q-Q plots for batches of random data from Student’s \\(t\\) (to be covered later in the next Chapter) and few other symmetric distributions is shown in Figure 24. Clearly normality is rejected for other symmetric non-normal distributions.\n\n\n\n\n\nFigure 24: Normal Quantile-Quantile Plot for Checking Normality\n\n\n\n\n\n\nSome non-normal distributions\nFor a normal distribution, which is specified by its mean \\(\\mu\\) and variance \\(\\sigma^2\\), parameter estimation is fairly straight forward. The sample mean and variance become the estimates. The Poisson parameter \\(\\lambda\\) is also the mean of the distribution and hence the sample mean becomes the estimate. On the other hand, distributions such as beta, and gamma involve shape and/or scale parameters and hence their estimation becomes complicated. The population mean and SD of these distributions are functions of the distributional parameters. Data from skewed distributions are not well represented by the sample mean and SD.\nIn order to estimate the distributional parameters, we follow a method called the Maximum Likelihood (ML) method. The likelihood function (or simply the likelihood) is based on the joint probability of the observed data as a function of the distributional parameters. The ML method maximises the likelihood function to estimate the distributional (model) parameters. There are also other methods of estimation such as the method of moments and we will not delve into the theory behind these methods in this course. We will simply use R packages such as VGAM or fitdistrplus to estimate the parameters.\nExample: Your are familiar with the rangitikei dataset which gives the number of people who made use of a recreational facility. This variable is right skewed, and let us fit a lognormal distribution to this variable using the fitdistrplus package.\n\n\nCode\nlibrary(fitdistrplus, exclude = c(\"select\", \"area\"))\n\n\nWarning in rm(list = exclude, envir = env): object 'select' not found\n\n\nWarning in rm(list = exclude, envir = env): object 'area' not found\n\n\nCode\nfitdist(rangitikei$people, \"lnorm\")\n\n\nFitting of the distribution ' lnorm ' by maximum likelihood \nParameters:\n        estimate Std. Error\nmeanlog 3.775514  0.1789851\nsdlog   1.028191  0.1265611\n\n\nThe adequacy of the lognormal distribution fitted, can be examined using graphical displays as in Figure 25.\n\n\nCode\nlnormfit &lt;- fitdist(rangitikei$people, \"lnorm\")\nplot(lnormfit)\n\n\n\n\n\nFigure 25: Log-normal fit for people\n\n\n\n\nThe lognormal fit does not appear too bad to people data. How about fitting gamma distribution instead of lognormal? This fit can again be accomplished using the fitdistrplus package.\n\n\nCode\nfitdist(rangitikei$people, \"gamma\")\n\n\nFitting of the distribution ' gamma ' by maximum likelihood \nParameters:\n        estimate  Std. Error\nshape 1.14299006 0.249453814\nrate  0.01593588 0.004314727\n\n\nThe EDA plots to assess the gamma distribution fit are shown in Figure 26.\n\n\nCode\ngammafit &lt;- fitdist(rangitikei$people, \"gamma\")\nplot(gammafit)\n\n\n\n\n\nFigure 26: Gamma fit for people\n\n\n\n\nNote that this package gives the estimated gamma parameters in a reparametrised form. For small and medium size datasets \\((n&lt;300)\\), we may not be able discriminate between gamma & Weibull type of right skewed distributions.\nWe can of course fit a gallery of probability distributions but we have to select the best fitting distribution, if it exists.\nThe function descdist() in the fitdistrplus package will obtain a plot of kurtosis, a measure of excessive peakedness, against the square of a measure of skew for a number of probability distributions. This plot is known as the Cullen and Frey plot. We can visibly judge the best fitting distribution using this plot. For the number of people data, this plot is shown in Figure 27.\n\n\nCode\ndescdist(rangitikei$people)\n\n\nsummary statistics\n------\nmin:  4   max:  470 \nmedian:  46 \nmean:  71.72727 \nestimated sd:  86.28089 \nestimated skewness:  3.3264 \nestimated kurtosis:  17.11618 \n\n\n\n\n\nFigure 27: Cullen and Frey plot for people\n\n\n\n\nThe presence of a large outlier in peoples data particularly affect the measures of skewness and kurtosis. So Cullen and Frey plot does not particularly strongly support the lognormal fit but suggests the lack of fit for symmetrical distributions such as normal (read corresponding to zero skew).\nWhen the observed data is a mixture from two or more distributions or contain a large number of unusual observations, no single distributional fit will be satisfactory. For example, count data may not fit the Poisson distribution due to excessive zeros.\nWe will cover the power transformation of data in the next Chapter, which is a remedy for dampening the effect of outliers and mixture of subgroups (distributions)."
  },
  {
    "objectID": "studyguide/1-data-collection.html",
    "href": "studyguide/1-data-collection.html",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "",
    "text": "“Data! data! he cried impatiently, I can’t make bricks without clay.”\n— Sir Arthur Conan Doyle, The Vopper Beeches"
  },
  {
    "objectID": "studyguide/1-data-collection.html#categorical-or-qualitative-data",
    "href": "studyguide/1-data-collection.html#categorical-or-qualitative-data",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Categorical or Qualitative Data",
    "text": "Categorical or Qualitative Data\n\nNominal Data\nAt this level, objects are classified by qualities or characteristics, and are placed in categories. Often these categories are really only names, for example Ford Focus, Mondeo, or Mustang; Boeing 737, 747 or 767, etc. Hence we describe the data as ‘nominal’. Sometimes confusion arises because the names are replaced (coded) by numbers as in the aeroplane example, or to facilitate data entry into a computer. An example of the latter would be a person’s marital status, coded as 1 (never married), 2 (currently married), 3 (widowed) or 4 (separated or divorced). It would make no sense to perform any mathematical manipulation on these numbers for they are only abbreviations for the categories. Notice that categories should cover all possibilities so that a further category may be needed, namely 5 (in a de facto relationship).\n\n\nOrdinal Data\nOrdinal data are categorical but the levels have an intrinsic order to them. Typical examples are a 3-point scale for burns - mild, moderate, severe, or a 5-point agreement scale - strongly disagree; disagree; no opinion; agree; strongly agree.\nThe distances between any two responses may differ. For example, disagree may be perceived by some respondents, although not others, as closer to strongly disagree than to no opinion. For this reason, there are problems in aggregating such responses. There are ‘nonparametric’ statistical methods to cope with these problems. It may be feasible, however, to add or average responses if the sample size is reasonably large and the assumption can be made that the differences in usage of the measures will average out over the sample."
  },
  {
    "objectID": "studyguide/1-data-collection.html#quantitative-i.e.-measured-or-counted-data",
    "href": "studyguide/1-data-collection.html#quantitative-i.e.-measured-or-counted-data",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Quantitative (i.e., measured or counted) data",
    "text": "Quantitative (i.e., measured or counted) data\n\nDiscrete Data\nDiscrete data usually arises by counting events or objects. Because of this discrete data are usually integers, but not always. For example, one may count the number of accidents last year at a particular intersection. On the one hand you can’t have fractions since you can’t have half an accident. On the other hand a further analysis may require you to state the proportion of the accidents that were nose-to-tail. Then if there were n accidents, the proportion of nose-to-tail ones could be \\(\\frac{0}{n}, \\frac{1}{n}, \\frac{2}{n}, ... ,\\frac{n}{n}\\) - a finite range of values, so the proportion is still discrete.\n\n\nContinuous Data\nWithout going into the mathematical definition of continuity, we tend to visualise continuous data as having a flowing nature, like water flowing under a bridge, or time passing in a continuous fashion. So one could in theory measure amounts (of water or time) to any level of precision, with many decimal places. Of course in practice, the measured responses will rarely be exactly continuous for, even if the underlying variables are assumed continuous, the instrument used to take readings will only be able to do so on a discrete scale. If the variable of interest is time, responses may be integers (say) 1, 2, … seconds or if the instrument can distinguish to the nearest \\(\\frac{1}{100}\\) of a second, the response will still be discrete but is now 1.00, 1.01 or 0.99. What we can say is that if the unit of measurement is sufficiently small then each individual possible reading will have an infinitesimal probability of occurring and we can basically ignore such individual points and only talk about probability in terms of intervals (e.g. one-second intervals rather than individual 10.99 seconds, say). Thus the practical difference between discrete and continuous data is that with real discrete data, outcomes each have their own separate probability of occurring, but with continuous data, probability is only thought of as associated with intervals.\n\n\nInterval and ratio scales\nStatisticians sometimes further distinguish among quantitative variables as being interval or ratio data. In fact some theoreticians write at length about the ‘four levels of measurement’: nominal, ordinal, interval or ratio.\nFor interval scale responses are assumed to be points on a linear scale. For example on a temperature scale the difference between 10 and 20 degrees is the same as between 30 and 40 degrees. On the usual Fahrenheit or Celsius scale there is a problem with respect to the origin, or zero point, as 20 degrees cannot be assumed to be twice as hot as 10 degrees. It would be meaningful to take differences between measurements or add them together but not to divide or multiply them. In many statistical formulae, deviations from means are involved which obviates the need for an absolute zero, and ratios such as slopes are quite acceptable.\nRatio scales are sometimes regarded the highest level of measurement, and are usually referred to by saying there is a fixed (or absolute) zero. An example would be the number of people in a room (can’t have negatives!) or the time taken to complete a given task. In this case, a reading of 4.2 seconds is twice that of 2.1 seconds. That is, it is meaningful to perform all the mathematical operations of addition, subtraction, multiplication and division.\nFrom a practical point of view, the type of data (categorical - nominal or ordinal; discrete; or continuous) may be crucial to the statistical analysis. It may be convenient to use a statistical method appropriate to ratio level data in the case mentioned above of the time to complete a given task, but the results may also be analysed by a non-parametric technique which actually only requires ordinal data. Similarly it may be satisfactory as a first step to analyse ordinal data by assigning scores {1, 2, 3, 4, and 5} to the categories and running the data through a regression package that assumes Normal data (better approaches are available!). Indeed, it may be difficult in some circumstances to clearly decide on the appropriate level, and the distinction particularly between interval and ratio levels is often unimportant."
  },
  {
    "objectID": "studyguide/1-data-collection.html#measuring-devices-or-instruments",
    "href": "studyguide/1-data-collection.html#measuring-devices-or-instruments",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Measuring Devices or Instruments",
    "text": "Measuring Devices or Instruments\nAny device used to collect measurements is called an instrument. It may be a physical device such as a measuring rule to gauge the heights of plants or a counting device such as a Geiger-counter for measuring radioactive material. In the social sciences or marketing, the instrument may be such a physical device or it may be a questionnaire which requires a more subjective response.\n\nMeasurement Error\nMeasurement error may arise if the instrument tends to give a reading which is too high or too low. Instruments should be checked as often as possible and standardised by checking them against known standards. This is not really possible with questionnaires but one should note the need for pilot testing the questionnaire on a group of subjects who are not in the frame for the survey.\n\n\nIndirect Measures\nIt is a common mistake to assume that the values reported for a variable are direct measurements of that variable. It is more usually the case that the recorded value reflects the desired value but is only an indirect measure of it. A doctor may wish to have some measure of the health of a patient, and, for this reason, a blood pressure measurement is taken, but blood pressure is not a direct measure of health for there are many inherited, climatic and individual effects which also play a part in the blood pressure reading. Even then a typical blood pressure reading would be required, but the mere fact that a measurement is being made will have an effect on the patient and possibly affect the reading. The stress of being in a hospital or a doctor’s surgery may increase the measurement.\nIt should be noted that the instrument used will only rarely be able to give a direct reading. Temperature is often gauged by the expansion of mercury in glass tube which is an indirect measure and will depend on a number of factors and assumptions, perhaps the greatest being that the expansion of the mercury is linear with temperature. In most experiments the measurements, even if indirect, are closely correlated with the variable under study but such assumptions should not be accepted too readily.\nAnother question which should be asked of data is who did the recording? Often, questionnaires require that people provide the information themselves, that is by self-reporting. Conscious or sub-conscious influences may have some bearing on the numbers provided. If someone is asked his/her income or workload, the answer may depend on what is perceived by the respondent as the aim of the survey, who is asking the question and who will have access to the results. A seemingly direct question may evoke an indirect answer. If the data is collected by a researcher or technician there may be some possible hiccups on the interface between the respondent and the researcher or the instrument and researcher."
  },
  {
    "objectID": "studyguide/1-data-collection.html#range-of-instruments",
    "href": "studyguide/1-data-collection.html#range-of-instruments",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Range of Instruments",
    "text": "Range of Instruments\nThe range of instruments is immense and in the remainder of this section a few examples will be given.\n\nObservation\nIt is common in the study of animals or birds to observe them in their natural settings. This may be done directly such as counting the number of wingbeats per minute or it may be of an indirect approach such as counting the frequency of spore in order to determine the number of animals in a given area. This method of data gathering is often costly and time-consuming. It may help to answer what is occurring rather than why.\nAs there is a high labour content with this instrument, errors may occur due to tiredness and boredom and coding errors may be a problem depending on how well defined are the categories to be observed. If many observers are involved, there is the added difficulty that the same behaviour may be perceived differently and hence coded differently by different observers.\nObservation does have an important part to play in a preliminary study to decide on the areas which require further study. Some researchers tend to bypass or shorten this phase by using a multivariate package on the computer hoping that it will sort out important relationships from a morass of complex data. It is surprising how many novel and correct scientific processes were discovered by such painstaking workers as Darwin and Mendel using only visual observation.\n\n\nInterviews\nThis method can suffer from some of the problems of observation in that it can be time consuming, involve coding errors and be influenced by different interviewers making different judgements.\nInterviewers need to be carefully trained so that they allow the person being interviewed to express his or her own views rather than interposing their own feelings or prejudices. On the positive side, a trained interviewer can sense nuances behind supplied answers in a way that observations or questionnaires cannot. A much richer range of responses can then be recorded which makes the interview ideal for pilot studies in particular as it can open up new avenues of study.\n\n\nQuestionnaires\nQuestionnaires are a common method used to collect data and they are very versatile as they can be administered by mail, telephone or face to face. Mailed questionnaires have the advantage of being efficient time-wise for a researcher and may be less threatening to the respondent as they can be answered when and where the respondent chooses. They are appropriate if the respondent needs to give some thought to the questions or where the answers involve sensitive information. Unfortunately, this flexibility can lead to respondents procrastinating or not replying at all. The problem of non-response will be considered later.\nHow long should the questionnaire be? Almost invariably, it is too long as the researcher strives to obtain as much information as possible. A long and tiresome questionnaire may be counter-productive as the quality of the data may suffer. There has been a lot of research on questionnaire design. Some points to note are that straightforward questions should be asked near the beginning so that the respondent feels at ease while questions requiring judgements or moral decisions should come later."
  },
  {
    "objectID": "studyguide/1-data-collection.html#the-human-interface",
    "href": "studyguide/1-data-collection.html#the-human-interface",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "The Human Interface",
    "text": "The Human Interface\nMention has been made of some problems associated with respondents giving replies and researchers seeking to understand those replies and to code them. It is worthwhile taking more time to reconsider these processes for the quality of data will invariably be greatly affected by the human interface with the research instrument.\nThe yields of different varieties of field grasses would seem to be a clear cut and well defined and measured quantity but the yield would be influenced by many factors such as the height of the cut, perhaps the speed at which the mower is operated and the ability of the assistant to mow within the specified boundaries.\nWith more subjective instruments and human respondents other problems may arise and affect the quality of the data.\n\nHuman Respondents\nA number of possible difficulties may arise:\n\nA question, which is clear-cut and unambiguous to the researcher, may not be so to the respondent who may not understand the question or is not too sure about it because it is framed in unfamiliar language or relates to something outside his or her experience.\nThe response may depend on a number of extraneous factors such as how cheerful and at ease the respondent feels.\nQuestions may require remembering certain facts or feelings and memory can be very selective.\nThe situation may involve many variables while the respondent is asked to respond on only one dimension, which may be perceived as a composite of these other variables. The respondent may not be able to make the judgements necessary to give an accurate response. In one questionnaire, some questions may be answered very accurately while others may be of doubtful value. If responses are added in some way to give a total, there may be a problem in that some of the components have been measured accurately but others are less precise.\nQuestionnaires can be self-fulfilling prophecies in that the structure and language of the questions suggest the correct or desired responses. Socialising factors may prejudice the results in that most people are hesitant to be rude or to reveal to strangers their deep desires and true feelings, particularly if these are thought to be socially undesirable.\n\n\n\nResearchers\nThe human interface also arises when someone administers the instrument of the research and some points to note are:\n\nThe competence of the researchers and assistants is important. Judgement may be required to understand the response and to make a note of it. In an interview situation, the researcher must listen and try to understand the responses in terms of his or her framework of judgements and in relation to the aim of the interview. With more objective instruments, they must be read accurately but even with such measuring scales one assistant may tend to read high or to record even calibrations whereas another would not do so, which introduces an unwanted variation in the data collection process.\nThe feelings and beliefs of the assistant may intrude into the collection process. The interaction with the subject in this way may introduce other unwanted variation.\nIt is a salutary point to note that usually the individual who collects the raw data is an underpaid and possibly under-trained junior who is not highly motivated to produce high quality data.\n\n\n\nSelf-Administered Tests\nIn some situations, the subject records the response as in self-administered questionnaires, for example, tick a box or circle a number. There may be advantages in this for the respondent is presumably the best person to know what response should be given. On the other hand, the respondent is usually not trained so that there may be a higher incidence of coding errors and questions which are left unanswered. On the other hand, if the researcher does the coding, unwarranted assumptions may be made.\nIt is conceivable that many human interventions and possible errors may occur in an experiment. For example, a field worker may make certain observations which are studied by another assistant who decides on categories for them while a further assistant codes these categories and finally another person enters them into a computer. Errors are possible at each step along the way.\nAt the other extreme, it is possible for information to be relayed directly to the computer without human interference. Microprocessors can be used to collect and transmit a whole variety of information. In experiments on animals, sensors can be placed on different parts of their bodies to relay information to the computer. These facilities open up new vistas and challenges for statisticians to monitor large amounts of data, for it may not be obvious how to cope with them."
  },
  {
    "objectID": "studyguide/1-data-collection.html#non-response-bias",
    "href": "studyguide/1-data-collection.html#non-response-bias",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Non-response Bias",
    "text": "Non-response Bias\nNon response is very common in all types of data collection and is probably the largest source of non-sampling error. It can occur at three stages of the process. An element may be selected but not found. For example, sheep in a flock may be tagged with individual identification number but one may not be found at the time of the survey. It may be hiding or perhaps has died. If the elements are people, some may not be at home at the time, or, if it is a telephone survey, the telephone may be cut off from certain selected individuals.\nIt may not be possible to take a measurement. This happens quite often with people as some may forget, or refuse, to answer the questionnaire. The measuring instrument may not work or give a ludicrous response which may be deleted by the technician. An element may be contacted, a measurement taken but the result not written down in the correct place, or could be so illegible that it cannot be used.\nWherever possible, nonresponse bias should be reduced by making every possible effort to obtain a response. When interviewing people about a particular product, market researchers will often make two call backs (that is three attempts in all) to reach a subject if that person is not at home on the first call. Cost factors will often prohibit further attempts. Surveys by mail often include a stamped addressed envelope or even money or a token for merchandise to arouse moral scruples and force a reply.\nThe problems arising from nonresponse can be illustrated with a simple example: Suppose that 200 people are asked their opinion on a certain issue, only 100 reply of whom 70 are in favour, 30 opposed.\nWhat percentage of the original 200 are in favour?\nThe answer depends on the assumptions made.\n\nSuppose the nonrespondents feel the same about the issue as the respondents. (This is the usual assumption made). This gives:\n\n\n\n\nCase\nIn favour\nOpposed\nTotal\n\n\n\n\nRespondents\n70\n30\n100\n\n\nNonrespondents\n70\n30\n100\n\n\nTotal\n140\n60\n200\n\n\n\nThus the estimate of the number in favour is then 140/200 or 70%.\n\nSuppose that those who did not respond did so because they were opposed to the issue. Thus\n\n\n\n\nCase\nIn favour\nOpposed\nTotal\n\n\n\n\nRespondents\n70\n30\n100\n\n\nNonrespondents\n0\n100\n100\n\n\nTotal\n70\n130\n200\n\n\n\nThe estimate of the number in favour is 70/200 or 35%.\n\nIt may be reasonable in other circumstances to assume that the nonrespondents were quite happy with the status quo and were in favour. Then the estimate of the number in favour is 170/200 or 85%.\nIt may be possible to make other reasonable assumptions if additional information is at hand on the whole batch of 200. We can massage the results in different ways but none of them is very convincing. The moral is: Try to reduce nonresponse by all legitimate means."
  },
  {
    "objectID": "studyguide/1-data-collection.html#some-terminology",
    "href": "studyguide/1-data-collection.html#some-terminology",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Some Terminology",
    "text": "Some Terminology\nIn a census, every element of the population is contacted, counted and other quantities collected or evaluated. Conceptually, the procedure is straightforward and perhaps trivial. In practice and particularly if the population is large, difficulties often arise. There are three critical operations in census taking, namely identifying an element as belonging to the population under study, making contact with that element and obtaining the appropriate information in a suitable form.\nIt may be helpful, at this point, to introduce some terminology to formalise the process of taking a census. The population at which the study is aimed is termed the target population . To obtain information from this population it is necessary to have a means of operationalising the data collection. This involves a frame, which may be a listing of the elements of the population or it may be an operating method such as a geographical map, suitably divided into smaller, manageable areas. A further step is necessary to contact each element of the frame and collect the appropriate information. The resulting set of elements on which completed, usable data has been collected could be called the actual population. It is possible to consider these populations as consisting of the measurements under study rather than the elements, be they objects, animals or people. More will be said on this later.\nCensus taking can then be represented by the sequence\ntarget population \\(\\Rightarrow\\) frame \\(\\Rightarrow\\) actual population.\nIdeally, these three populations would consist of the same elements but in practice they could be represented in Figure 1).\n\n\n\nFigure 1: Relationship between the target population, frame and actual population\n\n\nA simple example of a census would be the exercise of collecting information from the students in a particular school. Suppose the school authorities want to discover, from each student, the number of younger siblings who may wish to attend the school in future years. At first glance, the target population is well defined, as the current students in the school. However queries could arise such as whether adult or part-time pupils should be included. The most suitable frame would, no doubt, be the school roll. Hopefully, the roll would accurately reflect the students currently at the school but it may not exactly match the target population. Some students may be temporarily visiting the school, either staying with relatives or a parent who has a local, temporary job. On the other hand, some students on the roll may be visiting another part of the country, or may be in a hospital or other institution, and there may be some doubt whether their absence is temporary or permanent.\nFrame inefficiencies may well occur to the extent that the frame does not match the target population. The collection of information on siblings may encounter snags as some students may not respond due to absence, or parents or student may refuse to divulge the required information. The actual population may then fall short of the frame, which contains the maximum possible elements, which in this case, are students."
  },
  {
    "objectID": "studyguide/1-data-collection.html#elements-measures-and-associated-variables",
    "href": "studyguide/1-data-collection.html#elements-measures-and-associated-variables",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Elements, Measures and Associated Variables",
    "text": "Elements, Measures and Associated Variables\nWith a census, there is usually some ambivalence as to whether the objects under study are the elements of the population (such as plants, animals or people) or the measurements on these elements. The measurements may simply be the count, or may be such variables as the length, weight, salary or any other measure. The selection process will focus mainly on the elements, but from then on attention will be focussed on measurement and then analysis. These two ideas can be incorporated into the notation \\[U=\\left(L,Y,X\\right)\\] Here \\(U\\) denotes the population, \\(L\\) is the set of labels \\((i = 1,2,...,N)\\) attached to the \\(N\\) elements of the population, \\(Y\\) is the measurement, or response, under study from each element in the population, and \\(X\\) is an associated variable measured on each element in the population.\nFor example, for the census carried out on the students of a high school, the labels \\(i\\) could be the identification numbers of the students on the roll \\(L\\). The measurement on the \\(i^{th}\\) student, represented by \\(y_i\\), may be a variable such as the daily expenditure at the school tuck shop. The associated variable, \\(x_i\\), for the \\(i^{th}\\) student may be the student’s weight. On the other hand, the mark a student gets on a standardised test when entering high school may be \\(x_i\\), with \\(y_i\\) the school certificate mark of that student in mathematics.\nIn this example the survey may, in practice, be carried out as a sample rather than a census but conceptually it would be possible, to carry out such a census. Another point which may need discussion and clarification is whether the population represented by \\(U\\) should refer to the target population, the frame or the actual population; usually \\(U\\) would refer to the target population."
  },
  {
    "objectID": "studyguide/1-data-collection.html#non-sampling-errors",
    "href": "studyguide/1-data-collection.html#non-sampling-errors",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Non-sampling Errors",
    "text": "Non-sampling Errors\nA number of errors may occur in collecting information even when a census is being taken. The elements of the census must be contacted, a measurement or observation taken and this information recorded. As no sampling procedure is involved, it seems reasonable to label errors as non-sampling errors.\nThe goal of the survey will be to produce summary statistics such as the population mean and standard deviation. If errors are committed in the collection process, the summary statistics may be incorrect. It is possible that these errors will cancel out but we cannot rely on this. The errors may lead to a tendency for the calculated mean to be too low and the mean is said to be biased. Alternatively, the bias may be on the high side and, at times, it may be difficult even to decide the direction of the bias although we know there is a strong possibility of it occurring."
  },
  {
    "objectID": "studyguide/1-data-collection.html#selection-bias",
    "href": "studyguide/1-data-collection.html#selection-bias",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Selection Bias",
    "text": "Selection Bias\nIt may seem a contradiction in terms to speak of errors in selecting elements of a census for the aim is to contact every element in the population. From the discussion in the above section, it is clear that some difficulties may arise in the sequence from target population to frame and these difficulties are often called frame inefficiencies.\nA survey may be carried out on all exporting firms in New Zealand. The frame may be a published list of exporting firms but the list may be out of date as some firms have gone broke or been taken into receivership. There may be firms not on the list but which, nevertheless, do engage in some form of exporting besides their other activities.\nOnce the elements of the frame have been located, there remains the step of extracting information from them. For some reasons particular firms in the frame may not wish to take part in the survey so that errors may occur in this second step also."
  },
  {
    "objectID": "studyguide/1-data-collection.html#inference",
    "href": "studyguide/1-data-collection.html#inference",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Inference",
    "text": "Inference\nIt is not appropriate to carry out any statistical test or form a confidence interval on census data because they are measurements on the whole population. Statistical inference is using data from a sample to make statements about the broader population from which the sample was taken. When we do a census, we have the whole population, so we are no longer making inferences.\nNext, we’ll consider sampling from the population instead of taking a census."
  },
  {
    "objectID": "studyguide/1-data-collection.html#introduction-1",
    "href": "studyguide/1-data-collection.html#introduction-1",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Introduction",
    "text": "Introduction\nSampling as a scientific exercise is a peculiarly twentieth century phenomenon despite the fact that sampling has been used in an ad hoc manner from earliest times. Sampling, as in sample surveys, was accepted only after many years of argument and counter-argument. The most intensive and extensive discussion took place at the International Statistical Institute (ISI) meeting in Berne in 1895. Kiaer explained the ‘representative method’ he had used in Norway. Further meetings of the ISI enabled Kiaer to expand and clarify his approach. Four principles have been identified in Kiaer’s reasoning:\n\nRepresentativeness in terms of adequate representation of identifiable groups in the population.\nSelection of objects to be as objective as possible.\nThe reliability of the results should be assessed. Each survey should be divided into a number of distinct parts using a different representative method for each. Comparison of results of these parts would provide evidence as to how much faith could be placed in the results of the survey.\nFinally, Kiaer insisted on a complete specification of the method of selection.\n\nBowley (1906) provided a theory of inference for survey samples which helped to answer the question of how accurate an estimate from a large sample was. Using a Bayesian argument, his approach was only valid for the case where the chances were the same for all items of the groups to be sampled.\nIt was not until Neyman (1934) that random, rather than purposeful, sampling was given a strong mathematical justification. Neyman showed that it did not matter if equal or unequal probabilities were used provided that probabilities were known in advance. His arguments were not based on specific distributions as in Bayesian approaches so that the touchstone of randomness seemed to have finally won the day. For an interesting discussion on the controversies in the history of survey sampling, see Brewer (2013)."
  },
  {
    "objectID": "studyguide/1-data-collection.html#why-sample",
    "href": "studyguide/1-data-collection.html#why-sample",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Why Sample?",
    "text": "Why Sample?\nThere is a widespread belief that a census, rather than a sample, should be taken if at all possible. Most people feel secure with a census but are somewhat uneasy about only collecting a sample and using mathematical means to estimate properties of the population. Lack of sufficient resources of money, trained manpower and so on is often the motivation to reject a complete enumeration. A compromise, of course, is necessary in that a cheaper, quicker collection method using a sample may result in less accurate estimates. Often, however, the reduction in accuracy is slight compared with the considerable reduction in cost. Time is also a factor in that a census may require officials to collect data over a long period which may raise difficulties in that the responses would be meant to relate to a particular point in time.\nAn important point which is often overlooked is that a complete enumeration is only superior to a sample if the data is collected and processed at the same high standard. Often, researchers try to collect too much data for the resources and trained man-power available which results in a large amount of data whose accuracy and validity is suspect. A well collected sample will always be of more use than a shoddily taken census. Another common fault is the attempt to collect too much information from every element, be it an object, animal or person. A questionnaire has a tendency to expand to the point that it frustrates or annoys the recipient or the personnel administering it. In countries where monthly household surveys are held, their results are invariably more accurate than the less frequent population censuses.\nOne classical case in which a sample is preferable to a census is that of destructive sampling in which each object is tested until it breaks or expires. For example, testing the number of hours a light bulb shines before it burns out. The selected items are useless after the test so that there are compelling reasons for limiting the size of the sample. It is salutary to realise that many people selected to give information or to take part in tests or experiments will be affected in some ways. The process of collecting data will itself affect the elements in the sample so that they may no longer be representative of the population about which inferences are to be made. For example, in some quarters, there is a fear that political opinion polls do not merely reflect public opinion but to some extent form it.\nNonsampling errors, nonresponse in particular may introduce biases. Unfortunately, the size and/or direction of these biases are generally unknown. Due to the smaller size of a sample, it may be much easier to reduce nonresponse than in a much more cumbersome enumeration. Call-backs and other procedures which have been devised to reduce nonresponse are very time consuming so that they are rarely feasible when the number of elements is large.\nTo reduce unwanted effects such as interviewer biases, statistical tests can be employed. These require certain assumptions about the population and selection method which have been used. We can never be sure that the assumptions associated with these tests are valid. If the sample suggests that they may not be true, some action must be taken such as a data transformation to make the results more ‘Normal’."
  },
  {
    "objectID": "studyguide/1-data-collection.html#the-case-for-randomisation",
    "href": "studyguide/1-data-collection.html#the-case-for-randomisation",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "The Case for Randomisation",
    "text": "The Case for Randomisation\n\n“Randomization is too important to be left to chance.”\n– J. D. Petruccelli\n\nThe sample should be similar to the population. It is easier to state this than it is to define what is meant by the statement and how to achieve this similarity. Randomisation can be used to obtain a sample which, on average, has similar properties to the population. Not only is this so but it is possible to estimate how closely the sample reflects the population and attach a probability to the possible discrepancy. It is not possible to state with certainty that a particular measurement from a sample is peculiar although it would be possible to attach a probability to such an unusual value. Similarly, it would be possible to make a probability statement about the sample itself if the sampling process was replicated.\nIf the distribution of the variables is known, randomisation enables the distributions of some combinations of the sample values to be calculated. In common with other areas of statistics, it is more likely that the distributions of these variables are not known although these distributions may be hypothesised. The validity of any conclusions will be subject to the proviso that the assumptions are correct. If the distributions are not known the Central Limit Theorem (CLT) may be invoked so that the approximate Normal distributions may be assumed for some estimators.\nOn the other hand, there are enticing reasons for placing restrictions on the sample to make it similar to the population in many respects such as proportions in certain subgroups. Surveys are rarely carried out as a theoretical exercise and results may only be believed if those who commissioned and financed the survey believe in its accuracy. If the sample appears peculiar in relation to some known variables, the validity of the survey may be called in question. To obtain a representative sample to fit the known, or assumed, distribution in the population, the elements may be selected purposely. The resulting estimators of variables may be very accurate but there is no way of determining how accurate they are.\nAs a compromise, it is possible to use a randomisation scheme with certain restrictions. A simple example is that of stratified sampling with different probabilities of selection for each stratum. This procedure can be extended to allow each element in the population to have its own probability of inclusion into the sample. Alternatively, a randomisation approach can be used to draw a sample which may be rejected, however, if certain restrictions are not fulfilled. An important point is that the sample should be chosen with a known probability. Another point to note is that the known information about a population will involve a variable, for example \\(X\\), whereas the measurements to be collected are from a another variable, say \\(Y\\). The efficacy of the survey will depend heavily on the accuracy of \\(X\\) and the relationship between \\(X\\) and \\(Y\\).\nIt should be noted that restrictions may limit the number of elements selected from particular subgroups so that it may not be reasonable to invoke the Central Limit Theorem and assume that estimators in these subgroups follow a Normal distribution."
  },
  {
    "objectID": "studyguide/1-data-collection.html#terminology-and-notation-for-samples",
    "href": "studyguide/1-data-collection.html#terminology-and-notation-for-samples",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Terminology and Notation for Samples",
    "text": "Terminology and Notation for Samples\nA sample is a subset of a population. There are innumerable ways of drawing a sample so that there is a richness and variety in sampling methods which do not exist in census methods. The first step in obtaining a sample is the same as for taking a census in that a frame must be set up. The sample is then drawn from the frame so that the sequence is target population \\(\\Rightarrow\\) frame \\(\\Rightarrow\\) sample.\nThe relationship between population and sample is shown in Figure 2 (which is almost the same as Figure 1).\n\n\n\nFigure 2: Relationship between the target population, frame and sample\n\n\nIt was pointed out above that a population, in practice the frame, could be written as \\(U=\\left(L,Y,X\\right)\\). By contrast probability samples could be denoted by \\(S = \\{P_r, l, y, x\\}\\). The curly brackets, {.} are meant to indicate that there are many samples which can be drawn from the one population. Note that \\(S\\) is the set of all possible samples, \\(s\\), selected under the probability scheme \\(P_r\\). The labels in the sample as well as the sample values for \\(Y\\) and \\(X\\) are denoted by lower case letters. In general, the convention is to use capitals when referring to the population but lower case for the sample. The term \\(P_r\\) indicates the probability, \\(P_r(s)\\), of the sample \\(s\\) being selected.\nAnother way to look at this is to consider the probability that a particular element, or label, be included in the sample. Let \\(\\pi_{i}\\) be the inclusion probability for the \\(i^{th}\\) label. A special but common occurrence is the equality of the \\(\\pi_{i}\\) for \\(i\\) from \\(1\\) through \\(N\\) and, in this case, the \\(P_r(s)\\) will be the same for all \\(s\\) in \\(S\\). In this case, the selection is said to be EPSEM, standing for equal probability of selection, or it is said that the elements are self weighing as the probabilities do not depend on the particular labels which have already been selected. The most obvious example of a self weighting sample is that of a simple random sample but multistage samples can also be arranged with unequal probabilities at each stage but such that the inclusion probabilities are equal."
  },
  {
    "objectID": "studyguide/1-data-collection.html#errors-in-samples",
    "href": "studyguide/1-data-collection.html#errors-in-samples",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Errors in Samples",
    "text": "Errors in Samples\nSamples will be liable to the same kind of biases as a census as well as others inherent in the sampling process. Major sources of biases in samples are classified in Figure 3. On the one hand, the biases can be split into those which occur at the time of selecting the sample, while others occur at the stage of collecting and operation on the data. On the other hand, a division can be made into sampling and nonsampling, depending on whether the bias occurs in a random or fixed manner. The nonsampling errors can be further subdivided according to whether an observation is made or not.\n\n\n\nFigure 3: Classification of Sources of Survey Biases\n\n\nBriefly, the cells in Figure 3 refer to the following problems:\n\nFrame inefficiencies: To select a sample, a frame, which is often a listing but may be such as a map, is used. If certain items are missing or occur more than once, these items and the others in the frame will have a probability of selection which is different from what it should be.\nBiased estimation: In this course, only unbiased estimates such as the sample mean are considered.\nNon-observation: Non-observation could be due to frame inefficiencies or not being able to locate the item.\nNon-response: The item, say a particular person, may be located but he or she may be not at home or refuse to answer all or some of the questions.\nPurposive Selection: When selection occurs deliberately rather than at random then biases will often intrude. In quota sampling, the interviewer is told to contact so many people over 45, or male, or in some other category. The personal choice involved in this selection could introduce bias.\nInterviewer bias, coding errors: Certain characteristics of the interviewer may incline the subject to respond in a certain way. With another interviewer, the subject may respond differently. Once the data is collected, errors can occur in an amazing number of ways in coding the answer, transcribing to summary sheets, entering the data into a computer, as well as error in computer programming.\n\nOther errors are termed random or stochastic in that probabilities are involved so that samples, but not censuses, may suffer from these errors. The assumption is made that the measurement is correct but the estimator varies depending on the sample which has been drawn. Rather than giving the name error to this outcome it would be preferable to speak of sampling variation.\nOur aim in sampling and in all statistical estimation, is to find an estimate which is close to the population parameter. Of course, we have a problem in defining “close” as we do not know if our sample is representative of the whole population or whether it is an unusual one. If the sample yields an estimate of the population parameter we can at least talk about the expected or average distance of the estimate from the parameter.\nAn illustration of the ideas of bias and variance is that of shots fired at a target. If the shots are widely scattered, they exhibit a large variance. If they are spread around the target so that on average they are equally scattered around the bulls-eye (neither too high nor too low etc.), we could say they are unbiased shots of the bulls-eye. The four possible cases are shown in Figure 4).\n\n\n\nFigure 4: Bias and variance\n\n\nNotice that bias can occur in census as well as sample data, but we can only talk about variance in a probability sample. From a single sample yielding one value of a statistic, it is not possible to estimate the bias in the statistic but an estimate of the variance of the statistic can be obtained from the variation in the sample.\nEach sample is not an exact match of the population so that the resulting estimator will invariably differ from the population parameter it is estimating. As the parameter is unknown, the actual difference between the estimator and the parameter is also unknown so that it is important to have certain assumptions about the distributor of the variable under study and distribution of the resulting estimator. Mathematical theory is often available to indicate the amount of bias and variance in the estimator.\nUsually, a sampling method is chosen to ensure that the resulting estimator is unbiased. A peculiar sample does not necessarily mean that the estimator is biased but it may be difficult to convince others that this is so. For example, it may turn out that the sample consists only of females even though the population is evenly divided between the sexes. If females and males give similar responses to the variable under question, the strange composition of the sample in this case would not affect the value obtained for the estimator. On the other hand, a sample which seems to be representative of the population on certain variables may yield a strange value for the estimator. One must assume, for there is no proof as the parameters under study are unknown, that if the sample is chosen carefully, which usually means that an appropriate randomisation scheme is employed, the measurements and hence the estimate obtained will be representative of the population.\nOften, the sample and the estimator are chosen in such a way that the estimator is unbiased. The sampling scheme and the estimator can be considered as two sides of the same coin so that the estimator is chosen to fit the sampling scheme. Furthermore, the sampling scheme can be chosen in such a way that the variance of the estimator is a minimum subject to certain constraints such as cost and time. At times an estimator is chosen deliberately to be biased if in so doing the variance is made smaller. To compensate for this for it may be that a biased estimator will ensure that it is closer to the parameter of interest than an unbiased estimator. These considerations are of more interest to the analysis of data than to their collection; so this topic will not be considered at length here.\nIn sample surveys, an estimator may turn out to be biased because the probability of selection of certain elements is different to what the researcher thought. If the frame used to select people is a list of homeowners, wealthy owners would have a higher probability of selection for they are more likely than less well off people to own more than one house. Consequently, an owner of multiple houses would have a high probability of selection which would bias the estimate towards such people unless the multiple ownership is taken into account. This source of error is termed frame inefficiency and it is likely to arise whenever there are discrepancies with the sampling frame."
  },
  {
    "objectID": "studyguide/1-data-collection.html#simple-random-sample",
    "href": "studyguide/1-data-collection.html#simple-random-sample",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Simple Random Sample",
    "text": "Simple Random Sample\nIn our lottery example, each of the 1,000 tickets has the same probability (1 in 1,000) of winning any one prize and, indeed, of winning any one of the 3 prizes (3 chances in 1,000, that is, 1 chance in 333).\nIf the researcher has no information about the sampling frame, except perhaps the number of elements \\(N\\) present, simple random sampling may be a feasible approach. It is common in experimental design when treatments are allocated to experimental units. In quality control, simple random sampling is often used.\nIf the population is large, at least in relation to the sample size, it would usually be inefficient to use simple random sampling for it would require considerable time and cost to select and contact the elements of such a sample. If the sampling fraction is large, (say) more than ten percent, it may be reasonable to use simple random sampling as it is likely that the sample will be spread reasonably evenly throughout the population.\nThe more additional information available, the more structured the sampling design can be. The additional information may be of an exact dichotomous or polychotomous nature such as the natural division of the frame into three disparate geographical areas. The additional variable may be continuous such as the age structure of the frame. In other cases, the information may be assumed rather than known for certain. For example, the educational level achieved by every person in a frame may be listed but this information may be a few years out of date. In these cases, a sample survey may be taken based on the age structure of the frame, but the conclusions will only be valid to the extent that there has been no change in this age structure.\nThe additional information can be the basis for the logistics of the survey as different parts of the frame may require more intensive sampling or different methods of organisation. For example, if the frame covers rural and urban areas, the urban sectors may be reached by public transport, but respondents may not be at home during the day as they are likely to be at work. The rural areas may involve other difficulties due to the scattered nature of the population.\nThe obvious danger is that sampling decisions made for administrative ease may hinder accumulation of data and may discredit inferences drawn from the resulting sample. For example, one must query whether the responses collected by one interviewer are likely to be considerably different from those of another, whether data collected by face-to-face methods can be aggregated with data collected by telephone interviews; whether data collected on some elements can be assumed to be similar to other data collected at a later time; whether responses to questions which must be translated into different languages, or explained in simpler English to someone of limited education or facility in English, can give responses which can be aggregated with responses to these same questions when not translated. There are no easy answers to questions such as these but researchers should be aware of such difficulties. Whenever possible, the peculiarities in the collection method should be noted and made available to the reader so that he/she can better judge the conclusions of the survey.\nEstimators with a low variance are likely to be more accurate in that they reflect population values more closely. The use of additional information can lead to a reduction in the variance of estimators such as sample means and sample proportions. This gain in efficiency will depend on the strength of the relationship between the additional information and the variable under study. At times, this relationship may be estimated but, at other times, it must be taken for granted.\nIf the population is small but there is no information available on other aspects of the population, a simple random sample may be appropriate. Other sampling schemes may be used, though, for some of the following reasons:\n\nAdministrative problems\nIf the population size is large, it will be rather clumsy to try to select a simple random sample. If the population is spread out geographically, then considerable cost may be involved unless the sample is grouped in some way.\n\n\nImprove accuracy\nUnder certain conditions, or assumptions, it may be possible to use a scheme which leads to more accurate estimates - accurate in the sense that the variance and/or bias is reduced.\n\n\nPolitical reasons\nA simple random sample of (say) supermarkets in New Zealand may, by chance, consist only of stores in Auckland. The results of this survey may be viewed very sceptically by readers and, more importantly, by those who are funding the survey."
  },
  {
    "objectID": "studyguide/1-data-collection.html#stratified-sampling",
    "href": "studyguide/1-data-collection.html#stratified-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Stratified Sampling",
    "text": "Stratified Sampling\nStratified sampling is a very versatile approach. It can be easy to implement, and can also lead to more efficient estimation of parameters. With stratified sampling, the population is divided into groups called “strata”, and then random samples are taken from each stratum. The advantages of this approach are:\n\nIt is a flexible approach, as strata can be designated in different ways.\nEach stratum can be treated as a separate population so that strata could be surveyed at different times and even different ways. For example, a survey in NZ could be divided into north and south island, urban and rural; the rural sample could be contacted by telephone but the urban by face-to-face interviews. One stratum could involve systematic sampling; other involve simple random sampling.\nIf we have information regarding the variation in strata then the resulting estimator (of the population mean for instance) could be more precise (less variance) than in simple random sampling if the more variable strata are sampled more intensely.\nIt is probably the most common method employed in sample surveys.\nThe sample is spread throughout the population.\nThe variance of an estimate from stratified sampling is usually equal to, or less than, the variance of an estimate from simple random sampling. It would be sensible to sample more heavily the larger strata. This is the approach taken in proportional (stratified) sampling in which the sample size in each stratum is proportional to the size of the stratum. Proportional sampling is appropriate when all the strata are equally variable. When this is not the case the more variable strata are sampled more heavily."
  },
  {
    "objectID": "studyguide/1-data-collection.html#cluster-sampling",
    "href": "studyguide/1-data-collection.html#cluster-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Cluster Sampling",
    "text": "Cluster Sampling\nFor some investigations, it is advisable to consider the population as being composed of clusters or groups and to proceed with the sample survey by selecting certain clusters and collecting measurements from (usually) a random selection of the elements within the chosen clusters. Cluster and stratified sampling differ in that all the strata are sampled whereas only some of the clusters are sampled. It is usual to have a large number of small clusters in cluster sampling but a small number of strata in stratified sampling. Consider a sample survey of the pupils in a school. The classes within the school may be considered as possible strata or possible clusters. There are advantages for cluster sampling in this case.\n\nEase of administration. It will be easier to select certain classes in a school (say) and interview a random sample or all of the students in those classes rather than selecting students from all the classes in the school.\nA listing of all the children is only necessary for the selected classes.\nConfidentiality and/or independence of response may be aided by this approach. It would take longer to collect information from all the classes in the school and the longer it takes the more likely it is that respondents will talk among themselves about the questions before the researcher can collect his/her information. If the whole class is presented with the questionnaire at the same time, confidentiality is more likely to be preserved.\nLocating elements (students) is easier, travel (between classrooms) will be reduced, and it is easy to divide responsibility between interviewers.\n\nIn some cases, it makes more sense to collect measurements on whole clusters rather than just a few individuals within each selected cluster. This is probably true in the case of the above example. Surveys on TV watching collect information on a family is another example where this is probably true. If the TV set is on, it may be difficult to pin down which members are watching as some may claim not to be interested (although they may know quite a lot about that program). Alas, there must be some payment for these benefits and this is reflected in the higher variances of cluster sampling estimators over those of simple random sampling."
  },
  {
    "objectID": "studyguide/1-data-collection.html#systematic-sampling",
    "href": "studyguide/1-data-collection.html#systematic-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Systematic Sampling",
    "text": "Systematic Sampling\nIn this section, we shall only consider a particular type of systematic sampling, namely that which includes a component of randomness, as this allows probability statements to be made.\nIn the simplest presentation, the frame is assumed to consist of \\(N\\) elements where \\(N\\) = \\(kn\\) with \\(n\\) being the required sample size. A random digit, \\(i\\), in the range of l through \\(k\\) is chosen as a starting point and then every \\(k^{th}\\) element from then on is placed in the sample. The sample then consists of the elements \\(i, i + k, i + 2k,\\) etc. Clearly this approach is likely to produce a sample which is evenly distributed over the entire sampling frame, and it is therefore likely that the sample will be representative of the population. This is particularly true when the sampling frame is sorted geographically. However, when this is not the case, the variance of the estimators obtained using systematic sampling will generally be greater than those obtained for simple random sampling."
  },
  {
    "objectID": "studyguide/1-data-collection.html#probability-proportional-to-size-pps-sampling",
    "href": "studyguide/1-data-collection.html#probability-proportional-to-size-pps-sampling",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Probability-proportional-to-size (PPS) Sampling",
    "text": "Probability-proportional-to-size (PPS) Sampling\nProbability-proportional-to-size sampling, or PPS sampling, can be thought of as an extension of stratified sampling. Stratified sampling allows the flexibility for the probabilities of selection to vary with the size and characteristics of the strata. PPS sampling takes this a step further so that each element has its own probability of selection.\nThe procedure requires that an associated variable be known so that each element in the population has a measure of size associated with it. This variable may be a related variable whose values are known from a previous census. For example, auditors usually check only a sample of all the accounts in existence. The size of each account is crucial. So the large value accounts should be checked more intensely than the smaller accounts. This is achieved by giving each account a probability of selection which is proportional to its value. This is called Dollar-Unit Sampling."
  },
  {
    "objectID": "studyguide/1-data-collection.html#estimates-for-common-sampling-methods",
    "href": "studyguide/1-data-collection.html#estimates-for-common-sampling-methods",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Estimates for Common Sampling Methods",
    "text": "Estimates for Common Sampling Methods\nAssociated with each sampling method is a particular form of estimator for the population parameters of concern. If an element has a high probability of selection, its effect must be reduced when an estimate is formed or, otherwise, the estimate will tend to be too large.\nIf the \\(i^{th}\\) observation is \\(y_{i}\\) and the probability that this element is chosen into the sample is \\(\\pi _{i}\\), it turns out that:\nEstimate of population total = \\(\\sum \\frac{y_{i} }{\\pi _{i} }\\) = sum(each observation in sample/its probability of selection)\nThis is easy to show with simple random sampling when \\(\\pi _ i =\\frac{n}{N}\\) for \\(i=1,2,...n\\). Let \\(Y\\) = population total, \\(\\bar{Y}\\) = population mean, and \\(Y=N\\bar{Y}\\). We estimate \\(\\bar{Y}\\) by \\(\\bar{y}\\) = mean of the sample and we estimate \\(Y\\) by \\(N\\bar{y}\\). If we write the estimate of the population total as \\(\\hat{Y}\\), we have: \\[\\hat{Y}=N\\bar{y}= N\\sum \\frac{y_{i} }{n} =\\sum \\frac{y_{i} }{\\left({\\tfrac{n}{N}} \\right)}\\] Notice that the summation is over all observations in the sample. For simple random samples, the probability that each observation is selected is \\(\\frac{n}{N}\\) so that the estimate of the total is, indeed, the sum of each observation divided by its probability of selection.\nFor stratified sampling with \\(k\\) strata, the estimated total and mean are, respectively \\[\\hat{Y}=N_{1} \\bar{y}_{1} +N_{2} \\bar{y}_{2} +....+N_{k} \\bar{y}_{k}\\] and \\[\\bar{Y}=\\left(N_{1} \\bar{y}_{1} +N_{2} \\bar{y}_{2} +....+N_{k} \\bar{y}_{k} \\right)/N\\] Assume that stratum \\(j\\) contains \\(N_{j}\\) elements and that the sample size for this stratum is \\(n_{j}\\), then the probability that an element in the \\(j^{th}\\) stratum is selected is \\(\\frac{n_{j} }{N_{j} }\\). As an example, suppose that a stratified sample of adult males, in full time employment in NZ, was taken to determine the number of working days lost in the previous year and also their party political preference. In this hypothetical example, the strata are urban versus rural. The results were shown in Figure 5).\n\n\n\nFigure 5: STRS estimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate of total days lost \\(\\hat{Y}=N_{1} \\bar{y}_{1} +N_{2} \\bar{y}_{2} = 400,000\\times 5 + 100,000\\times 2 = 2,200,000.\\)\nThe estimate of the total number who prefer Labour is \\[\\hat{Y}=N_{1} p_{1} +N_{2} p_{2} = 400,000 \\times 0.5 + 100,000\\times 0.3 = 230,000.\\] Notice that proportions are means and fit into this structure quite naturally by defining\n\\[x_i = \\left\\{ {\\begin{array}{*{20}c} 1 & {{\\text{if the element has the property understudy}}} \\\\\n0 & {{\\text{otherwise}}} \\\\ \\end{array}} \\right.\\]\nNotice also that we could estimate \\(\\bar{y}\\) by \\(\\hat{Y}/N\\) so that:\nEstimate of total days lost per person = \\(\\hat{Y}/N\\) = 2,200,000/500,000 = 4.4.\nEstimate of proportions who prefer Labour = \\(\\hat{Y}/N\\) = 230,000/500,000 = 0.46.\nThese same formulae for estimating the population total and the population mean are appropriate for probability proportional to size (PPS) sampling. However, sampling without replacement makes the formulae rather complicated so we will not consider the details here."
  },
  {
    "objectID": "studyguide/1-data-collection.html#multistage-designs-and-epsem",
    "href": "studyguide/1-data-collection.html#multistage-designs-and-epsem",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Multistage Designs and EPSEM",
    "text": "Multistage Designs and EPSEM\nLarge, and even moderately sized, sample surveys are often multistage surveys. The probability that an element is selected becomes the product of the probabilities of it being selected at each stage. As a simple example, consider 3 schools having 100, 200 and 300 students, respectively, and suppose that 12 students are to be selected from one of these schools. This could be carried out in a two-stage survey with the first stage being the selection of one school by probability proportional to size (PPS) sampling. In the second stage, 12 students are selected from the chosen school. Let \\(\\pi _{a}\\) be the probability of choosing a particular school and \\(\\pi _{b}\\) the probability of choosing a particular student (given that his/her school has been chosen). The probability of selecting a particular student is \\(\\pi_{a} \\times \\pi_{b}\\); see Figure 6.\n\n\n\nFigure 6: PPS example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this example, each student has the same probability of being selected which has been brought about by the PPS sampling of the schools. It would be possible, in this small example, to use only a one stage procedure such as a simple random sample of 12 students from a list of the 600. Alternatively each school could be viewed as a separate stratum and a stratified sample of students could be chosen. Of course, if 4 students are chosen from each school, those in the smallest school would have a better chance (4 chances in 100) of being selected than those in the largest school (4 chances in 300). On the other hand, if 2 are chosen from the first school, 4 from the next and 6 from the largest then each student will have the same probability of selection.\nThese EPSEM, equal probability of selection methods, have the advantage that it seems “fair” to allow each person the same probability of selection but, also, it makes for simpler and neater formulae for estimates and their variances."
  },
  {
    "objectID": "studyguide/1-data-collection.html#other-sampling-methods",
    "href": "studyguide/1-data-collection.html#other-sampling-methods",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Other Sampling Methods",
    "text": "Other Sampling Methods\n\nVolunteers\nSamples are often selected because they are easy to obtain. Many psychology experiments referred to in the literature make use of long suffering students in introductory psychology courses. Quite often, volunteers are sought for experiments. Indeed, even when a probability sampling scheme is used, some subjects will withdraw so that the actual respondents are, in a sense, volunteers. Sometimes, people are accosted in the street or supermarket by market researchers or media reporters. The sample is by and large an opportunity sample for it is difficult to obtain a random sample under such conditions. Any generalisations from such a sample should be treated with some suspicion.\n\n\nSnowball samples\nA snowball sampling approach is used by medical authorities to track down possible sufferers of sexually transmitted diseases. All former contacts of a known sufferer are contacted, if possible, and former contacts of theirs also contacted. Both volunteer and snowball sampling are purposive methods in that no randomisation is employed.\n\n\nQuota samples\nAnother purposive sampling method is that of quota sampling. The idea is to make the sample representative of the population at least for certain variables. The quotas required in the sample may be decided by partitioning the sample into males and females, and also partitioning it by age groupings. The interviewer would be required to find the stipulated number of respondents in each cell of the sample. Although the sample is representative on the given variables, one is not sure how representative it is in terms of the responses obtained. Furthermore, being a purposive sample, it is difficult to make probability statements about estimators.\n\n\nEstimating wild life populations\nIt would be difficult to estimate the number of deer in a large untamed area such as some parts of the South Island. The target population of deer is very large but also is very hard to locate. One sampling approach would be to use recent deer droppings as a sampling frame, and for researchers to walk in a straight line noting the number of occurrences of droppings in (say) a 2 meter wide path. If the researchers are spaced at intervals of 20 meters then one could assume actual number of droppings of 10 times that obtained in the sample. Further assumptions would be needed to extrapolate from the number of droppings to the number of deer. A more direct approach would be afforded by a capture-recapture method in which a sample of animals is captured, tagged, released and then at a later date a further sample is captured. The proportion of tagged animals in this second sample indicates the size of the population. A number of assumptions must be made such as the uniform spread of the tagged animals throughout the areas of the second sampling. The initial capturing may affect the animals in unusual ways, by making them shy tending to avoid capture in future or making them tamer so that they do not fear capture as much on a subsequent occasion. The method of tagging may also be disadvantageous causing stress. Other assumptions need to be made about the distribution of the untagged and tagged animals. A number of different estimators have been suggested to obtain unbiased and efficient estimators of the number of animals in the population."
  },
  {
    "objectID": "studyguide/1-data-collection.html#sample-size-some-practical-issues",
    "href": "studyguide/1-data-collection.html#sample-size-some-practical-issues",
    "title": "Chapter 1: Data Collection and Quality Issues",
    "section": "Sample Size & Some Practical issues",
    "text": "Sample Size & Some Practical issues\nFigure 7 shows a fictitious example of how samples of four people are drawn from a population of 20 people.\n\n\n\nFigure 7: Methods of Sampling: A toy example\n\n\nIn reality, sampling issues are not as simple as in the above example. Most large studies require quite a complex form of sampling involving layered sampling frames and combinations of design strategies. A typical design for an urban study is described below. The first layer consists of 20 regional areas; the second layer consists of all the municipalities within each regional area; the third layer consists of all the suburbs within each municipality; the fourth layer consists of all the city blocks within each suburb and the fifth layer consists of all the households within each city block. Judgement may be used to select only 5 of the 20 regional areas, one typical of the central regions, one each typical of the northern, southern, eastern and western regions. Next a systematic sample of municipalities is chosen (from a list sorted by postal code) for each of these regional areas, a sample of suburbs is chosen (stratified by socio-economic level) within each municipality, a simple random sample of city blocks is chosen within each suburb and a simple random sample of households is chosen within each city block. Such designs aim to be cost effective in terms of producing a representative sample at reasonable cost. However, complex designs require complex analyses, so try to keep things as simple as possible. The important criteria for the selection of an appropriate sample design are stratification, randomisation and cost. Note that non-response also has cost implications in that a survey with a high non-response rate is more costly than a survey with a good response rate. Of course the sample size also has an influence on cost and this provides opportunities for sample design/sample size trade-off. In the following discussion we assume that there is a 100% response rate. In other words sample size refers to the number of (good) sample elements. When only a 50% response rate is expected for a mail survey it means that the number of questionnaires distributed should be double the required sample size.\n\nSample Size\nUnfortunately, it is the available funds rather than research requirements that often determine what the sample size will be. We will therefore not ask the question How large a sample do we need? Instead we will ask What can a certain sample size \\((n)\\) do for us? The answer to this question depends on the sample design that is to be used. A quantity called a design effect \\((d)\\), related to the variance of the estimate, is used to measure the quality of a sample design. The larger the design effect (variance), the worse the design in terms of producing representative samples. The design effect is used to compute an effective sample size \\((e)\\) from the actual sample size \\({n}\\) using the formula \\({e=}\\frac{n}{d}\\). The effective sample size provides a more meaningful indication of the number of independent (useful) responses than the actual sample size. Figure 8 roughly shows the typical ranges for the design effect \\((d)\\) associated with the various sample designs.\n\n\n\nFigure 8: Crude comparison of effective sample sizes\n\n\nFor a simple random sample the design effect is treated as one. So, for a simple random the effective sample size is the same as the actual sample size. Each observation in the sample is doing a full job. You will see that a quota sample typically has a design effect of 2, producing an effective sample size, which is half the actual sample size. This means that each observation in a quota sample is only doing half a job. However, in the case of a stratified random sample, the design effect is typically less than one indicating a very good design and producing an effective sample size that is greater than the actual sample size \\((n)\\). In this case each observation is working overtime. Clearly a design effect of more than one means that the sample is not as efficient as a random sample would be, while a design effect of less than one means that the sample if more efficient than a random sample would be.\nMost opinion polls give a margin of error (this term will be formally defined later on) when they give the result of their survey. This is the level of inaccuracy associated with the sample result. For example, we hear that a poll indicates that a proportion \\(p\\) of 40% of voters support National but the margin of error could be 6.1% if the effective sample size is only 300. When the purpose of the research is to estimate some proportion (i.e. percentage) then the margin of error can be calculated using an approximate formula (namely \\(2\\sqrt{p(1-p)/e}\\) where \\(p\\) is the expected proportion and e is the effective sample size). As indicated in Figure 9, the margin of error declines as the effective sample size increases. By setting \\(e\\) equal to the actual sample size divided by the design effect we can estimate the margin of error for any design. If the margin of error is too big it means that the sample size needs to be increased. Unfortunately this is usually the case. This example is hypothetical but it illustrates the general relationship between the margin of error and the effective sample size.\n\n\nCode\nlibrary(tidyverse)\n\ndfm &lt;- tibble(\n  \"Sample size\" = 30:1000,\n  \"Margin of error\" = 100/sqrt(`Sample size`)\n)\n\ndfm |&gt; \n  ggplot() +\n  aes(x = `Sample size`,\n      y = `Margin of error`) +\n  geom_path() +\n  theme_minimal()\n\n\n\n\n\nFigure 9: Margin of Error and Sample Size\n\n\n\n\nIn Figure 9, the effective sample size required for a margin of error of 10% is 100. Since the actual sample size equals the effective sample size for a simple random sample this means that the size of the simple random sample must be 100 if the margin of error is 10% when a population proportion is estimated. However the effective sample size depends on the sample design. The actual sample size required for a given margin or error also depends on the sampling design. The actual sample size required in order for the margin of error to be 10 is highest for quota samples at 200 and lowest for a random stratified sample at 80. For the random cluster sample, an actual sample size of 120 will produce this margin of error.\nThe following rule of thumb can be used when sample results are to be generalised to a population. Firstly, if fairly sophisticated (multivariate analysis) is required then the effective sample size should be at least 200. Secondly, there should be at least four sample elements for every variable included in the study. However, this rule and the above formula are irrelevant when the population is relatively small or when we do not wish to generalise our findings to the population. In studies of this nature smaller sample sizes are sufficient. But the relative percentage to the size of the population should be large. For example if the population is of size only 100, an effective sample of size 80 is needed to achieve a certain degree of margin of error when the true population proportion is about 0.5. However, only an effective sample of size 370 is needed for the same margin of error if the population size is 10,000. The effective sample size needs to be increased only to 385 if the population size is 100,000. The moral is that percentage sampling is a fallacy, and that the absolute sample size is more important than sampling a fixed percentage of the population."
  },
  {
    "objectID": "slides/Chapter08.html#learning-objectives",
    "href": "slides/Chapter08.html#learning-objectives",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine and understand an ANOVA\nUse a one-way ANOVA on example data\nUnderstand and calculate an ANOVA table\nDescribe Tukey’s post hoc tests and assumptions of ANOVAs\nUnderstand the difference between one-way and two-way ANOVAs"
  },
  {
    "objectID": "slides/Chapter08.html#anova",
    "href": "slides/Chapter08.html#anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA",
    "text": "ANOVA\n\nAnalysis of variance, or ANOVA, is an approach to comparing data with multiple means across different groups, and allows us to see patterns and trends within complex and varied data.\nUsed for categorical or grouped data.\nOften data from experiments (treatments make good factors).\nEDA bar, points, or box plots are options to show differences between groups."
  },
  {
    "objectID": "slides/Chapter08.html#one-way-anova",
    "href": "slides/Chapter08.html#one-way-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\n\nfabric burn-time data\n\n\n\n\nfabric 1\nfabric 2\nfabric 3\nfabric 4\n\n\n\n\n17.8\n11.2\n11.8\n14.9\n\n\n16.2\n11.4\n11\n10.8\n\n\n17.5\n15.8\n10\n12.8\n\n\n17.4\n10\n9.2\n10.7\n\n\n15\n10.4\n9.2\n10.7\n\n\n\n\nCan we regard the mean burn times of the four fabrics as equal?\n\n\nfabric 1 seems to take longer time to burn"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-single-factor-anova-model",
    "href": "slides/Chapter08.html#one-way-single-factor-anova-model",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way (single factor) ANOVA model",
    "text": "One-way (single factor) ANOVA model\nStart with hypotheses and EDA to visualize groups\n\n\\(H_0\\): The mean burn times are equal for the four fabrics\n\\(H_a\\): The mean burn time of at least one fabric is different.\n\n\nfabric=read.table(\"../data/fabric.txt\", header=TRUE, sep=\"\\t\")\nfabric |&gt;\n  ggplot(aes(x=factor(fabric), y=burntime))+\n  geom_point()+\n  stat_summary(fun.data = \"mean_cl_boot\", colour = \"blue\", linewidth = 2, size = 3)\n\n\n\n\n\n\n\nfabric |&gt;\n  ggplot(aes(x=factor(fabric), y=burntime))+\n  geom_point()+\n  stat_summary_bin(fun = \"mean\", geom = \"bar\", orientation = 'x')"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-single-factor-anova-model-1",
    "href": "slides/Chapter08.html#one-way-single-factor-anova-model-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way (single factor) ANOVA model",
    "text": "One-way (single factor) ANOVA model"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table",
    "href": "slides/Chapter08.html#anova-table",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table",
    "text": "ANOVA table\n\nobservation = mean + effect + error\n\n\nSS Total = SS Factor + SS Error\n\n\nsummary(aov(burntime ~ fabric, data = fabric))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nfabric effect on burntime is highly significant.\n\nIn other words, the null hypothesis of equal mean burntime is rejected.\n\nOr alternatively the mean burntime is different for at least one fabric"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table-1",
    "href": "slides/Chapter08.html#anova-table-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table",
    "text": "ANOVA table\nHow are these values calculated?\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nSS\ndf\nMeanSq\n\n\n\n\nFACTOR\nk-1\nFACTOR SS/(k-1)\n\n\nERROR\nn-k\nERROR SS/(n-k)\n\n\nTOTAL\nn-1\nTOTAL SS/(n-1)"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table-2",
    "href": "slides/Chapter08.html#anova-table-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table",
    "text": "ANOVA table\nCalculating F values of a factor:\n\\(F = \\frac{MS_{Factor}}{MS_{Error}}\\)\n\n\n\nSS\ndf\nMeanSq\n\n\n\n\nFACTOR\nk-1\nFACTOR SS/(k-1)\n\n\nERROR\nn-k\nERROR SS/(n-k)\n\n\nTOTAL\nn-1\nTOTAL SS/(n-1)"
  },
  {
    "objectID": "slides/Chapter08.html#anova-table-practice",
    "href": "slides/Chapter08.html#anova-table-practice",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA table practice",
    "text": "ANOVA table practice\n\n\n\nSS\ndf\nMeanSq\nF- value\nP-value\n\n\n\n\nFACTOR\nk-1\nFACTOR SS/(k-1)\nMSF/MSE\n\n\n\nERROR\nn-k\nERROR SS/(n-k)\n\n\n\n\nTOTAL\nn-1\nTOTAL SS/(n-1)\n\n\n\n\n\n\n\n\n\nSS\ndf\nMeanSq\nF- value\nP-value\n\n\n\n\nfabric\n120.5\n\n\n\n\n\n\nResiduals\n46.3\n\n\n\n\n\n\nTOTAL"
  },
  {
    "objectID": "slides/Chapter08.html#reminder-on-p-values",
    "href": "slides/Chapter08.html#reminder-on-p-values",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder on P values",
    "text": "Reminder on P values\nIn null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\nA very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis.\nWe calculate them based on our theoretical sampling distributions (normal, \\(t\\), \\(F\\), \\(\\chi^2\\))"
  },
  {
    "objectID": "slides/Chapter08.html#reminder-of-sampling-distributions",
    "href": "slides/Chapter08.html#reminder-of-sampling-distributions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder of Sampling Distributions",
    "text": "Reminder of Sampling Distributions\nA sampling distribution is a probabilistic model of sampling variation–it describes the behaviour of some sample statistic\nFor a normal population, when the population parameters and are known, we can easily derive the sampling distributions of the sample mean or sample variance.\nWhen the population parameters are unknown, we have to estimate them from data."
  },
  {
    "objectID": "slides/Chapter08.html#reminder-of-sampling-distributions-1",
    "href": "slides/Chapter08.html#reminder-of-sampling-distributions-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder of Sampling Distributions",
    "text": "Reminder of Sampling Distributions"
  },
  {
    "objectID": "slides/Chapter08.html#reminder-of-sampling-distributions-2",
    "href": "slides/Chapter08.html#reminder-of-sampling-distributions-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder of Sampling Distributions",
    "text": "Reminder of Sampling Distributions"
  },
  {
    "objectID": "slides/Chapter08.html#graphical-comparison-of-means",
    "href": "slides/Chapter08.html#graphical-comparison-of-means",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Graphical comparison of means",
    "text": "Graphical comparison of means\n\nThe graph below shows individual 95% confidence intervals for the fabric means\n\nPooled SD (\\(\\sqrt{MSE}\\)) is NOT used"
  },
  {
    "objectID": "slides/Chapter08.html#one-way-anova-model-assumptions",
    "href": "slides/Chapter08.html#one-way-anova-model-assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "One-way ANOVA model Assumptions",
    "text": "One-way ANOVA model Assumptions\n\nResiduals are randomly and normally distributed\nResiduals must be independent of means.\n\nIf SD increases with mean, try square root or logarithmic transformation.\n\nThe ANOVA model assumes equal SD for the treatments.\nIf experimental errors are more in some subgroups, divide the problem into separate ones.\nPositive correlation among residuals leads to under estimation of error variance; negative correlation leads to overestimation.\n\n\nThese assumptions are harder to validate to small experimental design data"
  },
  {
    "objectID": "slides/Chapter08.html#visualize-assumptions",
    "href": "slides/Chapter08.html#visualize-assumptions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Visualize assumptions",
    "text": "Visualize assumptions\n\nplot(aov(burntime ~ fabric, data = fabric))"
  },
  {
    "objectID": "slides/Chapter08.html#visualize-assumptions-1",
    "href": "slides/Chapter08.html#visualize-assumptions-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Visualize assumptions",
    "text": "Visualize assumptions\n\nfabric |&gt; \n  group_by(fabric) |&gt; \n  summarise(\n    mean = mean(burntime),\n    sd = sd(burntime)\n  ) |&gt; \n  ggplot() +\n  aes(x = mean, y = sd, group = factor(1)) +\n  geom_point() \n\n\nFigure 1: SD vs mean for four fabricsWith only four fabrics in the sample, it is difficult to make any definitive claim.\nIf the assumptions were valid, we would expect the four points to fall approximately along a horizontal band indicating constant standard deviations, and hence variances, regardless of the means of the groups.\nThis figure suggests that this is the case, so the assumption of equal variances appears to be valid."
  },
  {
    "objectID": "slides/Chapter08.html#equal-variance",
    "href": "slides/Chapter08.html#equal-variance",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Equal variance",
    "text": "Equal variance\nBartlett’s test: - null hypothesis: equal variances - but it has an assumption of its own (response variable must be normally distributed)\nLevene’s test - null hypothesis: equal variances - is applicable for any continuous distribution\n\nbartlett.test(burntime ~ fabric, data = fabric)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  burntime by fabric\nBartlett's K-squared = 2.6606, df = 3, p-value = 0.447\n\ncar::leveneTest(burntime ~ fabric, data = fabric)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  3  0.1788 0.9092\n      16"
  },
  {
    "objectID": "slides/Chapter08.html#normality",
    "href": "slides/Chapter08.html#normality",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Normality",
    "text": "Normality\n\naov_fabric &lt;- aov(burntime ~ fabric, data = fabric)\nshapiro.test(aov_fabric$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  aov_fabric$residuals\nW = 0.88926, p-value = 0.02606\n\n\nANOVAs are robust to mild issues of non-normality\nBut if have issues with normality and unequal variance try transformations"
  },
  {
    "objectID": "slides/Chapter08.html#tukey-hsd",
    "href": "slides/Chapter08.html#tukey-hsd",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Tukey HSD",
    "text": "Tukey HSD\n\nTukey HSD (Honest Significant Differences) plot allows pairwise comparison of treatment means.\nWhich fabric types have different burn times? (remember the alternative hypothesis of a one-way ANOVA is at least one of the means are different)\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = burntime ~ fabric, data = fabric)\n\n$fabric\n                   diff      lwr      upr     p adj\nFabric 2-Fabric 1 -5.02 -8.09676 -1.94324 0.0013227\nFabric 3-Fabric 1 -6.54 -9.61676 -3.46324 0.0000851\nFabric 4-Fabric 1 -4.80 -7.87676 -1.72324 0.0019981\nFabric 3-Fabric 2 -1.52 -4.59676  1.55676 0.5094118\nFabric 4-Fabric 2  0.22 -2.85676  3.29676 0.9968426\nFabric 4-Fabric 3  1.74 -1.33676  4.81676 0.3968476"
  },
  {
    "objectID": "slides/Chapter08.html#tukey-hsd-interval-plot",
    "href": "slides/Chapter08.html#tukey-hsd-interval-plot",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Tukey HSD (Interval Plot)",
    "text": "Tukey HSD (Interval Plot)"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-two-factor-anova",
    "href": "slides/Chapter08.html#two-way-two-factor-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two way (two factor) ANOVA",
    "text": "Two way (two factor) ANOVA\n\nTwo factors are present.\nA two-way ANOVA is used to estimate how the mean of a continuous variable changes according to the levels of two categorical variables.\nUse a two-way ANOVA when you want to know how two independent variables, in combination, affect a dependent variable.\nVery similar to multiple regression but with categorical variables."
  },
  {
    "objectID": "slides/Chapter08.html#two-way-two-factor-anova-example",
    "href": "slides/Chapter08.html#two-way-two-factor-anova-example",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two way (two factor) ANOVA example",
    "text": "Two way (two factor) ANOVA example\n\nExample: Twelve tanks, six each with water from one of the two lakes were set up. Three nutrient supplements were added to each tank. The count of zooplankton in a unit volume of water was then noted.\n\n\n\n Zooplankton Supplement     Lake\n          34          1     Rose\n          43          1     Rose\n          57          1 Dennison\n          40          1 Dennison\n          85          2     Rose\n          68          2     Rose\n          67          2 Dennison\n          53          2 Dennison\n          41          3     Rose\n          24          3     Rose\n          42          3 Dennison\n          52          3 Dennison"
  },
  {
    "objectID": "slides/Chapter08.html#run-a-two-way-anova",
    "href": "slides/Chapter08.html#run-a-two-way-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Run a Two-way ANOVA",
    "text": "Run a Two-way ANOVA\n\nsummary(aov(Zooplankton~factor(Supplement)+Lake, data=zooplankton))\n\n                   Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfactor(Supplement)  2 1918.5   959.2   6.486 0.0212 *\nLake                1   21.3    21.3   0.144 0.7140  \nResiduals           8 1183.2   147.9                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA two-way ANOVA tests two null hypotheses at the same time:\n\nAll group means are equal at each level of the first variable\nAll group means are equal at each level of the second variable"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit",
    "href": "slides/Chapter08.html#two-way-model-fit",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nModel:\nObservation = Overall Mean + Factor 1 Effect + Factor 2 Effect + Error\n\n\n\\(H_0\\): factor 1 means are equal; factor 2 means are equal\n\n\n\n                   Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfactor(Supplement)  2 1918.5   959.2   6.486 0.0212 *\nLake                1   21.3    21.3   0.144 0.7140  \nResiduals           8 1183.2   147.9                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSupplement effect is significant at 5% level but not the Lake effect"
  },
  {
    "objectID": "slides/Chapter08.html#main-effect-plots",
    "href": "slides/Chapter08.html#main-effect-plots",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Main effect plots",
    "text": "Main effect plots\n\nSimply plot of response means for factor levels"
  },
  {
    "objectID": "slides/Chapter08.html#tukey-hsd-1",
    "href": "slides/Chapter08.html#tukey-hsd-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Tukey HSD",
    "text": "Tukey HSD\n\nzoo &lt;- aov(Zooplankton~factor(Supplement)+Lake, data=zooplankton)\nTukeyHSD(zoo)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Zooplankton ~ factor(Supplement) + Lake, data = zooplankton)\n\n$`factor(Supplement)`\n      diff         lwr       upr     p adj\n2-1  24.75   0.1779866 49.322013 0.0485023\n3-1  -3.75 -28.3220134 20.822013 0.9017260\n3-2 -28.50 -53.0720134 -3.927987 0.0256900\n\n$Lake\n                   diff       lwr      upr     p adj\nRose-Dennison -2.666667 -18.85781 13.52447 0.7139803"
  },
  {
    "objectID": "slides/Chapter08.html#summary-so-far",
    "href": "slides/Chapter08.html#summary-so-far",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Summary so far",
    "text": "Summary so far\n\nANOVA: 1 and 2 factor\nANOVA table\nAssumptions\nTukey’s HSD"
  },
  {
    "objectID": "slides/Chapter08.html#learning-objectives-part-2",
    "href": "slides/Chapter08.html#learning-objectives-part-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Learning Objectives: Part 2",
    "text": "Learning Objectives: Part 2\n\nUnderstand the use of interactions in linear models\nUse a two-way ANOVA with interactions on example data\nUnderstand and calculate an ANOVA table when interactions are present\nDescribe Tukey’s post hoc tests and assumptions of ANOVAs with interactions\nUnderstand the difference and similarities between this chapter and previous linear models covered"
  },
  {
    "objectID": "slides/Chapter08.html#interaction-effect",
    "href": "slides/Chapter08.html#interaction-effect",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction effect",
    "text": "Interaction effect\n\nWhether Factor A effects are constant over Factor B effects or Factor B effects are constant over Factor A effects?\n\nIf the answer is no, then there is an interaction between A & B.\n\nExample:\n\nTemperature and pressure are factors affecting the yield in chemical experiments.\nThey do interact in a mechanistic sense.\n\n\nInteraction may or may not have physical meaning."
  },
  {
    "objectID": "slides/Chapter08.html#two-way-anova-and-interaction-effects",
    "href": "slides/Chapter08.html#two-way-anova-and-interaction-effects",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way ANOVA and Interaction effects",
    "text": "Two-way ANOVA and Interaction effects\nA two-way ANOVA with interaction tests three null hypotheses at the same time:\n\nAll group means are equal at each level of the first variable\nAll group means are equal at each level of the second variable\nThere is no interaction effect between the two variables"
  },
  {
    "objectID": "slides/Chapter08.html#interaction-plots",
    "href": "slides/Chapter08.html#interaction-plots",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction Plots",
    "text": "Interaction Plots\n\nIn the absence of interaction, the plotted means of factor will be roughly parallel\n\nsee Plot 1. A & B do not interact.\n\nIf the the plotted means of factor crossings are far from parallel, then there is interaction\n\nPlot 2 shows extreme (antagonistic) interaction between A & B."
  },
  {
    "objectID": "slides/Chapter08.html#interaction-plot-for-zooplankton-data",
    "href": "slides/Chapter08.html#interaction-plot-for-zooplankton-data",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interaction Plot for Zooplankton data",
    "text": "Interaction Plot for Zooplankton data\n\nattach(zooplankton)\nlibrary(ggplot2)\np1= ggplot(data = zooplankton, aes(x = factor(Supplement), y = Zooplankton, group=Lake, colour=Lake)) +\n  stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_abline(intercept = mean(Zooplankton), slope=0)+ \n  theme_bw()+ggtitle(\"Lake*Supplement Interaction effect\")\np2= ggplot(data = zooplankton, aes(x = Lake, y = Zooplankton, group=factor(Supplement), colour=factor(Supplement))) +stat_summary(fun=mean, geom=\"point\")+\n  stat_summary(fun=mean, geom=\"line\")+\n  geom_abline(intercept = mean(Zooplankton), slope=0)+ \n  theme_bw()+ggtitle(\"Lake*Supplement Interaction effect\")\nlibrary(patchwork)\np1+p2\n\n\n\nInteraction effect may be present"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit-1",
    "href": "slides/Chapter08.html#two-way-model-fit-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nobservation = mean + Factor A + Factor B + interaction effect + error\n\n\nThe above model is known as multiplicative model\nIf interaction effect is ignored, we deal with an additive model\n\nExample: zooplankton data"
  },
  {
    "objectID": "slides/Chapter08.html#two-way-model-fit-2",
    "href": "slides/Chapter08.html#two-way-model-fit-2",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Two-way model fit",
    "text": "Two-way model fit\n\nzooplankton |&gt; \n  mutate(Supplement = factor(Supplement)) |&gt; \n  aov(formula = Zooplankton ~ Supplement * Lake) |&gt; \n  summary()\n\n                Df Sum Sq Mean Sq F value Pr(&gt;F)  \nSupplement       2 1918.5   959.2   9.253 0.0147 *\nLake             1   21.3    21.3   0.206 0.6660  \nSupplement:Lake  2  561.2   280.6   2.707 0.1453  \nResiduals        6  622.0   103.7                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter08.html#residual-diagnostics",
    "href": "slides/Chapter08.html#residual-diagnostics",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics"
  },
  {
    "objectID": "slides/Chapter08.html#interpreting-interactions",
    "href": "slides/Chapter08.html#interpreting-interactions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Interpreting interactions",
    "text": "Interpreting interactions\n\nIf an interaction term is significant, the effect of Factor A depends on Factor B (and vice versa)\nUse interaction plots to visualize\nPosthoc tests to examine different level combinations\n\n\nTukeyHSD(modl)$`Supplement:Lake`\n\n                       diff        lwr       upr      p adj\n2:Dennison-1:Dennison  11.5 -29.021538 52.021538 0.85366635\n3:Dennison-1:Dennison  -1.5 -42.021538 39.021538 0.99998404\n1:Rose-1:Dennison     -10.0 -50.521538 30.521538 0.90834374\n2:Rose-1:Dennison      28.0 -12.521538 68.521538 0.19396469\n3:Rose-1:Dennison     -16.0 -56.521538 24.521538 0.64065141\n3:Dennison-2:Dennison -13.0 -53.521538 27.521538 0.78826884\n1:Rose-2:Dennison     -21.5 -62.021538 19.021538 0.38790493\n2:Rose-2:Dennison      16.5 -24.021538 57.021538 0.61553864\n3:Rose-2:Dennison     -27.5 -68.021538 13.021538 0.20490820\n1:Rose-3:Dennison      -8.5 -49.021538 32.021538 0.94964396\n2:Rose-3:Dennison      29.5 -11.021538 70.021538 0.16443780\n3:Rose-3:Dennison     -14.5 -55.021538 26.021538 0.71593144\n2:Rose-1:Rose          38.0  -2.521538 78.521538 0.06517642\n3:Rose-1:Rose          -6.0 -46.521538 34.521538 0.98804470\n3:Rose-2:Rose         -44.0 -84.521538 -3.478462 0.03503205"
  },
  {
    "objectID": "slides/Chapter08.html#importance-of-interactions",
    "href": "slides/Chapter08.html#importance-of-interactions",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Importance of interactions",
    "text": "Importance of interactions\n\nWhen you have statistically significant interaction effects, you can’t interpret the main effects without considering the interactions\nInteractions can be used to control for unhelpful variation (blocking effect in experiments)\nInteractions can be mechanistically meaningful"
  },
  {
    "objectID": "slides/Chapter08.html#anova-extensions-and-inference",
    "href": "slides/Chapter08.html#anova-extensions-and-inference",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA extensions and inference",
    "text": "ANOVA extensions and inference\nLinear models can theoretically be performed with an infinite amount of predictors\nThe more predictors the more samples needed, your effect sample number is no longer your total sample size but the lowest treatment\nInteraction effects with more than 3 (sometimes even more than 2) become very difficult to interpret and visualize\nGood experimental design and causal thinking can aid in analysis design, design test then collect data, it is much harder the other way around"
  },
  {
    "objectID": "slides/Chapter08.html#indicator-variables",
    "href": "slides/Chapter08.html#indicator-variables",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nIndicator variables are used if the predictor is qualitative rather than quantitative.\n\nConsider gender, a categorical variable.\n\nLet \\(I_1\\) be an indicator variable that takes a value 1 for males and 0 for females.\n\nLet \\(I_2\\) takes 1 for females and 0 for males.\nNote only one of \\(I_1\\) & \\(I_2\\) is sufficient.\n\n\nThe minimum number of indicator variables needed is related to degrees of freedom."
  },
  {
    "objectID": "slides/Chapter08.html#anova-through-regression",
    "href": "slides/Chapter08.html#anova-through-regression",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANOVA through regression",
    "text": "ANOVA through regression\n\nConsider the burn-time data for the four fabrics.\n\nThe four fabric types are categorical.\nDefine-\n\n\\(I_1\\) = 1 for fabric 1 and 0 otherwise\n\\(I_2\\) = 1 for fabric 2 and 0 otherwise\n\\(I_3\\) = 1 for fabric 3 and 0 otherwise\n\\(I_4\\) = 1 for fabric 4 and 0 otherwise\n\n\nNote that any THREE indicator variables are sufficient for the four fabrics.\n\n3 df for 4 fabrics"
  },
  {
    "objectID": "slides/Chapter08.html#regression-summary",
    "href": "slides/Chapter08.html#regression-summary",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Regression summary",
    "text": "Regression summary\n\nRegress the burn-time response on the indicator variables \\(I_1\\),\\(I_2\\), \\(I_3\\) & \\(I_4\\)\n\n\n\n   burntime   fabric I1 I2 I3 I4\n1      17.8 Fabric 1  1  0  0  0\n2      16.2 Fabric 1  1  0  0  0\n3      17.5 Fabric 1  1  0  0  0\n4      17.4 Fabric 1  1  0  0  0\n5      15.0 Fabric 1  1  0  0  0\n6      11.2 Fabric 2  0  1  0  0\n7      11.4 Fabric 2  0  1  0  0\n8      15.8 Fabric 2  0  1  0  0\n9      10.0 Fabric 2  0  1  0  0\n10     10.4 Fabric 2  0  1  0  0\n11     11.8 Fabric 3  0  0  1  0\n12     11.0 Fabric 3  0  0  1  0\n13     10.0 Fabric 3  0  0  1  0\n14      9.2 Fabric 3  0  0  1  0\n15      9.2 Fabric 3  0  0  1  0\n16     14.9 Fabric 4  0  0  0  1\n17     10.8 Fabric 4  0  0  0  1\n18     12.8 Fabric 4  0  0  0  1\n19     10.7 Fabric 4  0  0  0  1\n20     10.7 Fabric 4  0  0  0  1"
  },
  {
    "objectID": "slides/Chapter08.html#regression-and-anova",
    "href": "slides/Chapter08.html#regression-and-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Regression and ANOVA",
    "text": "Regression and ANOVA\n\n\n\nCall:\nlm(formula = burntime ~ I1 + I2 + I3, data = fabric)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.780 -1.205 -0.460  0.775  4.040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.9800     0.7604  15.754 3.65e-11 ***\nI1            4.8000     1.0754   4.463 0.000392 ***\nI2           -0.2200     1.0754  -0.205 0.840485    \nI3           -1.7400     1.0754  -1.618 0.125206    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.7 on 16 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.6706 \nF-statistic: 13.89 on 3 and 16 DF,  p-value: 0.0001016\n\n\n\nCompare with the one-way output\n\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfabric       3 120.50   40.17   13.89 0.000102 ***\nResiduals   16  46.26    2.89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/Chapter08.html#regression-and-anova-1",
    "href": "slides/Chapter08.html#regression-and-anova-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Regression and ANOVA",
    "text": "Regression and ANOVA\n\nfabric |&gt; lm(formula = burntime~I1+I2+I3) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: burntime\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nI1         1 111.521 111.521 38.5718 1.248e-05 ***\nI2         1   1.408   1.408  0.4871    0.4952    \nI3         1   7.569   7.569  2.6179    0.1252    \nResiduals 16  46.260   2.891                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCompare with the one-way output\n\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = burntime ~ fabric, data = fabric)\n\n$fabric\n                   diff      lwr      upr     p adj\nFabric 2-Fabric 1 -5.02 -8.09676 -1.94324 0.0013227\nFabric 3-Fabric 1 -6.54 -9.61676 -3.46324 0.0000851\nFabric 4-Fabric 1 -4.80 -7.87676 -1.72324 0.0019981\nFabric 3-Fabric 2 -1.52 -4.59676  1.55676 0.5094118\nFabric 4-Fabric 2  0.22 -2.85676  3.29676 0.9968426\nFabric 4-Fabric 3  1.74 -1.33676  4.81676 0.3968476"
  },
  {
    "objectID": "slides/Chapter08.html#analysis-of-covariance-ancova",
    "href": "slides/Chapter08.html#analysis-of-covariance-ancova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Analysis of Covariance (ANCOVA)",
    "text": "Analysis of Covariance (ANCOVA)\nAnalysis of covariance (ANCOVA) is a statistical method that combines linear regression and analysis of variance (ANOVA) to evaluate the relationship between a response variable and various independent variables while controlling for covariates."
  },
  {
    "objectID": "slides/Chapter08.html#analysis-of-covariance-ancova-1",
    "href": "slides/Chapter08.html#analysis-of-covariance-ancova-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Analysis of Covariance (ANCOVA)",
    "text": "Analysis of Covariance (ANCOVA)\n\nIndicator variables are used as additional regressors along with a quantitative predictor (covariate).\n\n\n\n Restaurant  Sales Households Location I1 I2\n          1 135.27        155  Highway  0  0\n          2  72.74         93  Highway  0  0\n          3 114.95        128  Highway  0  0\n          4 102.93        114  Highway  0  0\n          5 131.77        158  Highway  0  0\n          6 160.91        183  Highway  0  0\n          7 179.86        178     Mall  1  0\n          8 220.14        215     Mall  1  0\n          9 179.64        172     Mall  1  0\n         10 185.92        197     Mall  1  0\n         11 207.82        207     Mall  1  0\n         12 113.51         95     Mall  1  0\n         13 203.98        224   Street  0  1\n         14 174.48        199   Street  0  1\n         15 220.43        240   Street  0  1\n         16  93.19        100   Street  0  1\n\n\n\nPlot of data. Do we need three separate models?"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-fit",
    "href": "slides/Chapter08.html#ancova-fit",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA fit",
    "text": "ANCOVA fit\n\nrestaurant |&gt; \n  lm(formula = Sales~Households*Location) |&gt; \n  summary() |&gt; coef() |&gt; round(digits=3)\n\n                          Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 -6.203     11.818  -0.525    0.611\nHouseholds                   0.909      0.083  10.906    0.000\nLocationMall                39.223     16.451   2.384    0.038\nLocationStreet               8.036     16.273   0.494    0.632\nHouseholds:LocationMall     -0.074      0.104  -0.710    0.494\nHouseholds:LocationStreet   -0.012      0.101  -0.120    0.907"
  },
  {
    "objectID": "slides/Chapter08.html#ancova-fit-1",
    "href": "slides/Chapter08.html#ancova-fit-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "ANCOVA fit",
    "text": "ANCOVA fit\n\n\nIn order to allow for different slopes for each location, we define the product (or interaction) variables \\(I_1*X\\), and \\(I_2*X\\) (\\(X\\) being the covariate, Households)\nLocation is categorical with levels Mall, Street, and Highway"
  },
  {
    "objectID": "slides/Chapter08.html#explanation-of-fit",
    "href": "slides/Chapter08.html#explanation-of-fit",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Explanation of fit",
    "text": "Explanation of fit\n\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   -6.203     11.818  -0.525    0.611\nX              0.909      0.083  10.906    0.000\nI1            39.223     16.451   2.384    0.038\nI2             8.036     16.273   0.494    0.632\nX:I1          -0.074      0.104  -0.710    0.494\nX:I2          -0.012      0.101  -0.120    0.907\n\n\n\nFor \\(I_1=0\\) & \\(I_2=0\\), the model becomes the fit for Highway\n\nSo this set-up compares Mall/Street with Highway\n\nSignificant \\(I_1\\) coefficient means that Mall location has a constant level of higher sales\nMall location model has a higher intercept but the slopes are the same for all three locations."
  },
  {
    "objectID": "slides/Chapter08.html#graphing-the-model",
    "href": "slides/Chapter08.html#graphing-the-model",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Graphing the model",
    "text": "Graphing the model"
  },
  {
    "objectID": "slides/Chapter08.html#summary",
    "href": "slides/Chapter08.html#summary",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Summary",
    "text": "Summary\nANOVA models study categorical predictors (factors).\n-   Interaction between factors is important. \n-   ANOVA models and regression models are related and fall under a general family of linear models.\nANCOVA models employs both numerical variables (covariates) and qualitative factors for modelling.\n-   Interaction between factors and covariates is important."
  },
  {
    "objectID": "slides/Chapter08.html#linear-models",
    "href": "slides/Chapter08.html#linear-models",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Linear models",
    "text": "Linear models\nFamily tree\nT-tests, regressions, ANOVAs, and ANCOVAs are related\nSimilarities:\n-   1 continuous response variable\n-   Assumptions: normality, equal variance, and independence of residuals\nDifferences:\n- EDA\n- test statistics\n- number and type of predictors"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a t.test and an ANOVA?",
    "text": "What’s the difference between a t.test and an ANOVA?\nA t-test is used to determine whether or not there is a statistically significant difference between the means of two groups\n\nsample group vs hypothetical population One-sample t-test\ntwo independent samples Two-sample t-test\n\nalso called independent t-test because the two samples are considered independent\n\nsamples linked in some why, not independent Paired t-test\n\nAn ANOVA is used to determine whether or not there is a statistically significant difference between the means of three or more groups, but not which group\n\none factor split into 3 or more groups (levels) One-way ANOVA\ntwo factors split into 3 or more groups (levels) Two-way ANOVA\nmore than two factors Three factor ANOVA etc…"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova-1",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-t.test-and-an-anova-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a t.test and an ANOVA?",
    "text": "What’s the difference between a t.test and an ANOVA?\nThe main difference between a t-test and an ANOVA is in how the two tests calculate their test statistic to determine if there is a statistically significant difference between groups.\nT-test:\n\nT statistic: ratio of the mean difference between two groups relative to overal standard deviation of the differences\nOne-sample t-test: \\(t = \\frac{x-\\mu}{s/\\sqrt n}\\)\nTwo-sample t-test: \\(t = \\frac{(\\bar{x_{1}}-\\bar{x_{2}})-d}{\\frac{s_1}{\\sqrt n} + \\frac{s_2}{\\sqrt n}}\\)\n\nPaired t-test: \\(t=\\frac{\\bar{d}}{\\frac{S_{d}}{\\sqrt n}}\\)\n\n\\(s =\\) sample standard deviation \\(d =\\) difference\nANOVA:\n\nF statistic: ratio of the variance between the groups relative to the variance within the groups\n\n\\(F = \\frac{s_b^2}{s_w^2}\\) Where \\(s_b^2\\) is the between sample variance, and \\(s_w^2\\) is the within sample variance.\nMSF/MSE"
  },
  {
    "objectID": "slides/Chapter08.html#when-to-use-a-t.test-or-an-anova",
    "href": "slides/Chapter08.html#when-to-use-a-t.test-or-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "When to use a t.test or an ANOVA?",
    "text": "When to use a t.test or an ANOVA?\nIn practice, when we want to compare the means of two groups, we use a t-test. When we want to compare the means of three or more groups, we use an ANOVA.\nSuppose we have three groups we wish to compare the means between: group A, group B, and group C.\nIf you did the following t-tests:\n\nA t-test to compare the difference in means between group A and group B\nA t-test to compare the difference in means between group A and group C\nA t-test to compare the difference in means between group B and group C"
  },
  {
    "objectID": "slides/Chapter08.html#what-if-we-just-did-many-t-tests",
    "href": "slides/Chapter08.html#what-if-we-just-did-many-t-tests",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What if we just did many t-tests?",
    "text": "What if we just did many t-tests?\nFor each t-test there is a chance that we will commit a type I error, which is the probability that we reject the null hypothesis when it is actually true. Let’s say we set this to 0.05 (\\(\\alpha = 0.05\\)). This means that when we perform multiple t-tests, this error rate increases.\n\nThe probability that we commit a type I error with one t-test is 1 – 0.95 = 0.05.\nThe probability that we commit a type I error with two t-tests is 1 – (0.95^2) = 0.0975.\nThe probability that we commit a type I error with three t-tests is 1 – (0.95^3) = 0.1427.\n\nThe type I error just increases!\nWe use a post-hoc (after) test to see which group is driving differences, these pairwise comparisons have a correction factor to adjust our Type I error"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a regression and an ANOVA?",
    "text": "What’s the difference between a regression and an ANOVA?\nA regression is used to understand the relationship between predictor variable(s) and a response variable\n\none predictor Simple Regression\ntwo or more predictors Multiple Regression\n\nAn ANOVA is used to determine whether or not there is a statistically significant difference between the means of three or more groups, but not which group\n\none factor split into 3 or more groups (levels) One-way ANOVA\ntwo factors split into 3 or more groups (levels) Two-way ANOVA\nmore than two factors Three factor ANOVA etc…\n\nNot too much difference there"
  },
  {
    "objectID": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova-1",
    "href": "slides/Chapter08.html#whats-the-difference-between-a-regression-and-an-anova-1",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "What’s the difference between a regression and an ANOVA?",
    "text": "What’s the difference between a regression and an ANOVA?\nRegression:\n\nT statistic: test if each predictor’s estimate is different from 0\n\\(R^2\\): proportion of variance explained \\(\\frac{SSR}{SST}\\)\nF statistic: overall F statistic for the regression model, \\(\\frac{MS_{reg}}{MS_{error}}\\) where \\(MS = \\frac{SS}{df}\\)\n\nANOVA:\n\nF statistic: ratio of the variance between the groups relative to the variance within the groups\n\n\\(F = \\frac{s_b^2}{s_w^2}\\)\n\nwhere \\(s_b^2\\) is the between sample variance, and \\(s_w^2\\) is the within sample variance.\n\nMSF/MSE\n\nThe F statistic is the same, thats why you can produce an ANOVA table from your regression"
  },
  {
    "objectID": "slides/Chapter08.html#when-to-use-a-regression-or-an-anova",
    "href": "slides/Chapter08.html#when-to-use-a-regression-or-an-anova",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "When to use a regression or an ANOVA?",
    "text": "When to use a regression or an ANOVA?\nConventionally, a regression refers to continuous predictors and ANOVAs are used for discrete variables.\nBUT …\nYou can code discrete variable as indicator variables and then treat them as continuous and run a regression!\nGenerally, if you have 1 continuous and 1 discrete variable you should generate indicator variables and run a multiple regression\nIf you are using the continuous predictor as a covariate it would be called an ANCOVA.\nYou can code a discrete variable into continuous but if you do the reverse you lose statistical inference."
  },
  {
    "objectID": "slides/Chapter08.html#reminder-on-hypotheses",
    "href": "slides/Chapter08.html#reminder-on-hypotheses",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Reminder on Hypotheses",
    "text": "Reminder on Hypotheses\nT-Test: difference between the means of two groups\n\nNull: \\(\\mu_{1} - \\mu_{2}\\) is equal to \\(0\\) Can be \\(=\\), \\(\\le\\), or \\(\\ge\\)\nAlt: \\(\\mu_{1} - \\mu_{2}\\) is not equal to \\(0\\) Can be \\(\\ne\\), \\(&lt;\\), or \\(&gt;\\)\n\nRegressions: understand the relationship between predictor variable(s) and a response variable\n\nNull: true slope coefficient is \\(= 0\\) (i.e. no relationship)\nAlt: true slope coefficient \\(\\ne 0\\) (i.e. relationship)\n\nANOVA: difference between the means of three or more groups\n\nNull: group means are equal\nAlt: At least one mean is different"
  },
  {
    "objectID": "slides/Chapter08.html#where-to-go-from-here",
    "href": "slides/Chapter08.html#where-to-go-from-here",
    "title": "Chapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)",
    "section": "Where to go from here?",
    "text": "Where to go from here?\nLinear models form the basis of a lot more statistics tests. \n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter06.html#what-is-a-statistical-model",
    "href": "slides/Chapter06.html#what-is-a-statistical-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "What is a statistical model?",
    "text": "What is a statistical model?\n\nOBSERVATION = FIT + RESIDUAL\n\nFIT- to explain systematic (non-random) variation in the data\nRESIDUAL - to explain random variation in the data"
  },
  {
    "objectID": "slides/Chapter06.html#what-is-a-statistical-model-1",
    "href": "slides/Chapter06.html#what-is-a-statistical-model-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "What is a statistical model?",
    "text": "What is a statistical model?\n\nOBSERVATION = FIT + RESIDUAL\n\n\nWith paired (related) data (X,Y)\nTwo variables: one (Y) is random (response) and the other (X) is considered fixed (predictor)\nInterested in the functional relationship between these two variables\n\n\\(Y=f(X)\\)\npredict Y, given X"
  },
  {
    "objectID": "slides/Chapter06.html#what-is-a-statistical-model-2",
    "href": "slides/Chapter06.html#what-is-a-statistical-model-2",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "What is a statistical model?",
    "text": "What is a statistical model?\n\nOBSERVATION = FIT + RESIDUAL\n\n\nWith paired (related) data (X,Y)\n\nFitted Model: \\(\\hat{Y}=a+bX\\) (True Model: \\(Y=\\alpha+\\beta X+\\epsilon\\))\nresidual error: \\(e= Y-\\hat{Y}\\) (\\(\\epsilon\\) is not the same as \\(e\\))\n\nWhile fitting models to data\n\nit is required fit the model to the data as closely as possible (e.g. using least squares technique)\nwe need to make residuals free of patterns or trends.\nwe assume a probability distribution of residuals for statistical inference."
  },
  {
    "objectID": "slides/Chapter06.html#predict-y-given-x",
    "href": "slides/Chapter06.html#predict-y-given-x",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Predict Y, given X",
    "text": "Predict Y, given X\n\n\nA regression equation is a function that indicates how the average value of one response variable for given values of one or more predictor variables varies with these predictor variables; that is, \\(E(Y|X_1, X_2, ..., X_k)\\)"
  },
  {
    "objectID": "slides/Chapter06.html#simple-regression",
    "href": "slides/Chapter06.html#simple-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Simple regression",
    "text": "Simple regression\n\nThe term simple means there is a single predictor\nThe fitted model remains as: \\(\\hat{Y}=a+bX_{i}\\)\n\n\n\\(\\hat{Y}\\) is the predicted value of \\(y_{i}\\) given \\(x_{i}\\). Also called ‘fitted’ values, they are the linear funtion of X.\n\\(a\\) the intercept on the y-axis; value of \\(\\hat{Y}\\) when \\(x_{i}=0\\).\n\\(b\\) the slope of the line; the change in \\(\\hat{Y}\\) with a 1-unit increase in \\(x_{i}\\)."
  },
  {
    "objectID": "slides/Chapter06.html#example-alcohol-consumption-data",
    "href": "slides/Chapter06.html#example-alcohol-consumption-data",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Example (Alcohol consumption data)",
    "text": "Example (Alcohol consumption data)\n\n\n country Alcohol Death\n       1    22.9  30.0\n       2    15.2  23.6\n       3    12.3  18.9\n       4    11.9   5.0\n       5    10.8  12.3\n       6     9.9  14.2\n       7     8.3   7.4\n       8     7.2   3.0\n       9     6.6   7.2\n      10     5.8  10.6\n      11     5.7   3.7\n      12     5.6   3.4\n      13     4.2   4.3\n      14     3.9   3.6\n      15     3.1   5.4"
  },
  {
    "objectID": "slides/Chapter06.html#plot-of-alcohol-consumption-data",
    "href": "slides/Chapter06.html#plot-of-alcohol-consumption-data",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Plot of Alcohol consumption data",
    "text": "Plot of Alcohol consumption data"
  },
  {
    "objectID": "slides/Chapter06.html#plot-of-alcohol-consumption-data-1",
    "href": "slides/Chapter06.html#plot-of-alcohol-consumption-data-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Plot of Alcohol consumption data",
    "text": "Plot of Alcohol consumption data\n\n\\(min \\left( \\sum_{i=1}(y_{i}-\\hat{y_{i}})^2 \\right)\\)\n\n\\(i\\) number of observations\n\\(y_{i}\\) observed value of y\n\\(\\hat{y_{i}}\\) predicted value of y"
  },
  {
    "objectID": "slides/Chapter06.html#r-base-output",
    "href": "slides/Chapter06.html#r-base-output",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "R Base Output",
    "text": "R Base Output\n\nmod1 &lt;- lm(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1)\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n\n\n\nLet us tidy it."
  },
  {
    "objectID": "slides/Chapter06.html#fitted-model-and-testing-its-coefficients",
    "href": "slides/Chapter06.html#fitted-model-and-testing-its-coefficients",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Fitted model and testing its coefficients",
    "text": "Fitted model and testing its coefficients\n\n\nlibrary(broom)\ntidy(mod1) |&gt; mutate_if(is.numeric, round, 3) -&gt; out1\nlibrary(kableExtra)\nkable(out1, caption = \"t-tests for model parameters\") %&gt;% \n  kable_classic(full_width = F)\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.318\n2.065\n-1.123\n0.282\n\n\nAlcohol\n1.405\n0.202\n6.954\n0.000\n\n\n\n\n\n\n\n\n\nFocus on the model, its coefficients\n\nWhat is the scale of these variables? Are these coefficient estimates meaningful in the context?\nWhat is the error around these estimates?"
  },
  {
    "objectID": "slides/Chapter06.html#types-of-residuals",
    "href": "slides/Chapter06.html#types-of-residuals",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Types of residuals",
    "text": "Types of residuals\n\nraw or ordinary residual is just (observation-fit)\nStandardized Residual= Residual/Std.Dev of residual\nThe regression model is influenced by outliers or unusual points because the slope estimate of the regression line is sensitive to these outliers.\nSo we define Studentised or deleted t Residual\nSimilar to Standardized Residual without the observation under consideration. That is,\n\nStudentised residual = residual/std. dev of residual (after omitting the particular observation)."
  },
  {
    "objectID": "slides/Chapter06.html#assumptions",
    "href": "slides/Chapter06.html#assumptions",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Assumptions",
    "text": "Assumptions\nModel forming assumptions\n\n\\(X\\) is known without error\n\\(Y\\) is linearly related to \\(X\\)\nThere is a random variability of \\(Y\\) about this line"
  },
  {
    "objectID": "slides/Chapter06.html#assumptions-1",
    "href": "slides/Chapter06.html#assumptions-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Assumptions",
    "text": "Assumptions\nMore assumptions to form \\(t\\) and \\(F\\) statistics:\n\nVariability in \\(Y\\) about the line is constant and independent of \\(X\\) variable.\nThe variability of \\(Y\\) about the line follows normal distribution.\nThe distribution of \\(Y\\) given \\(X = X_i\\) is independent of \\(Y\\) given \\(X = X_j\\)."
  },
  {
    "objectID": "slides/Chapter06.html#residuals-and-assumptions",
    "href": "slides/Chapter06.html#residuals-and-assumptions",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residuals and assumptions",
    "text": "Residuals and assumptions\n\nMost assumptions are on the errors (residuals)\n\nindependent\nnormal\nrandom\n\nIs there a pattern in my residuals?\nDo they suggest what to do?"
  },
  {
    "objectID": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model",
    "href": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residual plot for Alcohol~deaths model",
    "text": "Residual plot for Alcohol~deaths model\n\nFor small sample sizes, residual diagnostics is difficult"
  },
  {
    "objectID": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model-1",
    "href": "slides/Chapter06.html#residual-plot-for-alcoholdeaths-model-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residual plot for Alcohol~deaths model",
    "text": "Residual plot for Alcohol~deaths model\n\npar(mfrow=(c(2,2)))\nplot(mod1)"
  },
  {
    "objectID": "slides/Chapter06.html#residuals-showing-need-for-transformation",
    "href": "slides/Chapter06.html#residuals-showing-need-for-transformation",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residuals showing need for transformation",
    "text": "Residuals showing need for transformation\n\nNon-constant Residual Variation"
  },
  {
    "objectID": "slides/Chapter06.html#adding-more-predictors",
    "href": "slides/Chapter06.html#adding-more-predictors",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Adding more predictors",
    "text": "Adding more predictors"
  },
  {
    "objectID": "slides/Chapter06.html#subgrouping-patterns",
    "href": "slides/Chapter06.html#subgrouping-patterns",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Subgrouping patterns",
    "text": "Subgrouping patterns"
  },
  {
    "objectID": "slides/Chapter06.html#outliers",
    "href": "slides/Chapter06.html#outliers",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/Chapter06.html#autocorrelation",
    "href": "slides/Chapter06.html#autocorrelation",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nNeighbouring residuals depend on each other"
  },
  {
    "objectID": "slides/Chapter06.html#improving-simple-regression",
    "href": "slides/Chapter06.html#improving-simple-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Improving simple regression",
    "text": "Improving simple regression\n\nUse a different predictor or explanatory variable\nTransform the \\(Y\\) variable\nAdd other explanatory variables to the model (next week)\nDeletion of invalid (as opposed to outlier) observations\nReconsider the linear relationship"
  },
  {
    "objectID": "slides/Chapter06.html#tests-of-significance",
    "href": "slides/Chapter06.html#tests-of-significance",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Tests of significance",
    "text": "Tests of significance\n\n\nlibrary(broom)\ntidy(mod1) |&gt; mutate_if(is.numeric, round, 3) -&gt; out1\nlibrary(kableExtra)\nkable(out1, caption = \"t-tests for model parameters\") %&gt;% \n  kable_classic(full_width = F)\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.318\n2.065\n-1.123\n0.282\n\n\nAlcohol\n1.405\n0.202\n6.954\n0.000\n\n\n\n\n\n\n\n\n-   $t$ tests of intercept and slope\n\n-   $H_0=true~slope=0$; $H_0=true~intercept=0$\n\n    -   For `Alcohol~deaths` model, the slope is significant\n        but not the intercept"
  },
  {
    "objectID": "slides/Chapter06.html#model-quality-measures",
    "href": "slides/Chapter06.html#model-quality-measures",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Model quality measures",
    "text": "Model quality measures\n\nsummary(mod1)\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n\n\n\nModel summary (or Quality) measures\n\n\\(R^2\\) is the proportion of variation explained by the fitted model\n\nA meaningful model must have at least 50% \\(R^2\\)\nA large \\(R^2\\) is important to explain the relationship(s)\n\nResidual standard deviation (error) \\(S\\) has to be small\n\nHow small? Difficult to say. Compare \\(S\\) with the overall spread in \\(Y\\) or with the mean of \\(Y\\)\nA small \\(S\\) is important for prediction"
  },
  {
    "objectID": "slides/Chapter06.html#model-quality-measures-1",
    "href": "slides/Chapter06.html#model-quality-measures-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Model quality measures",
    "text": "Model quality measures\n\n\nout1 &lt;- glance(mod1) |&gt; \n  select(r.squared, sigma, statistic, p.value) |&gt; \n  mutate_if(is.numeric, round, 2)\n\nout1 |&gt; t() |&gt; \n  kable(caption = \"Model summary measures\") |&gt; \n  kable_classic(full_width = T) \n\n\n\nModel summary measures\n\n\nr.squared\n0.79\n\n\nsigma\n3.94\n\n\nstatistic\n48.35\n\n\np.value\n0.00"
  },
  {
    "objectID": "slides/Chapter06.html#anova-f-test",
    "href": "slides/Chapter06.html#anova-f-test",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA \\(F\\)-test",
    "text": "ANOVA \\(F\\)-test\n\n\nmod1  &lt;-  lm(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1) \n\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05\n\n\n\n\n\nanova(mod1)\n\n\nAnalysis of Variance Table\n\nResponse: Death\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nAlcohol    1 751.44  751.44  48.353 1.001e-05 ***\nResiduals 13 202.03   15.54                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nmod1.aov  &lt;-  aov(Death ~ Alcohol, data=cirrhosis)\nsummary(mod1.aov) \n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nAlcohol      1  751.4   751.4   48.35  1e-05 ***\nResiduals   13  202.0    15.5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nFor the straight line model, the F-test is equivalent to testing the hypothesis that the true slope is zero."
  },
  {
    "objectID": "slides/Chapter06.html#anova-f-test-1",
    "href": "slides/Chapter06.html#anova-f-test-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA \\(F\\)-test",
    "text": "ANOVA \\(F\\)-test\n-   Significant F ratio need not always imply that the\n    straight line is the best fit to the data.\n-   For `Alcohol~deaths` model, the $F$ statistic is \n    significant which means that the fitted model explains significant variation\n\n\nout1 &lt;- anova(mod1) |&gt; tidy() |&gt; mutate_if(is.numeric, round, 2)\n\noptions(knitr.kable.NA = \" \")\n\nkable(out1, caption = \"ANOVA table\") |&gt; \n  kable_classic(full_width = F) \n\n\n\nANOVA table\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nAlcohol\n1\n751.44\n751.44\n48.35\n0\n\n\nResiduals\n13\n202.03\n15.54"
  },
  {
    "objectID": "slides/Chapter06.html#anova-table-construction",
    "href": "slides/Chapter06.html#anova-table-construction",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "ANOVA table construction",
    "text": "ANOVA table construction\n\nEach source has an associated degrees of freedom.\nFor regression, DF = \\(1\\) as there are two parameters \\(a\\) and \\(b\\) fitted in the model\nFor total, DF = \\(n-1\\) since there are n observations\nFor error, DF = by subtraction = \\((n-1)-1 = n-2\\)\nMean Square (MS) values are obtained as MS = SS/DF\nF ratio for regression =MS(Regression)/MS(Error)\nF ratio follows the \\(F\\) distribution with \\((1, n-2)\\) d.f and provides the significance of the model fitted.\n\nthe ratio of two sample variances (or MS) follows the \\(F\\) distribution (normal case)"
  },
  {
    "objectID": "slides/Chapter06.html#prediction",
    "href": "slides/Chapter06.html#prediction",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Prediction",
    "text": "Prediction\n\nThe predicted response at value \\(x=x_0\\) is obtained using the fitted regression equation.\nConfidence & prediction intervals can also be constructed.\nNote that prediction intervals are for individual observations whereas the confidence intervals are for the expected (mean) response for a given \\(x_0\\)\n\n\npredict(mod1, new = data.frame(Alcohol=10), interval=\"confidence\", level =0.95)\n\n       fit      lwr      upr\n1 11.72778 9.476416 13.97915\n\npredict(mod1, new = data.frame(Alcohol=10), interval=\"prediction\", level =0.95)\n\n       fit      lwr      upr\n1 11.72778 2.918701 20.53686"
  },
  {
    "objectID": "slides/Chapter06.html#outlier-effect-on-regression",
    "href": "slides/Chapter06.html#outlier-effect-on-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Outlier effect on regression",
    "text": "Outlier effect on regression\n\nScatter plot of People vs Vehicle"
  },
  {
    "objectID": "slides/Chapter06.html#leverage-and-cooks-distance",
    "href": "slides/Chapter06.html#leverage-and-cooks-distance",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Leverage and Cook’s distance",
    "text": "Leverage and Cook’s distance\n\n\nA distant \\(x\\) value has a higher leverage.\n\nThis leverage is often measured by the \\(h_{ii}\\) or hi value\nCheck \\(h_{ii}\\) &gt; \\(\\frac{3p}{n}\\) or not\n\nInfluence of a point on the regression is measured using the Cook’s distance \\(D_i\\)\n\nrelated to difference between the regression coefficients with and without the \\(i^{th}\\) data point.\n\\(D_{i} &gt;0.7\\) can be deemed as being influential (for \\(n&gt;15\\))"
  },
  {
    "objectID": "slides/Chapter06.html#leverage-and-cooks-distance-1",
    "href": "slides/Chapter06.html#leverage-and-cooks-distance-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Leverage and Cook’s distance",
    "text": "Leverage and Cook’s distance"
  },
  {
    "objectID": "slides/Chapter06.html#tukey-line",
    "href": "slides/Chapter06.html#tukey-line",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Tukey Line",
    "text": "Tukey Line\nAlso called Median-median line, Resistant line or Rline\nAdvantages:\n- Easy to fit\n- Outlier or peculiar values do not affect Rline to the extent they influence a regression line\nRline fitting\n\nSort the values of X first and copy the corresponding Y values\nDivide the X values equally into three groups with corresponding Y values\nCompute the medians of the X values from the lowest and the uppermost subgroups, and the corresponding medians of Y values\nThe resistant line is then fitted with these two median points (x median and y median)"
  },
  {
    "objectID": "slides/Chapter06.html#example-alcohol-consumption-data-1",
    "href": "slides/Chapter06.html#example-alcohol-consumption-data-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Example (Alcohol consumption data)",
    "text": "Example (Alcohol consumption data)\n\n# CV using Root Mean Sq Error as measure of prediction error\nlibrary(caret);  library(MASS,  exclude  =  \"select\")\nset.seed(123)\n\nfitControl &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 100)\nlmfit &lt;- train(Death ~Alcohol,  data= cirrhosis, \n                 trControl = fitControl, method=\"lm\")\nlm.rmses &lt;- lmfit$resample[,1]\nrlmfit &lt;- train(Death ~Alcohol,  data = cirrhosis, \n                  trControl=fitControl, method = \"rlm\")\nrlm.rmses &lt;- rlmfit$resample[,1]\ndfm  &lt;-  cbind.data.frame(lm.rmses,rlm.rmses)\nlibrary(patchwork)\nqplot(data=dfm,  lm.rmses,  geom=\"boxplot\")  /\nqplot(data=dfm,  rlm.rmses,  geom=\"boxplot\")"
  },
  {
    "objectID": "slides/Chapter06.html#r-function",
    "href": "slides/Chapter06.html#r-function",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "R function",
    "text": "R function\n\nline(cirrhosis$Death,cirrhosis$Alcohol) \n\n\nCall:\nline(cirrhosis$Death, cirrhosis$Alcohol)\n\nCoefficients:\n[1]  4.0797  0.4379\n\n\n\nThe line() function gives a slightly different slope & intercept."
  },
  {
    "objectID": "slides/Chapter06.html#two-more-robust-models",
    "href": "slides/Chapter06.html#two-more-robust-models",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Two more Robust models",
    "text": "Two more Robust models\nWe can also fit a robust linear model using the functions MASS::rlm() robustbase::lmrob()."
  },
  {
    "objectID": "slides/Chapter06.html#cross-validation-cv",
    "href": "slides/Chapter06.html#cross-validation-cv",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Cross Validation (CV)",
    "text": "Cross Validation (CV)\n\nA tool to avoid overfitting (more important with multiple predictors)\nEvaluate performance of model"
  },
  {
    "objectID": "slides/Chapter06.html#example-alcohol-consumption-data-2",
    "href": "slides/Chapter06.html#example-alcohol-consumption-data-2",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Example (Alcohol consumption data)",
    "text": "Example (Alcohol consumption data)\n\nlibrary(caret);  library(MASS,  exclude  =  \"select\")\nset.seed(123)\n\nfitControl &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 100)\nlmfit &lt;- train(Death ~Alcohol,  data= cirrhosis, \n                 trControl = fitControl, method=\"lm\")\nlm.rmses &lt;- lmfit$resample[,1]\nrlmfit &lt;- train(Death ~Alcohol,  data = cirrhosis, \n                  trControl=fitControl, method = \"rlm\")\nrlm.rmses &lt;- rlmfit$resample[,1]\ndfm  &lt;-  cbind.data.frame(lm.rmses,rlm.rmses)\nlibrary(patchwork)\nqplot(data=dfm,  lm.rmses,  geom=\"boxplot\")  /\nqplot(data=dfm,  rlm.rmses,  geom=\"boxplot\")"
  },
  {
    "objectID": "slides/Chapter06.html#choosing-the-best-model",
    "href": "slides/Chapter06.html#choosing-the-best-model",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Choosing the best model",
    "text": "Choosing the best model\n\nThe best model is not decided purely on statistical grounds.\nIf the main aim is to describe relationships, include all the relevant variables.\nIf the main aim is to predict, prefer the simplest feasible (parsimonious) model with smaller number of predictors.\n\nExamine the literature to discover similar examples, see how they are tackled, discuss the matter with the researcher etc."
  },
  {
    "objectID": "slides/Chapter06.html#main-points",
    "href": "slides/Chapter06.html#main-points",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Main points",
    "text": "Main points\nConcepts and practical skills you should have at the end of this chapter:\n\nUnderstand and be able to perform a simple linear regression on bivariate related data sets\nUse scatter plots or other appropriate plots to visualize the data and regression line\nExamine residual diagnostic plots and test assumptions, then perform appropriate transformations as necessary\nSummarize regression results and appropriate tests of significance. Interpret these results in context of your data\nUse a regression line to predict new data and explain confidence and prediction intervals\nUnderstand and explain the concepts of robust regression modeling, and cross-validation.\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-1",
    "href": "slides/Chapter04.html#statistical-inference-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\nThe term statistical inference means that we are using properties of a sample to make statements about the population from which the sample was drawn.\nFor example, say you wanted to know the mean length of the leaves on a tree (\\(\\mu\\)). You wouldn’t want to (nor need to) measure every single leaf! You would take a random sample of leaves and measure their lengths (\\(x_i\\)), calculate the sample mean (\\(\\bar x\\)), and use \\(\\bar x\\) as an estimate of \\(\\mu\\)."
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-for-a-population-mean",
    "href": "slides/Chapter04.html#statistical-inference-for-a-population-mean",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical Inference for a Population Mean",
    "text": "Statistical Inference for a Population Mean\n\n\nConsider a population \\(X\\) with mean \\(\\mu\\),\nvariance \\(\\text{Var}(X)=\\sigma^2\\), and\nstandard deviation \\(\\sigma\\).\nWe can estimate \\(\\mu\\) by:\n\ntaking a random sample of values \\(\\{x_1, x_2, ..., x_n\\}\\) from the population, \\(X\\), and\ncalculating the sample mean \\(\\bar x = \\frac 1 n \\sum_{i=1}^{n}x_i\\).\n\n\n\n\n\nImportantly:\n\nThe sample mean \\(\\bar x\\) is a single draw from a random variable \\(\\bar X\\). It will never be exactly equal to the population mean, \\(\\mu\\), unless you measure every single leaf.\nThere is uncertainty in our estimate of \\(\\mu\\). Each sample yields a different \\(\\bar x\\). This is called sampling error."
  },
  {
    "objectID": "slides/Chapter04.html#sampling-variation",
    "href": "slides/Chapter04.html#sampling-variation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling Variation",
    "text": "Sampling Variation"
  },
  {
    "objectID": "slides/Chapter04.html#sampling-error",
    "href": "slides/Chapter04.html#sampling-error",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling Error",
    "text": "Sampling Error\nThe variability of the sample means from sample to sample, \\(\\text{Var}(\\bar X)\\), depends on two things:\n\nthe variability of the population values, \\(\\text{Var}(x)\\), and\nthe size of the sample, \\(n\\).\n\nThe variability of the sample estimate of a parameter is usually expressed as a standard error (SE), which is simply the theoretical standard deviation of \\(\\bar X\\) from sample to sample.\nThe equation is surprisingly simple!\n\\(\\text{Var}(\\bar X) = \\text{Var}(X)/n\\)\n\\(\\text{SE}(\\bar X) = \\text{SD}(\\bar X) = \\sigma/\\sqrt n\\)"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset",
    "href": "slides/Chapter04.html#quetelets-dataset",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\n\n\nIn 1846, a Belgian scholar Adolphe Quetelet published an analysis of the chest sizes of a full population of 5,738 Scottish soldiers.\nThe distribution of the measurements (in inches) in his database has a mean of \\(\\mu\\) = 39.83 inches, a standard deviation of \\(\\sigma\\) = 2.05 inches, and is well approximated by a normal distribution.\n\n\n\n\nAdolphe Quetelet1796-1874\n\n\n\n\n\n\nqd &lt;- tibble(\n  Chest = 33:48, \n  Count = c(3, 18, 81, 185, 420, 749, 1073, 1079, \n            934, 658, 370, 92, 50, 21, 4, 1),\n  Prob = Count / sum(Count)\n  )\n\nqd |&gt; \n  ggplot() +\n  aes(x = Chest, y = Count) +\n  geom_col() +\n  xlab(\"\") + ylab(\"\") +\n  ggtitle(\"Chest circumferences of Scottish soldiers\")"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-1",
    "href": "slides/Chapter04.html#quetelets-dataset-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n\n# A single sample\nsample(qd_long, size = 6)\n\n[1] 38 41 42 35 38 37\n\n\n\n\n\n# Ten samples\nmap(1:7, ~ sample(qd_long, size = 6))\n\n[[1]]\n[1] 38 37 44 35 39 36\n\n[[2]]\n[1] 38 40 41 40 40 40\n\n[[3]]\n[1] 37 37 39 41 38 40\n\n[[4]]\n[1] 40 39 36 42 36 40\n\n[[5]]\n[1] 36 40 40 41 39 37\n\n[[6]]\n[1] 43 40 41 39 39 39\n\n[[7]]\n[1] 36 38 39 39 40 41"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-2",
    "href": "slides/Chapter04.html#quetelets-dataset-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n\n# Put ten samples in a tibble\nq_samples &lt;- tibble( sample = as_factor(1:10) ) |&gt; \n  mutate( \n    values = map(sample, ~ sample(qd_long, size = 6)) \n    ) |&gt; \n  unnest()\n\nhead(q_samples, 15)\n\n# A tibble: 15 × 2\n   sample values\n   &lt;fct&gt;   &lt;int&gt;\n 1 1          40\n 2 1          40\n 3 1          38\n 4 1          38\n 5 1          39\n 6 1          42\n 7 2          41\n 8 2          40\n 9 2          38\n10 2          40\n11 2          40\n12 2          38\n13 3          43\n14 3          38\n15 3          42"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-3",
    "href": "slides/Chapter04.html#quetelets-dataset-3",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\nEvery soldier was measured so we can treat this as the population, and \\(\\mu\\) = 39.83 and \\(\\sigma\\) = 2.05 as population parameters. Let’s take some samples of size \\(n\\) = 6 from this population.\n\n# Convert to a long vector of values\nqd_long &lt;- rep(qd$Chest, qd$Count)\n\n\n# Put ten samples in a tibble\nq_samples &lt;- tibble( sample = as_factor(1:10) ) |&gt; \n  mutate( \n    values = map(sample, ~ sample(qd_long, size = 6)) \n    ) |&gt; \n  unnest()\n\n\n\n# Calculate means of each sample\nsample_means &lt;- q_samples |&gt; \n  group_by(sample) |&gt; \n  summarise(mean = mean(values))\n\nsample_means\n\n# A tibble: 10 × 2\n   sample  mean\n   &lt;fct&gt;  &lt;dbl&gt;\n 1 1       40.2\n 2 2       39.2\n 3 3       39.7\n 4 4       39.5\n 5 5       39.7\n 6 6       39.5\n 7 7       41.8\n 8 8       39.5\n 9 9       40.5\n10 10      38.5"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-dataset-4",
    "href": "slides/Chapter04.html#quetelets-dataset-4",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s dataset",
    "text": "Quetelet’s dataset\n\n\n\n# Make a function to do it all and plot\nplot_samples &lt;- function(dat = qd_long,\n                         no_samples = 12,\n                         sample_size = 6) {\n\n  # Put samples in a tibble\n  q_samples &lt;- tibble( \n    sample = as_factor(1:no_samples),\n    values = map(sample, ~ sample(dat, size = sample_size))\n    ) |&gt; unnest()\n  \n  # Calculate means of each sample\n  sample_means &lt;- q_samples |&gt; group_by(sample) |&gt; \n    summarise(mean = mean(values))\n  \n  ggplot() + \n    xlim(min(dat), max(dat)) +\n    geom_vline(xintercept = mean(dat), alpha = .4) +\n    geom_jitter(\n      data = q_samples,\n      mapping = aes(y = sample, x = values),\n      width = 0, height = 0.1, alpha = .8\n      ) + \n    geom_point(\n      data = sample_means,\n      mapping = aes(y = sample, x = mean),\n      shape = 15, size = 3, \n      colour = \"dark orange\"\n      ) +\n    ggtitle(paste(\"Sample size =\", sample_size))\n}\n\n\n\nplot_samples(sample_size = 6)"
  },
  {
    "objectID": "slides/Chapter04.html#sample-means-and-sample-sizes",
    "href": "slides/Chapter04.html#sample-means-and-sample-sizes",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sample means and sample sizes",
    "text": "Sample means and sample sizes\n\n\nLet’s compare the distribution of means for different sample sizes across the samples.\n\nplot_samples(sample_size = 3)\nplot_samples(sample_size = 5)\nplot_samples(sample_size = 10)\nplot_samples(sample_size = 20)\nplot_samples(sample_size = 50)\nplot_samples(sample_size = 100)\nplot_samples(sample_size = 200)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe larger the \\(n\\), the less the sample means vary."
  },
  {
    "objectID": "slides/Chapter04.html#standard-error",
    "href": "slides/Chapter04.html#standard-error",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Standard error",
    "text": "Standard error\nIf is distributed as a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\),\n\\[X \\sim \\text{Normal}(\\mu, \\sigma)\\]\nthen sample means of size \\(n\\) are distributed as\n\\[\\bar X \\sim \\text{Normal}(\\mu_\\bar{X} = \\mu, \\sigma_\\bar{X} = \\sigma/\\sqrt{n})\\] \\(\\text{SE}(\\bar{X}) = \\sigma_\\bar{X}= \\sigma/\\sqrt{n})\\) is known as the standard error of the sample mean.\nMore generally, a standard error is the standard deviation of an estimated parameter over \\(\\infty\\) theoretical samples.\nAccording to the Central Limit Theorem (CLT), raw \\(X\\) values don’t have to be normally distributed for the sample means to be normally distributed (for any decent sample size)!"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-chests-standard-error-of-means-of-samples",
    "href": "slides/Chapter04.html#quetelets-chests-standard-error-of-means-of-samples",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s chests: Standard error of means of samples",
    "text": "Quetelet’s chests: Standard error of means of samples\nQuestion:\nIf Quetelet’s chest circumferences are distributed as \\(X \\sim \\text{N}(\\mu=39.83, \\sigma=2.05)\\), then how variable are the means of samples of size \\(n\\) = 6?\nAnswer:\n\\[\n\\begin{aligned}\nSE(\\bar{X}_{n=6})&=\\sigma/\\sqrt{n} \\\\\n&=2.05/\\sqrt{6} \\\\\n&=0.8369\n\\end{aligned}\n\\] Sample means of size \\(n\\) = 6 would be distributed as\n\\(\\bar{X}_{n=6} \\sim \\text{N}(\\mu=39.83, \\sigma=0.8369)\\)"
  },
  {
    "objectID": "slides/Chapter04.html#quetelets-chests-distribution-of-means-of-samples",
    "href": "slides/Chapter04.html#quetelets-chests-distribution-of-means-of-samples",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Quetelet’s chests: Distribution of means of samples",
    "text": "Quetelet’s chests: Distribution of means of samples"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals",
    "href": "slides/Chapter04.html#confidence-intervals",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\nAnother common way of expressing uncertainty when making statistical inferences is to present:\n\na point estimate (e.g., the sample mean) and\na confidence interval around that estimate.\n\nA confidence interval gives an indication of the sampling error.\nA 95% confidence interval is constructed so that intervals from 95% of samples will contain the true population parameter.\nIn reality the interval either contains the true value or not, so you must not interpret a confidence interval as “there’s a 95% probability that the interval contains the true parameter”.\nWe can say:\n\n“95% of so-constructed intervals will contain the true value of the parameter” or\n“with 95% confidence, the interval contains the true value of the parameter”."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-1",
    "href": "slides/Chapter04.html#confidence-intervals-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\nBecause we know the population parameters for the Quetelet dataset, we can calculate where 95% of sample means will lie for any particular sample size.\nRecall that, for a normal distribution, 95% of values lie between \\(\\mu \\pm 1.96\\times\\sigma\\)\n(i.e., \\(\\mu - 1.96\\times\\sigma\\) and \\(\\mu + 1.96\\times\\sigma\\)).\nIt follows that 95% of means of samples of size \\(n\\) will lie within \\(\\mu \\pm 1.96 \\times \\sigma/\\sqrt{n}\\).\nFor example, for samples of size 6 from the Quetelet dataset, 95% of means will lie within \\(39.83 \\pm 1.96\\times 2.05 / \\sqrt 6\\),\nso \\(\\{ 37.02 , 42.64\\}\\)."
  },
  {
    "objectID": "slides/Chapter04.html#estimating-the-distribution-of-sample-means-from-data",
    "href": "slides/Chapter04.html#estimating-the-distribution-of-sample-means-from-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Estimating the distribution of sample means from data",
    "text": "Estimating the distribution of sample means from data\n\n\nIt’s all very well deriving the distribution of sample means when we know the population parameters (\\(\\mu\\) and \\(\\sigma\\)) but in most cases we only have our one sample.\nWe don’t know any of the population parameters. We have to estimate the population mean (usually denoted \\(\\hat\\mu\\) or \\(\\bar x\\)) and estimate of the population standard deviation (usually denoted \\(\\hat\\sigma\\) or \\(s\\)) from the sample data.\nThis additional uncertainty complicates things a bit. In fact, we can’t even use the normal distribution any more!"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution",
    "href": "slides/Chapter04.html#the-t-distribution",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\n\n\n\nIntroducing William Sealy Gosset, who described the t distribution after working with random numbers.\nGosset was a chemist and mathematician who worked for the Guinness brewery in Dublin.\nGuinness forbade anyone from publishing under their own names so that competing breweries wouldn’t know what they were up to, so he published his discovery in 1908 under the penname “Student”. Student’s identity was only revealed when he died. It is therefore often called “Student’s t distribution”.\n\n\n\n\n\nWillam Sealy Gosset1876-1937"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution-1",
    "href": "slides/Chapter04.html#the-t-distribution-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\n\nThe t distribution is like the standard normal. It is bell-shaped and symmetric with mean = 0, but it has fatter tails (greater uncertainty) due to the fact that we do not know \\(\\sigma\\); we have to estimate it.\nThe t distribution is not just a single distribution. It is really an entire series of distributions which is indexed by something called the “degrees of freedom” (or \\(df\\)).\n\n\n\n\nAs \\(df \\rightarrow \\infty\\), \\(t \\rightarrow Z\\).\n\n\np &lt;- expand_grid(\n  df = c(2, 5, Inf),\n  x = seq(-4, 4, by = .01)\n  ) |&gt; \n  mutate(\n    Density = dt(x = x, df = df),\n    `degrees of freedom` = as_factor(df)\n  ) |&gt; \n  ggplot() +\n  aes(x = x, y = Density, \n      group = `degrees of freedom`, \n      colour = `degrees of freedom`) +\n  geom_line()"
  },
  {
    "objectID": "slides/Chapter04.html#the-t-distribution-2",
    "href": "slides/Chapter04.html#the-t-distribution-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "The t distribution",
    "text": "The t distribution\nFor a sample of size \\(n\\), if\n\n\\(X\\) is a normal random variable with mean \\(\\mu\\),\n\\(\\bar X\\) is the sample mean, and\n\\(s\\) is the sample standard deviation,\n\nthen the variable:\n\\[\nT = \\frac{\\bar X - \\mu} {s/\\sqrt{n}}\n\\]\nis distributed as a \\(t\\) distribution with \\((n – 1)\\) degrees of freedom."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-and-the-t-distribution",
    "href": "slides/Chapter04.html#confidence-intervals-and-the-t-distribution",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals and the t distribution",
    "text": "Confidence intervals and the t distribution\nThe process of using sample data to try and make useful statements about an unknown parameter, \\(\\theta\\), is called statistical inference.\nA confidence interval for the true value of a parameter is often obtained by:\n\\[\n\\hat \\theta \\pm t \\times \\text{SE}(\\hat \\theta)\n\\] where \\(\\hat \\theta\\) is the sample estimate of \\(\\theta\\) and \\(t\\) is a quantile from the \\(t\\) distribution with the appropriate degrees of freedom.\nFor example, to get the \\(t\\) score for a 95% confidence interval with 9 degrees of freedom:\n\nqt(p = 0.975, df = 9)\n\n[1] 2.262157\n\n\nThe piece that is being added and subtracted, \\(t \\times \\text{SE}(\\hat \\theta)\\), is often called the margin of error."
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean",
    "href": "slides/Chapter04.html#confidence-intervals-for-a-sample-mean",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals for a sample mean",
    "text": "Confidence intervals for a sample mean\nWe can calculate a 95% confidence interval for a sample mean with the following information:\n\n\nsample mean \\(\\bar x\\),\nsample standard deviation \\(s\\),\nthe sample size \\(n\\), and\nthe 0.025th quantile of the \\(t\\) distribution with degrees of freedom \\(df=n-1\\).\n\n\n\n\nA sample of size \\(n\\) = 6 from Quetelet data\n\nn1 &lt;- 6; df1 &lt;- n1-1\n( dq1 &lt;- sample(qd_long, size = n1) )\n\n[1] 39 42 38 36 40 38\n\n( mean1 &lt;- mean(dq1) )\n\n[1] 38.83333\n\n( sd1 &lt;- sd(dq1) )\n\n[1] 2.041241\n\n( t1 &lt;- qt(c(0.025, 0.975), df = df1) )\n\n[1] -2.570582  2.570582\n\nmean1 + t1 * sd1 / sqrt(n1)\n\n[1] 36.69118 40.97548\n\n\n\n\n\n\nOr, more simply…\n\nt.test(dq1)\n\n\n    One Sample t-test\n\ndata:  dq1\nt = 46.6, df = 5, p-value = 8.595e-08\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 36.69118 40.97548\nsample estimates:\nmean of x \n 38.83333"
  },
  {
    "objectID": "slides/Chapter04.html#confidence-intervals-for-sample-means",
    "href": "slides/Chapter04.html#confidence-intervals-for-sample-means",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Confidence intervals for sample means",
    "text": "Confidence intervals for sample means"
  },
  {
    "objectID": "slides/Chapter04.html#t-as-a-test-statistic",
    "href": "slides/Chapter04.html#t-as-a-test-statistic",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "t as a test statistic",
    "text": "t as a test statistic\nThis method can be used when the estimator \\(\\hat\\theta\\) is approximately normally distributed and\n\\[\n\\frac{\\hat\\theta - \\theta} {\\text{SE}(\\hat \\theta)}\n\\]\nhas approximately a Student’s t distribution.\nThis paves the way for hypothesis testing for specific values of \\(\\theta\\). Many of the methods you will learn in this course are based on this general rule."
  },
  {
    "objectID": "slides/Chapter04.html#testing-hypotheses",
    "href": "slides/Chapter04.html#testing-hypotheses",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Testing hypotheses",
    "text": "Testing hypotheses\n\nTesting hypotheses underpins a lot of scientific work.\nA hypothesis is a proposition, a specific idea about the state of the world that can be tested with data.\nFor logical reasons, instead of measuring evidence for a hypothesis of interest, scientists will often:\n\nspecify a null hypotheses, which must be true if our hypothesis of interest is false, and\nmeasure the evidence against the null hypothesis, usually in the form of a p-value."
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nSay a farmer has 9 paddocks of alfalfa. He’s hired a new manager, and wants to know if, on average, this year’s crop is different to last year’s. He measures the yield for each paddock in year 1 and year 2, and calculates the difference.\n\nyear1 &lt;- c(0.8, 1.3, 1.7, 1.7, 1.8, 2.0, 2.0, 2.0, 2.2)\nyear2 &lt;- c(0.7, 1.4, 1.8, 1.8, 2.0, 2.0, 2.1, 2.1, 2.2)\ndiff &lt;- year2 - year1\n( xbar &lt;- mean(diff) ) ; ( s &lt;- sd(diff) )\n\n[1] 0.06666667\n\n\n[1] 0.08660254\n\n\nThe mean difference is 0.067. On average, yields were 0.067 greater than last year.\nIs that convincing different from zero?\nHow likely is such a difference to have arisen just by chance, and really, if we had a million paddocks, there would be no difference?\nWhat is the probability of seeing a difference of 0.067 or more in our dataset if the true mean were zero?\nThese questions can be addressed with a \\(p\\)-value from a hypothesis test."
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-1",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe hypothesis of interest (called the “alternative” hypothesis) is:\n\\(\\ \\ \\ \\ \\ H_A: \\mu \\ne 0\\), that is, the mean difference in yield \\(\\mu\\) between the two years is not zero.\nThe null hypothesis (the case if the alternative hypothesis is wrong) is:\n\\(\\ \\ \\ \\ \\ H_0: \\mu = 0\\), that is, the mean difference is zero.\nWe can test the null hypothesis using a t statistic \\(t_0 = \\frac{\\bar x - \\mu_0} {\\text{SE}(\\bar x)}\\), where\n\\(\\ \\ \\ \\ \\ \\mu_0 = 0\\) is the hypothesised value of the mean and\n\\(\\ \\ \\ \\ \\ \\text{SE}(\\bar x) = s/\\sqrt{n}\\) is the (estimated) standard error of the sample mean.\n\n( t0 &lt;- (xbar - 0) / ( s / sqrt(9) ) )\n\n[1] 2.309401\n\n\nThe p-value is \\(\\text{Pr}(|t_{df=8}| &gt; t_0)\\)\n\n2 * pt(t0, df = 8, lower = F)\n\n[1] 0.04973556"
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-2",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-2",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe hypothesis of interest (called the “alternative” hypothesis) is:\n\\(\\ \\ \\ \\ \\ H_A: \\mu \\ne 0\\), that is, the mean difference in yield \\(\\mu\\) between the two years is not zero.\nThe null hypothesis (the case if the alternative hypothesis is wrong) is:\n\\(\\ \\ \\ \\ \\ H_0: \\mu = 0\\), that is, the mean difference is zero.\nThis can be done more easily:\n\nt.test(diff)\n\n\n    One Sample t-test\n\ndata:  diff\nt = 2.3094, df = 8, p-value = 0.04974\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 9.806126e-05 1.332353e-01\nsample estimates:\n mean of x \n0.06666667"
  },
  {
    "objectID": "slides/Chapter04.html#example-growth-of-alfalfa-3",
    "href": "slides/Chapter04.html#example-growth-of-alfalfa-3",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example: growth of alfalfa",
    "text": "Example: growth of alfalfa\nThe \\(p\\)-value of 0.05 means that, if the null hypothesis were true, only 5% of sample means would be as or more extreme than the observed value of 0.067. It is the area in orange in the graph below.\nWe can therefore reject the null hypothesis at the conventional 5% level, and conclude that, on average, yields were indeed higher this year.\nThis is an example of a paired t test. They two samples (year 1 and year 2) are not independent of one another because we have the same paddocks in both years. So, we take the differences, treat them like a single sample, and do a one-sample t test for the mean difference being zero.\n\\(p\\)-values are random variables too.\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.p.is.rv/"
  },
  {
    "objectID": "slides/Chapter04.html#truth-and-outcomes",
    "href": "slides/Chapter04.html#truth-and-outcomes",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Truth and outcomes",
    "text": "Truth and outcomes\nIn reality, the null hypothesis is either true or false.\nThe outcome of a test is either we reject the null hypothesis, or we fail to reject the null hypothesis (note, we never “confirm” or “accept” the null hypothesis – absence of evidence is not evidence of absence!).\nThis gives us four possibilities:\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nReject \\(H_0\\)\nType I error\nCorrect result\n\n\nDo not reject \\(H_0\\)\nCorrect result\nType II error\n\n\n\nWith probabilities usually represented by:\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nReject \\(H_0\\)\n\\(\\alpha\\)\n\\(1-\\beta\\) = “power”\n\n\nDo not reject \\(H_0\\)\n\\(1 - \\alpha\\)\n\\(\\beta\\)"
  },
  {
    "objectID": "slides/Chapter04.html#truth-and-outcomes-1",
    "href": "slides/Chapter04.html#truth-and-outcomes-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Truth and outcomes",
    "text": "Truth and outcomes\nThe probability of making a Type I error, rejecting \\(H_0\\) when \\(H_0\\) is true, is set by the experimenter a priori (beforehand) as the significance level, \\(\\alpha\\).\n\nSay we set \\(\\alpha\\) at the conventional 0.05. We know that, if \\(H_0\\) is true, we have a 5% chance of rejecting \\(H_0\\) (the Type I error rate is 0.05) and a 95% chance of not rejecting \\(H_0\\).\n\nThe probability of retaining (not rejecting) \\(H_0\\) when \\(H_0\\) is false is usually represented by \\(\\beta\\) (“beta”).\n\nWe usually do not know \\(\\beta\\) because it depends on \\(\\sigma\\), \\(n\\) and also the effect size under the alternative hypothesis. These must be asserted to calculate \\(\\beta\\) and power, \\(1-\\beta\\)."
  },
  {
    "objectID": "slides/Chapter04.html#calculating-power",
    "href": "slides/Chapter04.html#calculating-power",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Calculating power",
    "text": "Calculating power\nThe power of a hypothesis test is the probability of rejecting the null hypothesis if it is indeed false. Basically, “if we’re right, what is the probability that we’ll be able to show it?”\nThe power of the \\(t\\) test can be evaluated using R if you assert the effect size \\(\\delta\\):\n\n\n\npower.t.test(n = 10, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 10\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.5619846\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\npower.t.test(n = 50, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 50\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.9986074\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nThe bigger the sample size, the bigger more power we have to detect an effect.\nOften, experimenters might aim to have a power of 80% or more, so they will most likely reject the null if it is false."
  },
  {
    "objectID": "slides/Chapter04.html#sampling-distributions",
    "href": "slides/Chapter04.html#sampling-distributions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nThe term sampling distribution means the distribution of the computed statistic such as the sample mean when sampling is repeated many times.\nFor a normal population,\n\nStudent’s \\(t\\) distribution is the sampling distribution of the mean (after rescaling).\n\\(\\chi^2\\) distribution is the sampling distribution of the sample variance \\(S^2\\).\n\\((n-1)S^2/\\sigma^2\\) follows \\(\\chi^2\\) distribution.\n\n\\(F\\) distribution is ratio of two \\(\\chi^2\\) distributions.\n\nIt becomes the sampling distribution of the ratio of two sample variances \\(S_1^2/S_2^2\\) from two normal populations (after scaling).\n\n\\(t\\) distribution is symmetric but \\(\\chi^2\\) and \\(F\\) distributions are right skewed. - For large samples, they become normal - For \\(n&gt;30\\), the skew will diminish\nFor the three sampling distributions, the sample size \\(n\\) becomes the proxy parameter, called the degrees of freedom (df).\n\n\\(t_{n-1}\\), \\(\\chi_{n-1}^2\\) & \\(F_{(n_1-1),(n_2-1) }\\)"
  },
  {
    "objectID": "slides/Chapter04.html#two-sample-t-test",
    "href": "slides/Chapter04.html#two-sample-t-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two sample t-test",
    "text": "Two sample t-test\n\nThe two-sample t-test is a common statistical test. You have two populations, \\(X_1\\) and \\(X_2\\), with means \\(\\mu_1\\) and \\(\\mu_2\\) and variances \\(\\sigma^2_1\\) and \\(\\sigma^2_2\\), respectively.\nWe wish to test whether the two population means are equal.\nNull hypothesis: \\(H_0:\\mu=\\mu_1=\\mu_2\\)\nTwo-sided alternative hypothesis: \\(H_1:\\mu_1 \\neq \\mu_2\\)\nThe basic t-test assumes equal variances; that is, \\(\\sigma^2_1 = \\sigma^2_2\\).\nIf we make this assumption and it is false, the test may give the wrong conclusion!"
  },
  {
    "objectID": "slides/Chapter04.html#two-sample-t-test-with-equal-variances",
    "href": "slides/Chapter04.html#two-sample-t-test-with-equal-variances",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two sample t-test with equal variances",
    "text": "Two sample t-test with equal variances\nUnder the assumption of equal variances \\(\\sigma^2_1 = \\sigma^2_2\\), we can perform a pooled-sample t-test.\nWe calculate the estimated pooled variance as:\n\\[s_{p}^{2} = w_{1} s_{1}^{2} +w_{2} s_{2}^{2}\\]\nwhere the weights are \\(w_{1} =\\frac{n_{1}-1}{n_{1} +n_{2}-2}\\) and \\(w_{2} =\\frac{n_{2}-1}{n_{1} +n_{2}-2}\\),\nand \\(n_1\\) and \\(n_2\\) are the sample sizes for samples 1 and 2, respectively.\nFor the pooled case, the \\(df\\) for the \\(t\\)-test is simply\n\\[df = n_{1}+n_{2}-2\\]"
  },
  {
    "objectID": "slides/Chapter04.html#two-sample-t-test-with-unequal-variances",
    "href": "slides/Chapter04.html#two-sample-t-test-with-unequal-variances",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two sample t-test with unequal variances",
    "text": "Two sample t-test with unequal variances\nThis is often called the “Welch test”.\nWe do not need to calculated the pooled variance \\(s^2_p\\);\nwe simply use \\(s^2_1\\) and \\(s^2_2\\) as they are.\nFor the unpooled case, the \\(df\\) for the test is smaller: \\[df=\\frac{\\left(\\frac{s_{1}^{2}}{n_{1}} +\\frac{s_{2}^{2} }{n_{2}} \\right)^{2} }{\\frac{1}{n_{1} -1} \\left(\\frac{s_{1}^{2}}{n_{1}}\\right)^{2} +\\frac{1}{n_{2} -1} \\left(\\frac{s_{2}^{2}}{n_{2} } \\right)^{2}}\\] The \\(df\\) doesn’t have to be an integer in this case."
  },
  {
    "objectID": "slides/Chapter04.html#validity-of-equal-variance-assumption",
    "href": "slides/Chapter04.html#validity-of-equal-variance-assumption",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Validity of equal variance assumption",
    "text": "Validity of equal variance assumption\nWe can test the null hypothesis that the variances are equal using either a Bartlett’s test or Levene’s test. Levene’s is generally favourable over Bartlett’s.\n\ntv = read_csv(\"https://www.massey.ac.nz/~anhsmith/data/tv.csv\")\ncar::leveneTest(TELETIME~factor(SEX), data=tv)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  3.1789 0.08149 .\n      44                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHighish p-value means there’s no strong evidence against the null hypothesis that the variances are equal. Therefore, we might feel happy to assume equal variances and use a pooled variance estimate."
  },
  {
    "objectID": "slides/Chapter04.html#validity-of-equal-variance-assumption-1",
    "href": "slides/Chapter04.html#validity-of-equal-variance-assumption-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Validity of equal variance assumption",
    "text": "Validity of equal variance assumption\nt-test assuming equal variances:\n\nt.test(TELETIME ~ factor(SEX), var.equal = TRUE, data=tv)\n\n\n    Two Sample t-test\n\ndata:  TELETIME by factor(SEX)\nt = -0.7249, df = 44, p-value = 0.4723\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -461.3476  217.2606\nsample estimates:\nmean in group 1 mean in group 2 \n       1668.261        1790.304 \n\n\nt-test not assuming equal variances (Welch test):\n\nt.test(TELETIME ~ factor(SEX), data=tv)\n\n\n    Welch Two Sample t-test\n\ndata:  TELETIME by factor(SEX)\nt = -0.7249, df = 40.653, p-value = 0.4727\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -462.1384  218.0514\nsample estimates:\nmean in group 1 mean in group 2 \n       1668.261        1790.304"
  },
  {
    "objectID": "slides/Chapter04.html#shiny-apps-some-not-currently-working",
    "href": "slides/Chapter04.html#shiny-apps-some-not-currently-working",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Shiny apps (some not currently working)",
    "text": "Shiny apps (some not currently working)\nhttps://shiny.massey.ac.nz/anhsmith/demos/demo.2sample.t.test/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.2sample.t-test/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.paired.t-test/\nFor more on non-parametric tests, see Study Guide."
  },
  {
    "objectID": "slides/Chapter04.html#test-of-proportions",
    "href": "slides/Chapter04.html#test-of-proportions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Test of proportions",
    "text": "Test of proportions\nTesting the null hypothesis that a proportion \\(p=0.5\\)\nAlternative hypothesis: \\(p \\ne 0.5\\)\nSay a survey of 1000 beached whales had 450 females.\n\n\nIs the population 50:50?\n\nprop.test(450, 1000)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  450 out of 1000, null probability 0.5\nX-squared = 9.801, df = 1, p-value = 0.001744\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4189204 0.4814685\nsample estimates:\n   p \n0.45 \n\n\n\nAn exact version of the test\n\nbinom.test(c(450, 550))\n\n\n    Exact binomial test\n\ndata:  c(450, 550)\nnumber of successes = 450, number of trials = 1000, p-value = 0.001731\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4188517 0.4814435\nsample estimates:\nprobability of success \n                  0.45 \n\n\n\n\nTo test for a proportion other than 0.5\n\nbinom.test(c(450, 550), p = 3/4)\n\n\n    Exact binomial test\n\ndata:  c(450, 550)\nnumber of successes = 450, number of trials = 1000, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.75\n95 percent confidence interval:\n 0.4188517 0.4814435\nsample estimates:\nprobability of success \n                  0.45"
  },
  {
    "objectID": "slides/Chapter04.html#comparing-several-proportions",
    "href": "slides/Chapter04.html#comparing-several-proportions",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Comparing several proportions",
    "text": "Comparing several proportions\nData on smokers in four group of patients\n(from Fleiss, 1981, Statistical methods for rates and proportions).\n\nsmokers  &lt;- c( 83, 90, 129, 70 )\npatients &lt;- c( 86, 93, 136, 82 )\nprop.test(smokers, patients)\n\n\n    4-sample test for equality of proportions without continuity correction\n\ndata:  smokers out of patients\nX-squared = 12.6, df = 3, p-value = 0.005585\nalternative hypothesis: two.sided\nsample estimates:\n   prop 1    prop 2    prop 3    prop 4 \n0.9651163 0.9677419 0.9485294 0.8536585 \n\n\nMore on chi-squared tests next week."
  },
  {
    "objectID": "slides/Chapter04.html#tests-for-normality",
    "href": "slides/Chapter04.html#tests-for-normality",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Tests for normality",
    "text": "Tests for normality\nTesting the hypothesis that sample data came from a normally distributed population. \\(H_0: X \\sim N(\\mu, \\sigma)\\).\n\nKolmogorov-Smirnov test (based on the biggest difference between the empirical and theoretical cumulative distributions)\nShapiro-Wilk test (based on variance of the difference)\n\nExample: a sample of \\(n\\) = 50 from N(100,1).\n\n\n\nset.seed(123)\nshapiro.test(rnorm(50, mean=100))\n\n\n    Shapiro-Wilk normality test\n\ndata:  rnorm(50, mean = 100)\nW = 0.98928, p-value = 0.9279\n\n\n\n\nset.seed(123)\nks.test(rnorm(50), \"pnorm\")\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  rnorm(50)\nD = 0.073034, p-value = 0.9347\nalternative hypothesis: two-sided\n\n\n\n\nThe large p-values mean that there’s no evidence of non-normality.\nLike all tests, they can give the wrong answer. Try with set.seed(1234)."
  },
  {
    "objectID": "slides/Chapter04.html#can-also-use-plots-to-assess-normality",
    "href": "slides/Chapter04.html#can-also-use-plots-to-assess-normality",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Can also use plots to assess normality",
    "text": "Can also use plots to assess normality\n\ntv = read_csv(\"https://www.massey.ac.nz/~anhsmith/data/tv.csv\")\n\nggplot(tv, aes(sample = TELETIME)) + \n  stat_qq() + stat_qq_line() +\n  labs(title = \"Normal quantile plot for TV viewing times\")"
  },
  {
    "objectID": "slides/Chapter04.html#transformations",
    "href": "slides/Chapter04.html#transformations",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Transformations",
    "text": "Transformations\n\nIt can be useful to transform skewed data to make the distribution more like a ‘normal’ if that is required for a particular analysis.\nA linear transformation \\(Y^*= a+bY\\) only changes the scale or centre, not the shape of the distribution.\nTransformations can help to improve symmetry, normality, and stabilise the variance."
  },
  {
    "objectID": "slides/Chapter04.html#example",
    "href": "slides/Chapter04.html#example",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example",
    "text": "Example\nRight skewed distribution is made roughly symmetric using a log transformation for no. of vehicles variable (rangitikei.* dataset)"
  },
  {
    "objectID": "slides/Chapter04.html#a-ladder-of-powers-for-transforming-data",
    "href": "slides/Chapter04.html#a-ladder-of-powers-for-transforming-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "A Ladder of Powers for Transforming Data",
    "text": "A Ladder of Powers for Transforming Data\n\nRight skewed data needs a shrinking transformation\nLeft skewed data needs a stretching transformation\nThe strength or power of the transformation depends on the degree of skew.\n\n\n\n\n\n\n\n\n\n\n\nPOWER\nFormula\nName\nResult\n\n\n\n\n3\n\\(x^3\\)\ncube\nstretches large values\n\n\n2\n\\(x^2\\)\nsquare\nstretches large values\n\n\n1\n\\(x\\)\nraw\nNo change\n\n\n1/2\n\\(\\sqrt{x}\\)\nsquare root\nsquashes large values\n\n\n0\n\\(\\log{x}\\)\nlogarithm\nsquashes large values\n\n\n-1/2\n\\(\\frac{-1}{\\sqrt{x}}\\)\nreciprocal root\nsquashes large values\n\n\n-1\n\\(\\frac{-1}{x}\\)\nreciprocal\nsquashes large values"
  },
  {
    "objectID": "slides/Chapter04.html#box-cox-transformation",
    "href": "slides/Chapter04.html#box-cox-transformation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Box-Cox transformation",
    "text": "Box-Cox transformation\n\n\nThis is a normalising transformation based on the Box-Cox power parameter, \\(\\lambda\\).\nR gives a point estimate of the Box-Cox power & a confidence interval.\nUsually, we are concerned about whether the residuals from a model are normally distributed – put the model in the lm statement.\nSometimes, no “good” value of \\(\\lambda\\) can be found.\n\n\nlibrary(lindia)\ngg_boxcox(lm(rangitikei$vehicle ~ 1))"
  },
  {
    "objectID": "slides/Chapter04.html#statistical-inference-based-on-transformed-data",
    "href": "slides/Chapter04.html#statistical-inference-based-on-transformed-data",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Statistical inference based on transformed data",
    "text": "Statistical inference based on transformed data\n\nObtain the confidence interval using transformed data and then back transform the limits (to keep the original scale)\n\nThe confidence limits calculated on log-transformed data can be exponentiated back to the raw scale (i.e., \\(e^{\\text{confidence limit}}\\))\nThe confidence limits of square-root-transformed data can be squared back to the raw scale (ie. \\({\\text{confidence limit}^2}\\))\n\nFor hypothesis test, apply the same transformation on the value hypothesised under the null\nExplore https://shiny.massey.ac.nz/anhsmith/demos/explore.transformations/ app (not currently working)"
  },
  {
    "objectID": "slides/Chapter04.html#example-1",
    "href": "slides/Chapter04.html#example-1",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Example",
    "text": "Example\nBrain weights of Animals data are right-skewed.\n\n\ndata(Animals, package = \"MASS\")\n\nbind_cols( \n  # confidence intervals on raw data\n  `raw x` = Animals |&gt; \n    pull(brain) |&gt; \n    t.test() |&gt; \n    pluck(\"conf.int\"),\n  # confidence intervals on log-transformed data\n  `log x` = Animals |&gt; \n    pull(brain) |&gt; \n    log() |&gt; \n    t.test() |&gt; \n    pluck(\"conf.int\"),\n  # confidence intervals on log-transformed data \n  # and then back-transformed\n  `log x, CIs back-transformed` = Animals |&gt; \n    pull(brain) |&gt; \n    log() |&gt; \n    t.test() |&gt; \n    pluck(\"conf.int\") |&gt;\n    exp()\n  ) |&gt; \n  kable()\n\n\n\nLower and upper confidence limits \n\n\nraw x\nlog x\nlog x, CIs back-transformed\n\n\n\n\n56.88993\n3.495101\n32.95362\n\n\n1092.15293\n5.355790\n211.83131"
  },
  {
    "objectID": "slides/Chapter04.html#non-parametric-tests",
    "href": "slides/Chapter04.html#non-parametric-tests",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Non-parametric tests",
    "text": "Non-parametric tests\nNon-parametric tests are light on assumptions, and can be used for highly asymmetric data (as an alternative to using transformations).\nMany non-parametric methods rely on replacing the observed data by their ranks."
  },
  {
    "objectID": "slides/Chapter04.html#spearmans-rank-correlation",
    "href": "slides/Chapter04.html#spearmans-rank-correlation",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Spearman’s Rank Correlation",
    "text": "Spearman’s Rank Correlation\n\n\nRank the \\(X\\) and \\(Y\\) variables, and then obtain usual Pearson correlation coefficient.\nThe plot shows non-parametric Spearman in the the upper triangle and parametric Pearson in the bottom triangle.\n\nlibrary(GGally)\n\np &lt;- ggpairs(\n  trees, \n  upper = list(continuous = wrap('cor', method = \"spearman\")),\n  lower = list(continuous = 'cor') \n  )\n\n\n\n\n\n\n\nFigure 1: Comparison of Pearsonian and Spearman’s rank correlations"
  },
  {
    "objectID": "slides/Chapter04.html#wilcoxon-signed-rank-test",
    "href": "slides/Chapter04.html#wilcoxon-signed-rank-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Wilcoxon signed rank test",
    "text": "Wilcoxon signed rank test\nA non-parametric alternative to the one-sample t-test\n\\(H_0: \\eta=\\eta_0\\) where \\(\\eta\\) (Greek letter ‘eta’) is the population median\nBased on based on ranking \\((|Y-\\eta_0|)\\), where the ranks for data with \\(Y&lt;\\eta_0\\) are compared to the ranks for data with \\(Y&gt;\\eta_0\\)\n\n\n\nwilcox.test(tv$TELETIME, mu=1680, conf.int=T)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  tv$TELETIME\nV = 588, p-value = 0.6108\nalternative hypothesis: true location is not equal to 1680\n95 percent confidence interval:\n 1557.5 1906.5\nsample estimates:\n(pseudo)median \n          1728 \n\n\n\n\nt.test(tv$TELETIME, mu=1680)\n\n\n    One Sample t-test\n\ndata:  tv$TELETIME\nt = 0.58856, df = 45, p-value = 0.5591\nalternative hypothesis: true mean is not equal to 1680\n95 percent confidence interval:\n 1560.633 1897.932\nsample estimates:\nmean of x \n 1729.283"
  },
  {
    "objectID": "slides/Chapter04.html#mann-whitney-test",
    "href": "slides/Chapter04.html#mann-whitney-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Mann-Whitney test",
    "text": "Mann-Whitney test\nFor two group comparison, pool the two group responses and then rank the pooled data\nRanks for the first group are compared to the ranks for the second group\nThe null hypothesis is that the two group medians are the same: \\(H_0: \\eta_1=\\eta_2\\).\n\n\n\nwilcox.test(rangitikei$people~rangitikei$time, conf.int=T)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -88.99996 -10.00005\nsample estimates:\ndifference in location \n             -36.46835 \n\n\n\n\nt.test(rangitikei$people~rangitikei$time)\n\n\n    Welch Two Sample t-test\n\ndata:  rangitikei$people by rangitikei$time\nt = -3.1677, df = 30.523, p-value = 0.003478\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -102.28710  -22.13049\nsample estimates:\nmean in group 1 mean in group 2 \n       22.71429        84.92308"
  },
  {
    "objectID": "slides/Chapter04.html#another-form-of-test",
    "href": "slides/Chapter04.html#another-form-of-test",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Another form of test",
    "text": "Another form of test\n\n\n\nkruskal.test(rangitikei$people~rangitikei$time)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  rangitikei$people by rangitikei$time\nKruskal-Wallis chi-squared = 7.2171, df = 1, p-value = 0.007221\n\n\n\n\nwilcox.test(rangitikei$people~rangitikei$time)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "slides/Chapter04.html#permutation-tests",
    "href": "slides/Chapter04.html#permutation-tests",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Permutation tests",
    "text": "Permutation tests\nA permutation (or randomisation) test has the following steps:\n\nRandomly permute the observed data many times, thereby destroying any real relationship,\nRecalculate the test statistic \\(T\\) for each random permutation\nCompare the observed value of \\(T\\) (from the actual data) with the values of \\(T\\) under permutation.\n\nOne sample hypothesis test example follows:\n\nlibrary(exactRankTests)\nperm.test(tv$TELETIME, null.value=1500)\n\n\n    1-sample Permutation Test\n\ndata:  tv$TELETIME\nT = 79547, p-value = 2.842e-14\nalternative hypothesis: true mu is not equal to 0\n\n\nFor small samples, this approach is not powerful."
  },
  {
    "objectID": "slides/Chapter04.html#two-group-comparison",
    "href": "slides/Chapter04.html#two-group-comparison",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Two group comparison",
    "text": "Two group comparison\n\nperm.test(TELETIME~SEX, distribution ='exact', data=tv)\n\n\n    2-sample Permutation Test\n\ndata:  TELETIME by SEX\nT = 38370, p-value = 0.471\nalternative hypothesis: true mu is not equal to 0\n\n\nAlso using a linear model fit (cover later)\n\nlibrary(lmPerm)\nsummary(lmp(TELETIME~SEX, data=tv))\n\n[1] \"Settings:  unique SS : numeric variables centered\"\n\n\n\nCall:\nlmp(formula = TELETIME ~ SEX, data = tv)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-1178.261  -510.793    -7.283   402.989  1130.739 \n\nCoefficients:\n    Estimate Iter Pr(Prob)\nSEX      122   51     0.98\n\nResidual standard error: 570.9 on 44 degrees of freedom\nMultiple R-Squared: 0.0118, Adjusted R-squared: -0.01066 \nF-statistic: 0.5255 on 1 and 44 DF,  p-value: 0.4723 \n\n\nRead the study guide example for bootstrap tests (not examined)"
  },
  {
    "objectID": "slides/Chapter04.html#summary",
    "href": "slides/Chapter04.html#summary",
    "title": "Chapter 4:Introduction to Statistical Inference",
    "section": "Summary",
    "text": "Summary\nBasic methods of inference include:\n\nestimating a population parameter (e.g. mean) using a sample of values,\nestimating standard deviation of a sample estimate using the standard error,\nconstructing confidence intervals for an estimate, and\ntesting hypotheses about particular values of the population parameter.\n\nInference is relatively easy for normally distributed populations.\nStudent’s t-tests include:\n\none-sample t-test, including paired-sample t-test for a difference of zero\ntwo-sample t-test assuming equal variances (estimating pooled variance)\ntwo-sample t-test not assuming equal variances (Welch test)\n\nThe t-test is generally robust for non-normal populations (especially for large samples).\nPower transformations, such as square-root, log, or Box-Cox aim to reduce skewness.\nNon-parametric tests can be used for non-normal data, but they are usually less powerful than parametric tests.\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter02.html#two-modes-of-data-analysis",
    "href": "slides/Chapter02.html#two-modes-of-data-analysis",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Two modes of data analysis",
    "text": "Two modes of data analysis\n\n\nHypothesis-generating\n\n“Exploratory analysis”\nAim is to explore data to discover new patterns\nResults must not be presented as formal tests of a priori hypotheses\nTesting a hypothesis using the same data that gave rise to the hypothesis is circular reasoning\n\n\nHypothesis-testing\n\n“Confirmatory analysis”\nAim is to evaluate evidence for specific a priori hypotheses\nThe hypotheses and ideas were conceived of before the data were observed\nCan be used for formal scientific inference\n\n\n\nPresenting hypothesis-generating analyses as hypothesis-testing analyses (i.e., pretending the hypotheses were conceived prior to the analysis) is scientifically dishonest, and a major contributor to the replication crisis in science."
  },
  {
    "objectID": "slides/Chapter02.html#plots-for-categorical-data",
    "href": "slides/Chapter02.html#plots-for-categorical-data",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Plots for categorical data",
    "text": "Plots for categorical data\nBar graphs\n\nShow the frequency of each category (level) in categorical variables\nThe height of each bar is proportional to the frequency\nCan be “stacked” or “clustered”"
  },
  {
    "objectID": "slides/Chapter02.html#tea-data",
    "href": "slides/Chapter02.html#tea-data",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Tea data",
    "text": "Tea data\nData from 300 individuals’ tea-drinking habits (18 questions), perceptions (12 questions), and personal details (4 questions).\n\ndata(tea, package = \"FactoMineR\")\nglimpse(tea)\n\nRows: 300\nColumns: 36\n$ breakfast        &lt;fct&gt; breakfast, breakfast, Not.breakfast, Not.breakfast, b…\n$ tea.time         &lt;fct&gt; Not.tea time, Not.tea time, tea time, Not.tea time, N…\n$ evening          &lt;fct&gt; Not.evening, Not.evening, evening, Not.evening, eveni…\n$ lunch            &lt;fct&gt; Not.lunch, Not.lunch, Not.lunch, Not.lunch, Not.lunch…\n$ dinner           &lt;fct&gt; Not.dinner, Not.dinner, dinner, dinner, Not.dinner, d…\n$ always           &lt;fct&gt; Not.always, Not.always, Not.always, Not.always, alway…\n$ home             &lt;fct&gt; home, home, home, home, home, home, home, home, home,…\n$ work             &lt;fct&gt; Not.work, Not.work, work, Not.work, Not.work, Not.wor…\n$ tearoom          &lt;fct&gt; Not.tearoom, Not.tearoom, Not.tearoom, Not.tearoom, N…\n$ friends          &lt;fct&gt; Not.friends, Not.friends, friends, Not.friends, Not.f…\n$ resto            &lt;fct&gt; Not.resto, Not.resto, resto, Not.resto, Not.resto, No…\n$ pub              &lt;fct&gt; Not.pub, Not.pub, Not.pub, Not.pub, Not.pub, Not.pub,…\n$ Tea              &lt;fct&gt; black, black, Earl Grey, Earl Grey, Earl Grey, Earl G…\n$ How              &lt;fct&gt; alone, milk, alone, alone, alone, alone, alone, milk,…\n$ sugar            &lt;fct&gt; sugar, No.sugar, No.sugar, sugar, No.sugar, No.sugar,…\n$ how              &lt;fct&gt; tea bag, tea bag, tea bag, tea bag, tea bag, tea bag,…\n$ where            &lt;fct&gt; chain store, chain store, chain store, chain store, c…\n$ price            &lt;fct&gt; p_unknown, p_variable, p_variable, p_variable, p_vari…\n$ age              &lt;int&gt; 39, 45, 47, 23, 48, 21, 37, 36, 40, 37, 32, 31, 56, 6…\n$ sex              &lt;fct&gt; M, F, F, M, M, M, M, F, M, M, M, M, M, M, M, M, M, F,…\n$ SPC              &lt;fct&gt; middle, middle, other worker, student, employee, stud…\n$ Sport            &lt;fct&gt; sportsman, sportsman, sportsman, Not.sportsman, sport…\n$ age_Q            &lt;fct&gt; 35-44, 45-59, 45-59, 15-24, 45-59, 15-24, 35-44, 35-4…\n$ frequency        &lt;fct&gt; 1/day, 1/day, +2/day, 1/day, +2/day, 1/day, 3 to 6/we…\n$ escape.exoticism &lt;fct&gt; Not.escape-exoticism, escape-exoticism, Not.escape-ex…\n$ spirituality     &lt;fct&gt; Not.spirituality, Not.spirituality, Not.spirituality,…\n$ healthy          &lt;fct&gt; healthy, healthy, healthy, healthy, Not.healthy, heal…\n$ diuretic         &lt;fct&gt; Not.diuretic, diuretic, diuretic, Not.diuretic, diure…\n$ friendliness     &lt;fct&gt; Not.friendliness, Not.friendliness, friendliness, Not…\n$ iron.absorption  &lt;fct&gt; Not.iron absorption, Not.iron absorption, Not.iron ab…\n$ feminine         &lt;fct&gt; Not.feminine, Not.feminine, Not.feminine, Not.feminin…\n$ sophisticated    &lt;fct&gt; Not.sophisticated, Not.sophisticated, Not.sophisticat…\n$ slimming         &lt;fct&gt; No.slimming, No.slimming, No.slimming, No.slimming, N…\n$ exciting         &lt;fct&gt; No.exciting, exciting, No.exciting, No.exciting, No.e…\n$ relaxing         &lt;fct&gt; No.relaxing, No.relaxing, relaxing, relaxing, relaxin…\n$ effect.on.health &lt;fct&gt; No.effect on health, No.effect on health, No.effect o…"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts-one-variable",
    "href": "slides/Chapter02.html#bar-charts-one-variable",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts — one variable",
    "text": "Bar charts — one variable\n\n\nggplot(tea) +\n  geom_bar(aes(x = price)) + \n  ggtitle(\"Bar chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts-two-variables",
    "href": "slides/Chapter02.html#bar-charts-two-variables",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts — two variables",
    "text": "Bar charts — two variables\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where)\n    ) + \n  ggtitle(\"Stacked bar chart\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where), \n    position = \"dodge\"\n    ) +\n  ggtitle(\"Clustered bar chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#bar-charts---flipped",
    "href": "slides/Chapter02.html#bar-charts---flipped",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bar charts - flipped",
    "text": "Bar charts - flipped\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where)\n    ) + \n  ggtitle(\"Stacked bar chart\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  geom_bar(\n    aes(x = price, fill = where), \n    position = \"dodge\"\n    ) +\n  ggtitle(\"Clustered bar chart\") +\n  coord_flip()"
  },
  {
    "objectID": "slides/Chapter02.html#pie-charts-yeah-nah",
    "href": "slides/Chapter02.html#pie-charts-yeah-nah",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pie charts (yeah nah)",
    "text": "Pie charts (yeah nah)\n\n\nggplot(tea) +\n  aes(x = \"\", fill = price) +\n  geom_bar() +\n  coord_polar(\"y\") + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(tea) +\n  aes(x = price) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "slides/Chapter02.html#pie-charts-yeah-nah-1",
    "href": "slides/Chapter02.html#pie-charts-yeah-nah-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pie charts (yeah nah)",
    "text": "Pie charts (yeah nah)\n\n\nggplot(tea) +\n  aes(x = \"\", fill = price) +\n  geom_bar() +\n  coord_polar(\"y\") + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\nPie charts are popular but not usually the best way to show proportional data\nRequires comparison of angles or areas of different shapes\nBar charts are almost always better\n\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.counts.of.factors/"
  },
  {
    "objectID": "slides/Chapter02.html#one-dimensional-graphs",
    "href": "slides/Chapter02.html#one-dimensional-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "One-dimensional graphs",
    "text": "One-dimensional graphs\nDotplots and strip charts display one-dimensional data (grouped/ungrouped) and are useful to discover gaps and outliers.\nOften used to display experimental design data; not great for very small datasets (&lt;20)\n\n\ndata(Animals, package = \"MASS\")\n\nggplot(Animals) +\n  aes(x = brain) + \n  geom_dotplot() + \n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"Dotplot\")"
  },
  {
    "objectID": "slides/Chapter02.html#one-dimensional-graphs-1",
    "href": "slides/Chapter02.html#one-dimensional-graphs-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "One-dimensional graphs",
    "text": "One-dimensional graphs\nDotplots and strip charts display one-dimensional data (grouped/ungrouped) and are useful to discover gaps and outliers.\nOften used to display experimental design data; not great for very small datasets (&lt;20)\n\n\ndata(Animals, package = \"MASS\")\n\nAnimals |&gt; \n  mutate(\n    Animal = fct_reorder(\n      rownames(Animals), \n      brain )\n    ) |&gt; \n  ggplot() +\n  aes( y = Animal, \n       x = brain\n       ) + \n  geom_point() + \n  ylab(\"Animal\") + \n  ggtitle(\"Strip chart\")"
  },
  {
    "objectID": "slides/Chapter02.html#histograms",
    "href": "slides/Chapter02.html#histograms",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Histograms",
    "text": "Histograms\nDivide the data range into “bins”, count the occurrences in each bin, and make a bar chart.\nY-axis can show raw counts, relative frequencies, or densities\n\nset.seed(1234); dfm &lt;- data.frame(X = rnorm(50, 100))\n\np1 &lt;- ggplot(dfm, aes(X)) + geom_histogram(bins = 20) + ylab(\"count\") + ggtitle(\"Frequency histogram\", \"Heights of the bars sum to n\")\np2 &lt;- ggplot(dfm) + aes(x = X, y = after_stat(count/sum(count))) + geom_histogram(bins = 20) + ylab(\"relative frequency\") +\n  ggtitle(\"Relative frequency histogram\", \"Heights sum to 1\")\np3 &lt;- ggplot(dfm) + aes(x = X, y = after_stat(density)) + geom_histogram(bins = 20) + \n  ggtitle(\"Density histogram\",\"Heights x widths sum to 1\")\n\nlibrary(patchwork); p1+p2+p3"
  },
  {
    "objectID": "slides/Chapter02.html#frequency-polygon-kernel-density-plots",
    "href": "slides/Chapter02.html#frequency-polygon-kernel-density-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Frequency polygon & kernel density plots",
    "text": "Frequency polygon & kernel density plots\n\n\nHistograma coarse approximation of the density\n\nggplot(vital) + aes(Life_female) + \n  geom_histogram(bins = 12) +\n  geom_freqpoly(bins = 12)\n\n\n\n\n\n\n\n\n\nKernel densitya smooth approximation of the density\n\nggplot(vital) + aes(Life_female) +\n  geom_histogram(bins = 12, aes(y = after_stat(density))) + \n  geom_density()"
  },
  {
    "objectID": "slides/Chapter02.html#kernel-density-estimation-kde",
    "href": "slides/Chapter02.html#kernel-density-estimation-kde",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Kernel density estimation (KDE)",
    "text": "Kernel density estimation (KDE)"
  },
  {
    "objectID": "slides/Chapter02.html#summary-statistics-for-eda",
    "href": "slides/Chapter02.html#summary-statistics-for-eda",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Summary statistics for EDA",
    "text": "Summary statistics for EDA"
  },
  {
    "objectID": "slides/Chapter02.html#five-number-summary",
    "href": "slides/Chapter02.html#five-number-summary",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Five-number summary",
    "text": "Five-number summary\nMinimum, lower hinge, median, upper hinge and maximum\n\nset.seed(1234)\nmy.data &lt;- rnorm(50, 100)\nfivenum(my.data)\n\n[1]  97.65430  99.00566  99.46477  99.98486 102.41584\n\nsummary(my.data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  97.65   99.01   99.46   99.55   99.96  102.42"
  },
  {
    "objectID": "slides/Chapter02.html#boxplots",
    "href": "slides/Chapter02.html#boxplots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Boxplots",
    "text": "Boxplots\n\nGraphical display of 5-number summary\nCan show several groups of data on the same graph"
  },
  {
    "objectID": "slides/Chapter02.html#letter-value-table",
    "href": "slides/Chapter02.html#letter-value-table",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Letter-Value table",
    "text": "Letter-Value table\nAn extension of the boxplot, suitable for large data sets\n\n\nShows:\n\nM: Median\nF: Fourths\nE: Eighths\nD: Sixteenths\nC: Thirty-seconds\n…\n\n\n\n\n  Depth    Lower     Upper       Mid   Spread\nM  25.5 99.92736  99.92736  99.92736 0.000000\nF  13.0 99.43952 100.70136 100.07044 1.261832\nE   7.0 98.93218 101.20796 100.07007 2.275786\nD   4.0 98.73494 101.55871 100.14682 2.823770\nC   2.5 98.52396 101.75099 100.13747 3.227034\nB   1.5 98.17334 101.97793 100.07564 3.804590\nA   1.0 98.03338 102.16896 100.10117 4.135573\n\n\n\n\nHofmann, H; Wickham, H.; Kafadar, K. 2017. “Letter-Value Plots: Boxplots for Large Data.” Journal of Computational and Graphical Statistics 26 (3): 469–77. https://doi.org/10.1080/10618600.2017.1305277."
  },
  {
    "objectID": "slides/Chapter02.html#letter-value-plot",
    "href": "slides/Chapter02.html#letter-value-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Letter-Value Plot",
    "text": "Letter-Value Plot\nAn extension of the boxplot, suitable for large data sets"
  },
  {
    "objectID": "slides/Chapter02.html#cumulative-frequency-graphs",
    "href": "slides/Chapter02.html#cumulative-frequency-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Cumulative frequency graphs",
    "text": "Cumulative frequency graphs\n\nShow the left tail area\nUseful to obtain the quantiles (deciles, percentiles, quartiles etc)\n\n\n\nset.seed(123)\n\nd &lt;- data.frame(\n  x = rnorm(50, 100)\n  )\n\nggplot(d) + \n  aes(x) + \n  stat_ecdf()"
  },
  {
    "objectID": "slides/Chapter02.html#shiny-apps",
    "href": "slides/Chapter02.html#shiny-apps",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Shiny apps",
    "text": "Shiny apps\nLots of examples are available\n\nIn the study guide and workshops for this course (though not all of them are working currently)\nOn the web\n\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.univariate.graphs/\nhttps://shiny.massey.ac.nz/anhsmith/demos/get.univariate.plots/"
  },
  {
    "objectID": "slides/Chapter02.html#quantile-quantile-q-q-plot",
    "href": "slides/Chapter02.html#quantile-quantile-q-q-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Quantile-Quantile (Q-Q) plot",
    "text": "Quantile-Quantile (Q-Q) plot\nQ-Q plots compare the distributions of two data sets by plotting their quantiles against each other.\n\n\nvital &lt;- read.table(\n  \"https://www.massey.ac.nz/~anhsmith/data/vital.txt\", \n  header=TRUE, sep=\",\")\n\nquants &lt;- seq(0, 1, 0.05)\n\nvital |&gt; \n  summarise(\n    Female = quantile(Life_female, quants),\n    Male = quantile(Life_male, quants)\n  ) |&gt; \n  ggplot() +\n  aes(x = Female, y = Male) +\n  geom_point() + \n  geom_abline(slope=1, intercept=0) +\n  coord_fixed() +\n  ggtitle(\n    \"Quantiles of life expectancy\",\n    subtitle = \"are lower for males vs females\"\n    )"
  },
  {
    "objectID": "slides/Chapter02.html#some-q-q-plot-patterns",
    "href": "slides/Chapter02.html#some-q-q-plot-patterns",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Some Q-Q Plot patterns",
    "text": "Some Q-Q Plot patterns\n\nCase a: Quantiles of Y (mean/median etc) are higher than those of X\nCase b: Spread or SD of Y &gt; spread or SD of X\nCase c: X and Y follow different distributions \n\nR function: qqplot()."
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships",
    "href": "slides/Chapter02.html#bivariate-relationships",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1"
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships-1",
    "href": "slides/Chapter02.html#bivariate-relationships-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1 + \n  geom_smooth(span = 0.8, se = FALSE) + \n  ggtitle(\"Scatterplot with lowess smoother\")"
  },
  {
    "objectID": "slides/Chapter02.html#bivariate-relationships-2",
    "href": "slides/Chapter02.html#bivariate-relationships-2",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Bivariate relationships",
    "text": "Bivariate relationships\nA scatter plot shows the relationship between two quantitative variables. It can highlight linear or non-linear relationships, gaps/subgroups, outliers, etc. A lowess smoother or 2D density can help show the relationship.\n\n\np1 &lt;- ggplot(horsehearts) +\n  aes(x = EXTSYS, y = WEIGHT) +\n  geom_point() + ggtitle(\"Scatterplot\")\n\np1 + \n  geom_density_2d() +\n  ggtitle(\"Scatterplot with 2D density\")"
  },
  {
    "objectID": "slides/Chapter02.html#marginal-plot",
    "href": "slides/Chapter02.html#marginal-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Marginal Plot",
    "text": "Marginal Plot\nShows both bivariate relationships and univariate (marginal) distributions\n\n\np1 &lt;- ggplot(rangitikei) +\n  aes(x = people, y = vehicle) + \n  geom_point() + theme_bw()\n\nlibrary(ggExtra)\nggMarginal(p1, type=\"boxplot\")"
  },
  {
    "objectID": "slides/Chapter02.html#shiny-apps-1",
    "href": "slides/Chapter02.html#shiny-apps-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Shiny apps",
    "text": "Shiny apps\nExplore (though many of these are currently not working)\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.bivariate.plots/\nhttps://shiny.massey.ac.nz/anhsmith/demos/get.bivariate.plots/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.facet.wrapped.plots/\nhttps://shiny.massey.ac.nz/anhsmith/demos/get.facet.wrapped.plots/\nhttps://shiny.massey.ac.nz/anhsmith/demos/explore.facet.grid.plots/"
  },
  {
    "objectID": "slides/Chapter02.html#pairs-plot-scatterplot-matrix",
    "href": "slides/Chapter02.html#pairs-plot-scatterplot-matrix",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pairs plot / scatterplot matrix",
    "text": "Pairs plot / scatterplot matrix\n\n\nlibrary(GGally)\nggpairs(pinetree[,-1])"
  },
  {
    "objectID": "slides/Chapter02.html#pairs-plot-with-a-grouping-variable",
    "href": "slides/Chapter02.html#pairs-plot-with-a-grouping-variable",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Pairs plot with a grouping variable",
    "text": "Pairs plot with a grouping variable\n\n\nlibrary(GGally)\nggpairs(pinetree[,-1], \n        aes(colour = pinetree$Area))"
  },
  {
    "objectID": "slides/Chapter02.html#correlation-coefficients",
    "href": "slides/Chapter02.html#correlation-coefficients",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation coefficients",
    "text": "Correlation coefficients\nThe Pearson correlation coefficient measures the linear association between two variables."
  },
  {
    "objectID": "slides/Chapter02.html#correlation-matrix",
    "href": "slides/Chapter02.html#correlation-matrix",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\nTo show all pairwise correlation coefficients\nUseful to explore the inter-relationship between variables \n\n\n\nlibrary(psych)\ncorr.test(pinetree[,-1])\n\n\nCall:corr.test(x = pinetree[, -1])\nCorrelation matrix \n        Top Third Second First\nTop    1.00  0.92   0.96  0.97\nThird  0.92  1.00   0.95  0.91\nSecond 0.96  0.95   1.00  0.97\nFirst  0.97  0.91   0.97  1.00\nSample Size \n[1] 60\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n       Top Third Second First\nTop      0     0      0     0\nThird    0     0      0     0\nSecond   0     0      0     0\nFirst    0     0      0     0\n\n To see confidence intervals of the correlations, print with the short=FALSE option"
  },
  {
    "objectID": "slides/Chapter02.html#correlation-plots",
    "href": "slides/Chapter02.html#correlation-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Correlation Plots",
    "text": "Correlation Plots\n\n\nlibrary(corrplot)\ncorrplot(\n  cor(pinetree[,-1]),  \n  type = \"upper\", \n  method=\"number\"\n  )"
  },
  {
    "objectID": "slides/Chapter02.html#network-plots",
    "href": "slides/Chapter02.html#network-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Network plots",
    "text": "Network plots\n\n\nlibrary(corrr)\npinetree[,-1] |&gt; \n  correlate() |&gt; \n  network_plot(min_cor=0.2)"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots",
    "href": "slides/Chapter02.html#d-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D Plots",
    "text": "3-D Plots\nA bubble plot, shows the third (fourth) variable as point size (colour).\n\n\np1 &lt;- ggplot(pinetree) +\n  aes(x = First, \n      y = Second,\n      size = Third) + \n  geom_point() +\n  ggtitle(\"Bubble plot\")\n\np1"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots-1",
    "href": "slides/Chapter02.html#d-plots-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D Plots",
    "text": "3-D Plots\nA bubble plot, shows the third (fourth) variable as point size (colour).\n\n\np1 &lt;- ggplot(pinetree) +\n  aes(x = First, \n      y = Second,\n      size = Third) + \n  geom_point() +\n  ggtitle(\"Bubble plot\")\n\np1 + aes(colour = Area)"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them",
    "href": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D plots are far more useful if you can rotate them",
    "text": "3-D plots are far more useful if you can rotate them\nPackage plot3D\n\n\nlibrary(\"plot3D\")\n\nscatter3D(\n  x = pinetree$First, \n  y = pinetree$Second, \n  z = pinetree$Top, \n  phi = 0, bty = \"g\", \n  ticktype =\"detailed\"\n  )"
  },
  {
    "objectID": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them-1",
    "href": "slides/Chapter02.html#d-plots-are-far-more-useful-if-you-can-rotate-them-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "3-D plots are far more useful if you can rotate them",
    "text": "3-D plots are far more useful if you can rotate them\nPackage plotly\n\n\nlibrary(plotly)\n\nplot_ly(\n  pinetree, \n  x = ~First, \n  y = ~Second, \n  z = ~Top\n  ) |&gt; \n  add_markers()"
  },
  {
    "objectID": "slides/Chapter02.html#contour-plots",
    "href": "slides/Chapter02.html#contour-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Contour plots",
    "text": "Contour plots\n\n3D plots are difficult to interpret than 2D plots in general\nContour plots are another way of looking three variables in two dimensions\n\n\n\nlibrary(plotly)\nplot_ly(type = 'contour', \n        x=pinetree$First, \n        y=pinetree$Second, \n        z=pinetree$Top)"
  },
  {
    "objectID": "slides/Chapter02.html#conditioning-plots",
    "href": "slides/Chapter02.html#conditioning-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Conditioning plots",
    "text": "Conditioning plots\nConditioning Plots (Coplots) show two variables at different ranges of third variable\n\n\ncoplot(Top ~ First | Second*Area, \n       data = pinetree)"
  },
  {
    "objectID": "slides/Chapter02.html#conditioning-plots-1",
    "href": "slides/Chapter02.html#conditioning-plots-1",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Conditioning plots",
    "text": "Conditioning plots\nConditioning Plots (Coplots) show two variables at different ranges of third variable\n\n\n# install.packages(\"remotes\")\n# remotes::install_github(\"mpru/ggcleveland\")\nlibrary(ggcleveland)\ngg_coplot(\n  pinetree, \n  x = First, \n  y = Top, \n  faceting = Second, \n  number_bins = 6, \n  overlap = 3/4\n  )"
  },
  {
    "objectID": "slides/Chapter02.html#more-r-graphs",
    "href": "slides/Chapter02.html#more-r-graphs",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "More R graphs",
    "text": "More R graphs\nBuild plots in a single layout (R packages patchwork or gridExtra)\n\n\np1 &lt;- ggplot(testmarks) +\n  aes(y = English, x = Maths) + \n  geom_point()\n\np2 &lt;- p1 + \n  stat_density_2d(\n    geom = \"raster\",\n    aes(fill = after_stat(density)),\n    contour = FALSE) + \n  scale_fill_viridis_c() + \n  guides(fill=FALSE)\n\nlibrary(patchwork)\np1 / p2"
  },
  {
    "objectID": "slides/Chapter02.html#time-series-data",
    "href": "slides/Chapter02.html#time-series-data",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Time series data",
    "text": "Time series data\n\nA Time Series is an ordered sequence of observations of a variable(s) (often) made at equally spaced time points.\nTime series Components of variation\n\nTrend - representing long term positive (upward) or negative (downward) movement\nSeasonal - a periodic behaviour happening within a block (say Christmas time) of a given time period (say in a calendar year) but this periodic behaviour will repeat fairly regularly over time (say year after year)\nError (Residual)"
  },
  {
    "objectID": "slides/Chapter02.html#time-series-example",
    "href": "slides/Chapter02.html#time-series-example",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Time series example",
    "text": "Time series example\n\n\n\n\nnotes = read.table(\"../data/20dollar.txt\", header=TRUE, sep=\"\")\n\nlibrary(forecast)\n\n# value in millions; turn into time series (ts) object\nNZnotes20 &lt;- ts(notes$value / 1000, start=1968, frequency=1)\n\nautoplot(\n  NZnotes20, \n  xlab=\"Year\", \n  ylab=\"Value of $20 notes (millions)\"\n  ) + geom_point()"
  },
  {
    "objectID": "slides/Chapter02.html#autocorrelation-function-acf",
    "href": "slides/Chapter02.html#autocorrelation-function-acf",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Autocorrelation function (ACF)",
    "text": "Autocorrelation function (ACF)\nAutocorrelation captures the extent to which neighbouring data values are similar to each other.\nThe \\(k ^ \\text{th}\\) order ACF or the autocorrelation between \\(x_t\\) and \\(x_{t-k}\\) is\n\\[\\frac{\\text{Covariance}(x_t, x_{t-k})}{\\text{SD}(x_t)\\text{SD}(x_{t-k})} = \\frac{\\text{Covariance}(x_t, x_{t-k})}{\\text{Variance}(x_t)}\\]"
  },
  {
    "objectID": "slides/Chapter02.html#autocorrelation-function-acf-plot",
    "href": "slides/Chapter02.html#autocorrelation-function-acf-plot",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Autocorrelation function (ACF) Plot",
    "text": "Autocorrelation function (ACF) Plot\nThe significance of autocorrelations may be judged from the 95% confidence interval band\n\n\nggAcf(NZnotes20)\n\n\n\n\n\n\n\n\n\n\nAutocorrelations decay to zero ($20 notes positively depend on the values of $20 notes held in the immediate past rather than too distant past)"
  },
  {
    "objectID": "slides/Chapter02.html#pacf-partial-autocorrelation-function",
    "href": "slides/Chapter02.html#pacf-partial-autocorrelation-function",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "PACF (Partial Autocorrelation Function)",
    "text": "PACF (Partial Autocorrelation Function)\nA type of correlation after removing the effect of earlier lags\n\n\nggPacf(NZnotes20)"
  },
  {
    "objectID": "slides/Chapter02.html#time-series-trend-types",
    "href": "slides/Chapter02.html#time-series-trend-types",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Time series trend types",
    "text": "Time series trend types\nRequires a (parametric) model to fit the trend (covered later)\n\n\n\nNon-parametric fits can also be made"
  },
  {
    "objectID": "slides/Chapter02.html#seasonality",
    "href": "slides/Chapter02.html#seasonality",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Seasonality",
    "text": "Seasonality\nSimple scatter plot of the response variable against time may reveal seasonality directly\n\n\n# Data from:\n# https://www.stats.govt.nz/indicators/uv-intensity/\n\nuv = read.table(\"../data/uv.txt\",\n                header=TRUE, sep=\"\")\n\nuv &lt;- ts(uv$erythemal.uv, \n         start=c(1990,1), \n         frequency=12)\n\nautoplot(uv, \n         xlab=\"time\", \n         ylab=\"Erythemal UV\")"
  },
  {
    "objectID": "slides/Chapter02.html#sub-series-plots",
    "href": "slides/Chapter02.html#sub-series-plots",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Sub-series plots",
    "text": "Sub-series plots\nSeasonality is easily seen graphically when grouping variables are used\n\np1 &lt;- ggseasonplot(uv)\np2 &lt;- ggsubseriesplot(uv)\nlibrary(patchwork); p1 + p2"
  },
  {
    "objectID": "slides/Chapter02.html#time-series-decomposition",
    "href": "slides/Chapter02.html#time-series-decomposition",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Time series decomposition",
    "text": "Time series decomposition\n\n\n\nAdditive model\n\\(X_t\\) = Trend + Seasonal + Error\n(where \\(X_t\\) is an observation at time \\(t\\))\nMultiplicative model\n\\(X_t\\) = Trend \\(\\times\\) Seasonal + Error\n(trend and seasonal components are not independent)\nDetrending means removing the trend from the series, making it easier to see the seasonality.\nDeseasoning means removing the seasonality from the series, making it easier to see the trend.\n\n\n\n\n\nuv |&gt; decompose(type=\"additive\") |&gt; autoplot() + ggtitle(\"\")"
  },
  {
    "objectID": "slides/Chapter02.html#learning-eda",
    "href": "slides/Chapter02.html#learning-eda",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Learning EDA",
    "text": "Learning EDA\n\nThe best way to learn EDA is to try many approaches and find which are informative and which are not.\n\nChatfield (1995) on tackling statistical problems:\n\nDo not attempt to analyse the data until you understand what is being measured and why. Find out whether there is prior information such as are there any likely effects.\nFind out how the data were collected.\nLook at the structure of the data.\nThe data then need to be carefully examined in an exploratory way before attempting a more sophisticated analysis.\nUse common sense, and be honest!"
  },
  {
    "objectID": "slides/Chapter02.html#summary",
    "href": "slides/Chapter02.html#summary",
    "title": "Chapter 2:Exploratory Data Analysis (EDA)",
    "section": "Summary",
    "text": "Summary\n\nSize\n\nFor small datasets, we cannot be too confident in any patterns we see. More likely for patterns to occur ‘by chance’.\nSome displays are more affected by sample size than others\n\nShape\n\nIn can be interesting to display the overall shape of distribution.\nAre there gaps and/or many peaks (modes)?\nIs the distribution symmetrical? Is the distribution normal?\n\nOutliers\n\nBoxplots & scatterplots can reveal outliers\nMore influential than points in the middle\n\nGraphs should be simple and informative; certainly not misleading!\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site contains materials for 161250 Data Analysis, including the lecture slides, study guide, workshop code, and some extra code. The material is organised into 8 chapters, and all provided on one website for ease of access. Make use of the search function (using the magnifying glass icon on the top-right of the page)—it will allow you to quickly locate any text you enter across the whole site.\nThere is a lot of material on this site. You’ll notice that the Study Guide is big, especially the first two chapters. But don’t panic! You can treat the lectures and workshops as the core, examinable material, and treat the Study Guide as a supplement. The Study Guide is so big because it goes a bit deeper into some of the topics.\nFeel free to use resources other than this site, particularly when coding (there are lots of links in the first workshop). The internet is full of Q & A about coding R. If you have a persistent error message, copy and paste it into an internet search engine with “R” or “Quarto”. Large language models (LLMs), such as Bard and ChatGPT, are also very powerful tools to help with coding, if you can accurately describe what you want in words. Be careful though – LLMs often give you code that doesn’t work, but they can be useful nonetheless."
  },
  {
    "objectID": "slides/Chapter01.html#the-nature-of-data-random-variables",
    "href": "slides/Chapter01.html#the-nature-of-data-random-variables",
    "title": "Chapter 1:Data Collection",
    "section": "The nature of data: random variables",
    "text": "The nature of data: random variables\nData come from recording observations of the world. E.g.,\n\nrecording the number of heart beats per minute\ncounting birds in your yard\nmeasuring the lengths of fish you catch\n\nEach time you collect one of these observations, it is likely to be different.\n\nPulse may vary between 55 and 70 bpm, depending on when you record it\nNumber of birds varies at different times, and across different yards.\n\nWe call these random variables (often denoted with an upper case letter, \\(X\\)), because the particular value that a single observation or measurement will take is uncertain. The value varies across observations."
  },
  {
    "objectID": "slides/Chapter01.html#types-of-data",
    "href": "slides/Chapter01.html#types-of-data",
    "title": "Chapter 1:Data Collection",
    "section": "Types of data",
    "text": "Types of data"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-qualitative-data",
    "href": "slides/Chapter01.html#subtypes-of-qualitative-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of qualitative data",
    "text": "Subtypes of qualitative data\n\n\n\n\nNominal variables have no particular order (e.g., gender, colour, species, country)\n\n\n\nOrdinal variables can be ordered (e.g., altitude = {low, mid, high}, age group = {child, juvenile, adult} )"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-quantitative-data",
    "href": "slides/Chapter01.html#subtypes-of-quantitative-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of quantitative data",
    "text": "Subtypes of quantitative data\n\n\n\n\nContinuous variables have no gaps between possible values, as in measurements (e.g., weight, temperature, length)\n\n\n\nDiscrete variables have gaps between possible values, as in counts (e.g., number of siblings, number of flowers)"
  },
  {
    "objectID": "slides/Chapter01.html#subtypes-of-continuous-data",
    "href": "slides/Chapter01.html#subtypes-of-continuous-data",
    "title": "Chapter 1:Data Collection",
    "section": "Subtypes of continuous data",
    "text": "Subtypes of continuous data\n\nInterval scale\n\nNo absolute zero\nDivision & subtraction may not be meaningful\nTemperature in degrees Celcius is interval because 20°C is not twice as hot as 10°C.\n\n\nRatio scale\n\nZero = zero\nAll arithmetic manipulation can be done\nLength is ratio because 20 mm is twice as long as 10 mm."
  },
  {
    "objectID": "slides/Chapter01.html#data-collection-survey-experiment-census",
    "href": "slides/Chapter01.html#data-collection-survey-experiment-census",
    "title": "Chapter 1:Data Collection",
    "section": "Data Collection: Survey, Experiment, Census",
    "text": "Data Collection: Survey, Experiment, Census\n\n\n\nWe collect data from the world to get information about patterns and processes.\nMost datasets contain a subset, a sample, of a much bigger population of interest.\n\nWe may conduct a survey to collect a sample of data from different places, times, people, or organisms. We would rarely survey all of them.\nWe might conduct an experiment where we take a sample of elements (people, organisms, objects) and apply some treatment in a lab (e.g., drug, temperature, exercise regime, or other treatment) to study its effects.\n\nIf we are not dealing with a sample, if every element of the population of interest is represented in the dataset, we call this a census rather than a sample."
  },
  {
    "objectID": "slides/Chapter01.html#measurement-issues",
    "href": "slides/Chapter01.html#measurement-issues",
    "title": "Chapter 1:Data Collection",
    "section": "Measurement issues",
    "text": "Measurement issues\n\nMeasuring Devices or Instruments\n\na physical device - measuring rule to gauge the heights of plants\na counting device - a Geiger- counter for measuring radioactive material\na questionnaire - requires a more subjective response.\n\nMeasurement Error\n\nmeasuring instrument may be faulty (bias)\nvalues recorded from the same object may vary from one measurement to another (variance)\n\nIndirect measures\n\nFor example, we use Body Mass Index (BMI) as a measure of condition, and we measure temperature with the expansion of mercury."
  },
  {
    "objectID": "slides/Chapter01.html#non-response",
    "href": "slides/Chapter01.html#non-response",
    "title": "Chapter 1:Data Collection",
    "section": "Non-response",
    "text": "Non-response\n\na non-sampling error\nSelection stage: an element may be selected but not found\n\ne.g. sheep in a flock may be tagged with individual identification number but one may not be found at the time of the survey.\n\nCollection stage: it may not be possible to take a measurement\n\nsome respondents may forget, or refuse, to answer the questionnaire\n\nDocumentation stage\n\nIncorrect record of measurement\n\nCall-backs reduce non-response"
  },
  {
    "objectID": "slides/Chapter01.html#census-related-concepts",
    "href": "slides/Chapter01.html#census-related-concepts",
    "title": "Chapter 1:Data Collection",
    "section": "Census related concepts",
    "text": "Census related concepts\nTARGET POPULATION the population under study\nFRAME operationalises data collection from a target population. e.g. listing of elements in population.\nACTUAL POPULATION is the resulting set of elements on which usable data have been collected."
  },
  {
    "objectID": "slides/Chapter01.html#sample-vs-population",
    "href": "slides/Chapter01.html#sample-vs-population",
    "title": "Chapter 1:Data Collection",
    "section": "Sample vs population",
    "text": "Sample vs population\n\n\n\nA sample is a subset of the population.\nDatasets usually only contain a sample from the population; rarely do we have the entire population of data!\nWhy sample?\n\nSampling conserves resources (money, time, etc.).\nA well collected sample is more useful than a badly designed census.\nCollecting data may be destructive.\nThe disadvantage: the statistics we calculate from sample data is subject to sampling variation, which introduces uncertainty* about their true values.\n\n\n\n\n\n“You don’t have to eat the whole ox to know that the meat is tough” – Samuel Johnson (1709-1784)"
  },
  {
    "objectID": "slides/Chapter01.html#population-frame-and-sample",
    "href": "slides/Chapter01.html#population-frame-and-sample",
    "title": "Chapter 1:Data Collection",
    "section": "Population, frame, and sample",
    "text": "Population, frame, and sample"
  },
  {
    "objectID": "slides/Chapter01.html#statistical-inference",
    "href": "slides/Chapter01.html#statistical-inference",
    "title": "Chapter 1:Data Collection",
    "section": "Statistical inference",
    "text": "Statistical inference\n\n\nStatistical inference is the process of using information from sample data to make conclusions about the population.\n\nFor example, we want to know \\(\\mu\\), mean length of fish in a population. So, we collect a sample of fish, measure their lengths, calculate the mean \\(\\bar{x}\\), and use \\(\\bar{x}\\) as an estimate of \\(\\mu\\). This is statistical inference.\n\n\n\n\nThe sample mean \\(\\bar{x}\\) depends on which particular fish we happened to get in our sample.\nTherefore, the sample mean \\(\\bar{x}\\) itself is a random variable.\nIf we were to take 1000 different samples, we’d get 1000 different means."
  },
  {
    "objectID": "slides/Chapter01.html#bias-vs-sampling-variance",
    "href": "slides/Chapter01.html#bias-vs-sampling-variance",
    "title": "Chapter 1:Data Collection",
    "section": "Bias vs sampling variance",
    "text": "Bias vs sampling variance\n\n\nA method used to estimate \\(\\hat{\\theta}\\) a population parameter \\(\\theta\\) is called an estimator. An estimator includes the study design, methods of data collection, and mathematical operations.\nSampling variance is the sample-to-sample variation in an estimator.\nBias is when our estimator doesn’t get it right on average. That is, the average of estimates over \\(\\infty\\) samples is not centred on the population parameter; \\(\\text{Mean}(\\hat{\\theta}) \\neq \\theta\\).\n\n\n\n\nAn estimator can have high/low sampling variance and high/low bias."
  },
  {
    "objectID": "slides/Chapter01.html#principle-of-randomisation",
    "href": "slides/Chapter01.html#principle-of-randomisation",
    "title": "Chapter 1:Data Collection",
    "section": "Principle of randomisation",
    "text": "Principle of randomisation\n\n\n\nWe want our sample to be representative of (and have similar properties to) the population. The most straightforward way to do this is through randomisation.\nWe randomise the selection of objects for our sample to avoid bias. If we (consciously or subconsciously) tended to chose the largest fish for our sample, we’d get an upwardly biased estimate of the lengths.\nSimple random sampling or EPSEM (equal probability of selection) is the gold standard of random sampling."
  },
  {
    "objectID": "slides/Chapter01.html#simple-random-sampling-srs",
    "href": "slides/Chapter01.html#simple-random-sampling-srs",
    "title": "Chapter 1:Data Collection",
    "section": "Simple Random Sampling (SRS)",
    "text": "Simple Random Sampling (SRS)\n\n\n\nRandom selection of elements\n\n“Random” refers to the process not outcome\nEach (sampling) unit has same chance of being selected\nUnits can be selected with & without replacement\n\n\n\n\n\n\n\nSRS is easy to handle; suits even for a poor sampling frame\nSRS can be costly to implement\nSRS estimates are more variable than some alternatives\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#stratified-random-sampling-strs",
    "href": "slides/Chapter01.html#stratified-random-sampling-strs",
    "title": "Chapter 1:Data Collection",
    "section": "Stratified Random Sampling (STRS)",
    "text": "Stratified Random Sampling (STRS)\n\n\n\nSuitable for heterogeneous populations\nPopulation is divided into relatively homogeneous groups called strata and a random sample is taken from each stratum.\n\n\n\n\n\n\nSampling Approaches\n\nSample the larger strata more heavily (suits when all the strata are equally variable)\nSample the more varied strata are sampled\n\nAdvantages of STRS\n\nleads to efficient estimation That is, the variance (of an estimate) is usually less than that of SRS\nsample is spread throughout population\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#cluster-sampling",
    "href": "slides/Chapter01.html#cluster-sampling",
    "title": "Chapter 1:Data Collection",
    "section": "Cluster sampling",
    "text": "Cluster sampling\n\n\n\nA convenient method of sampling\npopulation is composed of clusters (groups)\nSelect certain clusters (randomly) and collect measurements from a random selection of the elements within the chosen clusters\nLarger variance than SRS!\n\n\n\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#systematic-random-sampling-syrs",
    "href": "slides/Chapter01.html#systematic-random-sampling-syrs",
    "title": "Chapter 1:Data Collection",
    "section": "Systematic Random sampling (SyRS)",
    "text": "Systematic Random sampling (SyRS)\n\n\n\nSelect every \\(k^{th}\\) element!\nRandom start within the first block of elements.\n\nConvenient and also the sample will be representative of population\nVariance of estimates - generally greater than those of SRS\nInefficient/inappropriate, if cycle or trend is present\n\n\n\n\n\n\n\nSmith et al. (2017)"
  },
  {
    "objectID": "slides/Chapter01.html#other-sampling-methods",
    "href": "slides/Chapter01.html#other-sampling-methods",
    "title": "Chapter 1:Data Collection",
    "section": "Other Sampling methods",
    "text": "Other Sampling methods\n\nProbability proportional to size (PPS)\n\ne.g., sampling high-value companies more likely than low-value companies\n\nMultistage\n\ne.g., first stage - cluster; second stage - SRS\n\nNon-probability sampling methods\n\nHaphazard / opportunistic / volunteer; take what you can get!\nSnowball; get your participants to find new participants\nPurposive; select items with certain characteristics; e.g., patients with particular symptoms\n\nNon-probability samples are often treated as random, requiring the assumption that the sample is representative. The validity of this assumption should be carefully considered."
  },
  {
    "objectID": "slides/Chapter01.html#some-sampling-methods",
    "href": "slides/Chapter01.html#some-sampling-methods",
    "title": "Chapter 1:Data Collection",
    "section": "Some sampling methods",
    "text": "Some sampling methods"
  },
  {
    "objectID": "slides/Chapter01.html#effective-sample-size-thumb-rule",
    "href": "slides/Chapter01.html#effective-sample-size-thumb-rule",
    "title": "Chapter 1:Data Collection",
    "section": "Effective Sample size (thumb rule)",
    "text": "Effective Sample size (thumb rule)\n\n\n\n\n\n\n\n\nSample Design\nDesign Effect (\\(d\\))\nEffective Sample Size (\\(\\frac{n}{d}\\))\n\n\n\n\nSRS\n1.00\n\\(n\\)\n\n\nSTRS\n0.80 to 0.90\n\\(\\frac{n}{0.9}\\) to \\(\\frac{n}{0.8}\\)\n\n\nCluster\n1.02 to 1.26\n\\(\\frac{n}{1.26}\\) to \\(\\frac{n}{1.02}\\)\n\n\nSyRS\n1.05\n\\(\\frac{n}{1.05}\\)\n\n\nQuota\n2\n\\(\\frac{n}{2}\\)"
  },
  {
    "objectID": "slides/Chapter01.html#summary",
    "href": "slides/Chapter01.html#summary",
    "title": "Chapter 1:Data Collection",
    "section": "Summary",
    "text": "Summary\n\nIssues to address\n\nWHAT are collected?\nWHO does the data collection?\nHOW are the data collected?\n\nBias occurs due to\n\nSELECTION\nCOLLECTION\nNON-RESPONSE (the single largest cause of bias!)\n\nA sample may have the same biases as a census along with sampling errors\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter03.html#probability-and-randomness",
    "href": "slides/Chapter03.html#probability-and-randomness",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability and randomness",
    "text": "Probability and randomness\n\n\n\n\nProbability and randomness are placeholders for incomplete knowledge.\nAfter I shuffled a deck of cards, you might consider the identity of the top card to be “random”.\nBut is it really?\nIf you knew the starting positions of the cards and a good HD video of my shuffling, you could surely know the positions of the cards, and which is on top.\nLikewise for rolling a die. If we know everything about the starting position, how it was thrown, the texture of the surface, humidity, etc., could we predict what it would roll?"
  },
  {
    "objectID": "slides/Chapter03.html#probability-as-a-relative-frequency",
    "href": "slides/Chapter03.html#probability-as-a-relative-frequency",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability as a relative frequency",
    "text": "Probability as a relative frequency\n\n\nThe classical definition of probability is just the relative frequency of an event.\n\nIf a fair die is rolled, there are \\(n = 6\\) possible outcomes. Let the event of interest be getting a number 4 or more. The probability of this event is 3 out of 6 or \\(p=1/2\\).\n\nThe sample space or the set of all possible outcomes need not be finite.\n\nExample: Tossing a coin until the first head appears will result in an infinite sample space. The probability can be viewed as a limiting or long run fraction of \\(m/n\\) (i.e. when \\(n \\to \\infty\\)).\n\nWhen the sample space is finite and outcomes are equally likely, we can assume that classical probability will be the same as empirical probability.\n\nExample: To find the probability of a fair coin landing heads it is not necessary to toss the coin repeatedly and observe the proportion of heads.\n\nProbabilities can only be between \\(0\\) (impossible) and \\(1\\) (certain).\nProbabilities can be subjective (such as expert opinion, or a guess)"
  },
  {
    "objectID": "slides/Chapter03.html#mutually-exclusive-events",
    "href": "slides/Chapter03.html#mutually-exclusive-events",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\nFor mutually exclusive events,\n\nThe probability of any two events co-occurring is zero\nThe probability of one event or another event occurring is the sum of the two respective probabilities.\nThe probability of any one event not occurring is the sum of those remaining.\n\n\nExample: A randomly selected single digit can be either odd (Event \\(O\\)) or even (Event \\(E\\)).\nThe events \\(O\\) and \\(E\\) are mutually exclusive because a number cannot be both odd and even.\nThe sample space is \\(\\{0,1,2,3,4,5,6,7,8,9\\}\\).\n\n\n\n\\(\\rm{Pr(E~\\&~O)=0}\\)\n\\(\\rm{Pr(E~ or~O)=1}\\)\n\\(\\rm{Pr(E)=1-Pr(O)}\\) and \\(\\rm{Pr(O)=1-Pr(E)}\\)"
  },
  {
    "objectID": "slides/Chapter03.html#statistical-independence",
    "href": "slides/Chapter03.html#statistical-independence",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Statistical Independence",
    "text": "Statistical Independence\nIf events \\(A\\) and \\(B\\) are statistically independent, then \\(P(A \\text{ and } B) = P(A) \\times P(B)\\).\n\n\n\n\n\n\nConditional probability\n\n\n\n\\(P(A|B)\\) is the probability of event \\(A\\) occurring given that event \\(B\\) is has occurred.\nFor example, the probability of a card you’ve drawn being a 5, given that it is a spade.\nThe sample space is reduced to that where \\(B\\) (e.g. the card is a spade) has occurred.\n\n\n\n\n\nWe say that two events (\\(A\\) and \\(B\\)) are independent if \\(P(A | B) = P(A)\\) and \\(P(B | A) = P(B)\\).\nObserving event \\(A\\) doesn’t make event \\(B\\) any more or less likely, and vice versa.\nFor any two independent events \\(A\\) and \\(B\\), \\(P(A \\text{ and } B ) = P(A|B) \\times P(B)\\) and \\(P(A \\textbf{ and } B ) = P(B|A) \\times P(A)\\)."
  },
  {
    "objectID": "slides/Chapter03.html#blood-group-example",
    "href": "slides/Chapter03.html#blood-group-example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Blood Group Example",
    "text": "Blood Group Example\n\n\n\n Two systems for categorising blood are:\n\nthe Rh system (Rh+ and Rh–)\nthe Kell system (K+ and K–)\n\nFor any person, their blood type in any one system\nis independent of their blood type in any other.\nFor Europeans in New Zealand,\nabout 81% are Rh+ and about 8% are K+.\n\nFrom the table:\n\nIf a European New Zealander is chosen at random, what is the probability that they are (Rh+ and K+) or (Rh– and K–)?\n\n0.0648 + 0.1748 = 0.2396\n\nSuppose that a murder victim has a bloodstain on him with type (Rh– and K+), presumably from the assailant. What is the probability that a randomly selected person matches this type?\n\n0.0152"
  },
  {
    "objectID": "slides/Chapter03.html#bayes-rule",
    "href": "slides/Chapter03.html#bayes-rule",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Bayes rule",
    "text": "Bayes rule\n\\[P(A\\mid B)=\\frac {P(B\\mid A)P(A)}{P(B)}~~~~~~~~\\rm{s.t}~~ P(B)&gt;0\\]\n\n\\(P(A\\mid B)\\) and \\(P(B\\mid A)\\) are conditional probabilities.\n\\(P(A)\\) and \\(P(B)\\) are marginal or prior probabilities."
  },
  {
    "objectID": "slides/Chapter03.html#prevalence-sensitivity-specificity-ppv-and-npv",
    "href": "slides/Chapter03.html#prevalence-sensitivity-specificity-ppv-and-npv",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Prevalence, sensitivity, specificity, PPV, and NPV",
    "text": "Prevalence, sensitivity, specificity, PPV, and NPV\nLet \\(D\\) be the event of a person having the Disease and \\(H\\) be the event of a person being Healthy (i.e., not having the disease). The outcome of a test for the disease can be either positive \\((T_+)\\) or negative \\((T_-)\\).\nConsider the following definitions of conditional probabilities:\n\nprevalence is the overall probability one has the disease, or \\(P(D)\\).\nsensitivity the probability that one tests positive given one has the disease, or \\(P(T_+ | D)\\).\nspecificity the probability that one tests negative given one does not have the disease, or \\(P(T_- | H)\\).\npositive predictive value of a test is the probability one has the disease given that one has tested positive, or \\(P(D \\mid T_{+})\\)\nnegative predictive value of a test is the probability that one is healthy given that one has tested negative, or \\(P(H \\mid T_{-})\\)"
  },
  {
    "objectID": "slides/Chapter03.html#example",
    "href": "slides/Chapter03.html#example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example",
    "text": "Example\nSay the following were true:\n\nPrevalence: \\(P(D) = 0.03\\) and \\(P(H) = 1-0.03=0.97\\)\nSensitivity: \\(P(T_+\\mid D) = 0.98\\)\nSpecificity: \\(P(T_{-}\\mid H) = 0.95\\)\n\nWe can use Bayes Rule to answer the following questions:\n\nWhat proportion of the overall population will test positive vs negative?\nWhat are the implications of a positive or negative test result?"
  },
  {
    "objectID": "slides/Chapter03.html#probability-tree",
    "href": "slides/Chapter03.html#probability-tree",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Probability tree",
    "text": "Probability tree\nIt can be useful to visualise the probabilities of the four possible states using a tree diagram.\n\n\n\n\n\n\n\nRules of the Probability Tree\n\nWithin each level, all branches are mutually exclusive events.\nThe tree covers all possibilities (i.e., the entire sample space).\nWe multiply as we move along branches.\nWe add when we move across branches.\n\n\n\n\n\n\n\n\nT+\nT-\n\n\n\n\n\nD\n0.0294\n0.0006\n0.03\n\n\nH\n0.0485\n0.9215\n0.97"
  },
  {
    "objectID": "slides/Chapter03.html#example-continued",
    "href": "slides/Chapter03.html#example-continued",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example continued",
    "text": "Example continued\nWhat proportion of the overall population will test positive vs negative?\nThe overall proportion of positive tests will be given by:\n\\[\n\\begin{aligned}\nP(T_{+}) &= P(T_{+} \\& D) + P(T_{+} \\& H) \\\\\n&= P(T_{+} \\mid D)P(D) + P(T_{+} \\mid H)P(H) \\\\\n&= 0.98 \\times 0.03 + 0.05 \\times 0.97 \\\\\n&= 0.0779\n\\end{aligned}\n\\] The overall proportion of negative tests will be given by:\n\\[\n\\begin{aligned}\nP(T_{-}) &= 1 - P(T_{+}) \\\\ &= 0.9221\n\\end{aligned}\n\\]\nComplete table of probabilities:\n\n\n\n\nT+\nT-\n\n\n\n\n\nD\n0.0294\n0.0006\n0.03\n\n\nH\n0.0485\n0.9215\n0.97\n\n\n\n0.0779\n0.9221\n1"
  },
  {
    "objectID": "slides/Chapter03.html#example-continued-1",
    "href": "slides/Chapter03.html#example-continued-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example continued",
    "text": "Example continued\nWhat are the implications of a positive or negative test result?\nAccording to Bayes rule, the probability of a random person having the disease given they’ve tested positive is given by:\n\\[\n\\begin{aligned}\nP(D\\mid T_{+}) &= \\frac {P(T_{+}\\mid D)P(D)} {P(T_{+})} \\\\\n&= \\frac{0.98 \\times 0.03}  {0.0779} \\\\\n&= 0.3774\n\\end{aligned}\n\\]\nAccording to Bayes rule, the probability of a random person not having the disease given they’ve tested negative is given by:\n\\[\n\\begin{aligned}\nP(H \\mid T_{-}) &= \\frac {P(T_{-} \\mid H)P(H)} {P(T_{-})} \\\\\n&= \\frac{0.95 \\times 0.97}  {0.9221} \\\\\n&= 0.9993\n\\end{aligned}\n\\]\nThe positive predictive value of the test is poor—only 38% of the subjects who tested positive will have the disease.\nThe negative predictive value is better—if a random subject tests negative, they’re very unlikely to have the disease."
  },
  {
    "objectID": "slides/Chapter03.html#discrete-probability-distributions-1",
    "href": "slides/Chapter03.html#discrete-probability-distributions-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Discrete probability distributions",
    "text": "Discrete probability distributions\nConsider the number of eggs \\((X)\\) in an Adelie penguin’s nest. The values range from \\(1\\) to \\(5\\), each with a certain probability (or relative frequency) of occurrence.\n\n\nNote the probabilities add to \\(1\\) because \\({1,2,3,4,5}\\) is a complete sample space.\n\nThe population mean \\(\\mu_X\\) is simply the sum of each outcome multiplied by its probability.\n\\[\\mu_X = E(X)= \\sum xP(X=x)=\\sum xP(x)\\]\nIn R,\n\nX &lt;- 1:5\nP &lt;- c(0.1, 0.2, 0.3,0.25,0.15)\n(Mean=sum(X*P))\n\n[1] 3.15\n\n\nThe population variance is given by\n\\[Var(X)= \\sigma_X^2=\\sum (x-\\mu_X)^2 P(x)\\]\nThe population SD is simply the square-root of the variance.\nIn R,\n\nX &lt;- 1:5\nP &lt;- c(0.1, 0.2, 0.3,0.25,0.15)\nMean=sum(X*P)\n(Variance =sum((X-Mean)^2*P))\n\n[1] 1.4275\n\n(SD=sqrt(Variance))\n\n[1] 1.19478"
  },
  {
    "objectID": "slides/Chapter03.html#binomial-distribution",
    "href": "slides/Chapter03.html#binomial-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\nConsider a variable that has two possible outcomes\n(say success and failure, with 50% probabilty each).\nThis can be described as a “Bernoulli” random variable.\nA “Binomial” is just a collection of Bernoulli trials.\nLet \\(X\\) be the number of heads when two coins are tossed.\nThe count of the number of successes \\(X\\) out of a fixed total of\n\\(n\\) independent trials follows the binomial distribution.\nThat is, \\(X \\sim Bin(n, p)\\), where \\(p\\) the probability of a success.\nThe binomial probability function \\(P(X=x)\\) or \\(P(x)\\)\nis given by \\[P(x)={n \\choose x}p^{x}(1-p)^{n-x}\\]\nFor \\(n=10\\), \\(p=0.3\\), the binomial probabilities,\n\\(P(x)\\) for \\(x=0,1,2, \\dots, 10\\), are plotted to the right.\nIf each of 10 basketball shots succeeded with probability 0.3, this describes the probability of your total score out of 10.\n\n\n\n\ndfm &lt;- data.frame(\n  x = as.factor(0:10), \n  Probability = dbinom(x = 0:10, size = 10, prob = 0.3))\nggplot(dfm) + aes(x = x, y = Probability) + geom_col() +\n  xlab(\"Number of successes (x)\") +\n  annotate(geom = \"table\", label = list(dfm), x=11, y=.05)"
  },
  {
    "objectID": "slides/Chapter03.html#example-1",
    "href": "slides/Chapter03.html#example-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example",
    "text": "Example\nA microbiologist plates out certain bacteria on a plate, and picks out 10 colonies. She knows that the probability of successfully creating a recombinant is 0.15.\nWhat is the probability that if she mixes all 10 colonies in a growth medium with penicillin, something (anything) will grow?\nIn other words:\nIf \\(X \\sim Bin(n = 10, p = 0.15)\\), what is \\(P(x &gt; 0)\\)?\nNote \\(P(x &gt; 0)=1-P(x = 0)\\). So in R, compute this as follows:\n\n1 - dbinom(x=0, size=10, prob=.15)\n\n[1] 0.8031256\n\n\nor\n\n1-pbinom(q=0, size=10, prob=.15)\n\n[1] 0.8031256"
  },
  {
    "objectID": "slides/Chapter03.html#binomial",
    "href": "slides/Chapter03.html#binomial",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Binomial",
    "text": "Binomial\nThe code pbinom(k,size=n,prob=p) gives the cumulative probabilities up to and including the quantile \\(k\\).\nThe Probability Mass Function (PMS) for a binomial random variable is:\n\\[P(X\\leq k)=\\sum _{i=0}^{k}{n \\choose x}p^{x}(1-p)^{n-x}\\]\nThe mean and variance of the binomial random variable is given by\n\\[\\mu_X=np~~~~ \\sigma^2_X=np(1-p)\\]\nIn the last example, the expected number of recombinant strain of bacteria is\n\\[\\mu_X=np=10*0.15=1.5\\]\nwith standard deviation\n\\[\\sigma_X=\\sqrt {np(1-p)}=1.129159\\]"
  },
  {
    "objectID": "slides/Chapter03.html#poisson-distribution",
    "href": "slides/Chapter03.html#poisson-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe Poisson distribution is used to obtain the probabilities of counts of relatively rare events that occur independently in space or time.\nSome Examples:\n\nThe number of snails in a quadrat \\((1~m^2)\\)\nFish counts in a visual transect (25m x 5m)\nBacterial colonies in 2 litres of milk"
  },
  {
    "objectID": "slides/Chapter03.html#poisson-distribution-1",
    "href": "slides/Chapter03.html#poisson-distribution-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe random variable \\(X\\), the number of occurrences (count), often follows the Poisson distribution whose probability function is given by\n\\[\\Pr(x)= \\frac{\\lambda^x e^{-\\lambda}}{x!}~~~ x=0,1,2,\\dots, \\infty\\]\nThe parameter \\(\\lambda\\) is the mean which is also equal to the variance.\n\\[\\mu_X=\\lambda~~~~ \\sigma^2_X=\\lambda\\]\nMain assumptions:\n\nThe events occur at a constant average rate of \\(\\lambda\\) per unit time or space.\nOccurrences are independent of one another as well as they do not happen at exactly the same unit time or space."
  },
  {
    "objectID": "slides/Chapter03.html#poisson-example",
    "href": "slides/Chapter03.html#poisson-example",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Poisson example",
    "text": "Poisson example\n\n\n\n\n\n\n\n\n\n\n\n Consider the number of changes that accumulate along a\nstretch of a neutrally evolving gene over a given period of time.\nThis is a Poisson random variable with a\npopulation mean of \\(\\lambda=kt\\), where\n\\(k\\) is the number of mutations per generation, and\n\\(t\\) is the time in generations that has elapsed.\n     \nAssume that \\(k = 1\\times10^{-4}\\) and \\(t = 500\\).\nFor \\(\\lambda=kt=0.05\\), the Poisson probabilities are shown in the following plot.\nWhat is the probability that at least one mutation has occurred over this period?\n\\(P(x &gt; 0)=1-P(x = 0)\\) is found in R as follows:\n\n1 - dpois(x=0, lambda=0.05)\n\n[1] 0.04877058"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-1",
    "href": "slides/Chapter03.html#continuous-probability-distributions-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nA discrete random variable takes values which are simply points on a real line. In other words, there is an inherent discontinuity in the values a discrete random variable can take.\nIf a random variable, \\(X\\), can take any value (i.e., not just integers) in some interval of the real line, it is called a continuous random variable.\nE.g., height, weight, length, percentage protein\nFor a discrete random variable \\(X\\), the associated probabilities \\(P(X=x)\\) are also just points or masses, and hence the probability function \\(P(x)\\) is also called as the probability mass function (PMF).\nFor continuous random variables, probabilities can be computed when the variable falls in an interval such as \\(5\\) to \\(15\\), but not when it takes a fixed value such as \\(10\\) (which is equal to zero).\nThe Probability Density Function (PDF) gives the relative likelihood of any particular value."
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-2",
    "href": "slides/Chapter03.html#continuous-probability-distributions-2",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\n\nFor example, consider a random proportion \\((X)\\) between \\(0\\) and \\(1\\). Here \\(X\\) follows a (standard) continuous uniform distribution whose (probability) density function \\(f(x)\\) is defined as follows:\n\\[f(x)=\\begin{cases}{1}~~~\\mathrm {for} \\ 0\\leq x\\leq 1,\\\\[9pt]0~~~\\mathrm {for} \\ x&lt;0\\ \\mathrm {or} \\ x&gt;1\\end{cases}\\] This constant density function is the simple one in the graph to the right.\n\n\n\ntibble(x = seq(-.5, 1.5, length=1000),\n       `f(x)` = dunif(x, min=0, max=1)) |&gt; \n  ggplot() + \n  aes(x = x, y = `f(x)`) +\n  geom_area(colour = 1, alpha = .2)\n\n\n\n\n\n\n\n\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-3",
    "href": "slides/Chapter03.html#continuous-probability-distributions-3",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n  \np\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1."
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-4",
    "href": "slides/Chapter03.html#continuous-probability-distributions-4",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n\np +\n  annotate(geom = \"path\", \n    x = c(19.3, 19.3, 13), \n    y = c(0, rep(dnorm(19.3,20,2),2) ),\n    arrow = arrow(),\n    colour = \"dodgerblue4\", size = 1.1)\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1.\nThe density at 19.3 is \\(f(19.3) = 0.1876\\)).\n\ndnorm(19.3, mean = 20, sd = 2)\n\n[1] 0.1876202"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-5",
    "href": "slides/Chapter03.html#continuous-probability-distributions-5",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe density is the relative likelihood of any value of \\(x\\); that is, the height of the Probability Density Function (PDF). Say, the leaves of a particular tree had mean length 20 cm, SD 2.\n\n\nd &lt;- tibble(x = seq(13, 27, by=0.01),\n            Density = dnorm(x, 20, 2)) \n\np &lt;- ggplot(d) + aes(x, Density) + \n  geom_hline(yintercept=0) +\n  geom_area(colour = 1,\n            fill = \"darkorange\", \n            size = 1.1, alpha = .6) \n\np +\n  geom_area(\n    data = d |&gt; filter(x &lt;= 19.3),\n    fill = \"dodgerblue4\",\n    size  = 1.1, alpha = .6)\n\n\n\n\n\n\n\n\n\n\nThe black line is the PDF, or \\(f(x)\\). The orange area underneath the whole PDF is 1.\nThe area under the curve to the left of the value 19.3 is given by the Cumulative Density Function (CDF), or \\(F(x)\\). It gives the probability that x &lt; 19.3; \\(F(19.3) = 0.3632\\).\n\npnorm(19.3, mean = 20, sd = 2)\n\n[1] 0.3631693"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-6",
    "href": "slides/Chapter03.html#continuous-probability-distributions-6",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe cumulative distribution function, CDF, \\(F(x)\\) gives the left tail area or probability up to \\(x\\). This is probability is found as\n\\[F_{X}(x)=\\int _{-\\infty }^{x}f_{X}(t)\\,dt\\] The relationship between the density function \\(f(x)\\) and the distribution function \\(F(x)\\) is given by the Fundamental Theorem of Calculus.\n\\[f(x)={dF(x) \\over dx}\\]"
  },
  {
    "objectID": "slides/Chapter03.html#continuous-probability-distributions-7",
    "href": "slides/Chapter03.html#continuous-probability-distributions-7",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Continuous probability distributions",
    "text": "Continuous probability distributions\nThe total area under the PDF curve is \\(1\\). The probability of obtaining a value between two points (\\(a\\) and \\(b\\)) is the area under the PDF curve between those two points. This probability is given by \\(F(b)-F(a)\\).\nFor the uniform distribution \\(U(0,1)\\), \\(f(x)=1\\). So\n\\[F_{X}(x)=\\int _{-\\infty }^{x}\\,dt=x\\]\nFor example, the probability of a randomly drawn fraction from the interval \\([0,1]\\) to fall below \\(x=0.5\\) is 50%.\nThe probability of a random fraction falling between \\(a=0.2\\) and \\(b=0.8\\) is\n\\[F(b)-F(a)=0.8-0.2=0.6\\]\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#the-normal-gaussian-distribution",
    "href": "slides/Chapter03.html#the-normal-gaussian-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "The Normal (Gaussian) Distribution",
    "text": "The Normal (Gaussian) Distribution\n\n\nThe Gaussian or Normal Distribution is parameterised in terms of the mean \\(\\mu\\) and the variance \\(\\sigma ^{2}\\) and its Probability Density Function (PDF) is given by\n\\[f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}\\] A Standard Normal Distribution has mean \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\). It has a simpler PDF:\n\\[f(z)={\\frac {1}{ {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}z^{2}}\\] If \\(X \\sim N(\\mu, \\sigma)\\), you can convert the \\(X\\) values into \\(Z\\)-scores by subtracting the mean \\(\\mu\\) and dividing by the standard deviation \\(\\sigma\\).\n\\[Z={\\frac {X-\\mu }{\\sigma }}\\]\nWe often deal with the standard normal because the symmetric bell shape of the normal distribution remains the same for all \\(\\mu\\) and \\(\\sigma\\).\n\n\n \n\ndfn &lt;- tibble(x=seq(-4,4,length=1000), \n              `f(x)` = dnorm(x), \n              `F(x)` = pnorm(x))\np1 &lt;- ggplot(dfn) + aes(x=x,y=`f(x)`) + geom_line() + \n  geom_vline(xintercept = 0) + \n  labs(title = \"Standard Normal Density\", \n       x = \"standard normal deviate, z\")\np2 &lt;- ggplot(dfn) + aes(x=x,y=`F(x)`) + geom_line() + \n  geom_vline(xintercept = 0) + \n  labs(title = \"Cumulative Standard Normal Density\", \n       x = \"standard normal deviate, z\")\np1/p2 \n\n\n\n\n\n\n\n\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#example-of-a-normal",
    "href": "slides/Chapter03.html#example-of-a-normal",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Example of a normal",
    "text": "Example of a normal\n\nThe weight of an individual of Amphibola crenata, a marine snail,\nis normally distributed with a mean of \\(40g\\) and variance of \\(20g^2\\).\n\n  \n\ndfs &lt;- tibble(x=seq(20, 60, length=1000), \n    `f(x)` = dnorm(x, mean=40, sd=sqrt(20)))\n\nps &lt;- ggplot(dfs) + aes(x = x, y = `f(x)`) + \n  geom_area(fill=\"gray\") +\n  geom_vline(xintercept=40) \n\nps\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability of getting a snail that weighs between \\(35g\\) and \\(50g\\)?\n\n\n\n\nIn R, the function pnorm() gives the CDF.\n\npnorm(50, mean=40, sd=sqrt(20)) - \n  pnorm(35, mean=40, sd=sqrt(20)) \n\n[1] 0.8555501\n\n\n\n\n\n\n\nps + \n  geom_area(data = dfs |&gt; filter(x &lt; 50 & x &gt; 35),\n            fill=\"coral1\", alpha=.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability of getting a snail that weighs below \\(35g\\) or over \\(50g\\)?\n\n\n\npnorm(35, mean=40, sd=sqrt(20)) + \n  pnorm(50, mean=40, sd=sqrt(20), lower.tail=FALSE) \n\n[1] 0.1444499\n\n\n\nps + \n  geom_area(data = dfs |&gt; filter(x &gt; 50),\n            fill=\"coral1\", alpha=.5) + \n  geom_area(data = dfs |&gt; filter(x &lt; 35),\n            fill=\"coral1\", alpha=.5)"
  },
  {
    "objectID": "slides/Chapter03.html#areas-probabilities-under-the-standard-normal",
    "href": "slides/Chapter03.html#areas-probabilities-under-the-standard-normal",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Areas (probabilities) under the standard normal",
    "text": "Areas (probabilities) under the standard normal\nUnder standard normal, the areas under the PDF curve are shown below for various situations.\n\n\nContinuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#log-normal-distribution",
    "href": "slides/Chapter03.html#log-normal-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Log-normal distribution",
    "text": "Log-normal distribution\n\nA random variable \\(X\\) is log-normally distributed, when \\(log_e(X)\\) follows normal.\nAlternatively, if \\(X\\) follows normal, then \\(e^X\\) follows log-normal.\nR function dlnorm() gives the log-normal density.\nMean & variance:\n\\[\n\\begin{align}\n\\mu_X&=e^{\\left(\\mu +{\\frac {\\sigma ^{2}}{2}}\\right)} \\\\\n\\sigma_X^2&=(e^{\\sigma ^{2}}-1) e^{(2\\mu +\\sigma ^{2})}\n\\end{align}\n\\]\n\n\n\ndfln &lt;- tibble(x=seq(0, 4, length=1000), \n              `f(x)` = dlnorm(x), \n              `F(x)` = plnorm(x))\np1 &lt;- ggplot(dfln) + aes(x=x,y=`f(x)`) + geom_area(alpha=.8) + \n  labs(title = \"Standard log-normal density\", \n       x = \"standard log-normal deviate, z\")\np2 &lt;- ggplot(dfln) + aes(x=x,y=`F(x)`) + geom_line() + \n  labs(title = \"Cumulative standard log-normal density\", \n       x = \"standard log-normal deviate, z\")\np1/p2 \n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#weibull-distribution",
    "href": "slides/Chapter03.html#weibull-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Weibull distribution",
    "text": "Weibull distribution\n\nThe PDF of the Weibull distribution is:\n\\[f(t;\\eta,\\beta) =\n\\begin{cases}\n\\frac{\\beta}{\\eta}\\left(\\frac{x}{\\eta}\\right)^{\\beta-1}e^{-(x/\\eta)^{\\beta}}  ~~x\\geq0 ,\\\\\n0~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  x&lt;0,\n\\end{cases}\\]\n\\(\\beta~(&gt; 0)\\) is the called the shape parameter and \\(\\eta~(&gt; 0)\\) is the called scale parameter.\nThe Weibull distribution becomes the exponential distribution for \\(\\beta=1\\).\nThe scale parameter \\(\\eta\\) is called the characteristic life because \\(\\eta\\) becomes the quantile with slightly less than two-thirds of the population (63%) below it irrespective of the shape \\(\\beta\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#gamma-distribution",
    "href": "slides/Chapter03.html#gamma-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nThe probability function of the gamma distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\beta\\) is given below:\n\\[\\displaystyle {\\begin{aligned}f(x)={\\frac {\\beta ^{\\alpha }x^{\\alpha -1}e^{-\\beta x}}{\\Gamma (\\alpha )}}\\quad {\\text{ for }}x&gt;0\\quad \\alpha ,\\beta &gt;0,\\\\[6pt]\\end{aligned}}\\] where \\(\\displaystyle \\Gamma (\\alpha)=\\int _{0}^{\\infty }x^{\\alpha-1}e^{-x}\\,dx.\\)\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#beta-distribution",
    "href": "slides/Chapter03.html#beta-distribution",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Beta distribution",
    "text": "Beta distribution\nThe beta distribution is bounded on the interval \\([0, 1]\\) and parameterised by two positive shape parameters, say \\(\\alpha\\) and \\(\\beta\\).\nProbability function of the beta distribution:\n\\[\\begin{aligned}f(x;\\alpha ,\\beta ) ={\\frac {x^{\\alpha -1}(1-x)^{\\beta -1}}{\\displaystyle \\int _{0}^{1}u^{\\alpha -1}(1-u)^{\\beta -1}\\,du}}={\\frac {1}{\\mathrm {B} (\\alpha ,\\beta )}}x^{\\alpha -1}(1-x)^{\\beta -1}\\end{aligned} \\] where \\(\\mathrm {B} (\\alpha ,\\beta )=\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}\\). Here’s a plot of beta density for various shape parameter combinations.\n\nWhen \\(\\alpha=\\beta=1\\), the beta distribution becomes the continuous uniform distribution.\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#small-sample-effect",
    "href": "slides/Chapter03.html#small-sample-effect",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Small sample effect",
    "text": "Small sample effect\nFor small samples, the shape might be difficult to judge.\n\n\nset.seed(1234)\ndfm &lt;- data.frame(\n  x=rnorm(50, \n          mean=80, \n          sd=12)\n  )\n\np1 &lt;- ggplot(dfm) + \n  geom_histogram(\n    aes(x=x, y=after_stat(density)), \n    colour=1\n    ) + \n  stat_function(\n    fun = dnorm, \n    args = list(mean = 80, sd = 12), \n    geom = \"line\"\n    ) +\n  xlim(min(dfm), max(dfm))\n\np2 &lt;- ggplot(dfm) + aes(x) + \n  geom_boxplot() +\n  xlim(min(dfm), max(dfm)) +\n  theme_void()\n\nlibrary(patchwork)\np1 / p2 + plot_layout(heights = c(5, 1))\n\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#normal-quantile-plots",
    "href": "slides/Chapter03.html#normal-quantile-plots",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Normal quantile plots",
    "text": "Normal quantile plots\nIn a normal quantile plot, the quantiles of the sample are plotted against the theoretical quantiles of the fitted normal distribution.\nThe points should roughly lie on a straight line\nWe can also compare the empirical and theoretical CDFs.\n\n\n\n\n\n\n\n\n\nTV viewing time data\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#skewed-data",
    "href": "slides/Chapter03.html#skewed-data",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Skewed data",
    "text": "Skewed data\nThe number of people who made use of a recreational facility is in the rangitikei dataset.\nThe deviation of points from the line indicate that this variable does not conform to a normal distribution.\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#fitting-lognormal",
    "href": "slides/Chapter03.html#fitting-lognormal",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Fitting lognormal",
    "text": "Fitting lognormal\nR package fitdistrplus can be used to estimate the lognormal parameters for people data.\nThe likelihood function is based on the joint probability of the observed data as a function of the distributional parameters.\nThe ML method maximises the likelihood function to estimate the distributional (model) parameters.\n\nfitdistrplus default is the ML method\n\n\nlibrary(fitdistrplus)\nfitdist(rangitikei$people, \"lnorm\")\n\nFitting of the distribution ' lnorm ' by maximum likelihood \nParameters:\n        estimate Std. Error\nmeanlog 3.775514  0.1789851\nsdlog   1.028191  0.1265611\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#fitting-lognormal-1",
    "href": "slides/Chapter03.html#fitting-lognormal-1",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Fitting lognormal",
    "text": "Fitting lognormal\nR package fitdistrplus can be used to estimate the lognormal parameters for people data.\nThe likelihood function is based on the joint probability of the observed data as a function of the distributional parameters.\nThe ML method maximises the likelihood function to estimate the distributional (model) parameters. Examine the fit using graphical displays.\n\n\nlnormfit &lt;- fitdist(rangitikei$people, \"lnorm\")\nplot(lnormfit)\n\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#gamma-fit",
    "href": "slides/Chapter03.html#gamma-fit",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Gamma fit",
    "text": "Gamma fit\nGamma distribution also fits OK but the outlier remains.\nOutliers and subgroups will affect the Maximum Likelihood (ML) employed in the fitdistrplus package.\n\n\nfitdist(rangitikei$people, \"gamma\")\n\n\nFitting of the distribution ' gamma ' by maximum likelihood \nParameters:\n        estimate  Std. Error\nshape 1.14299006 0.249453814\nrate  0.01593588 0.004314727\n\n\n\n\n\ngammafit &lt;- fitdist(rangitikei$people, \"gamma\")\nplot(gammafit)\n\n\n\n\n\n\n\n\n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#cullen-and-frey-plot",
    "href": "slides/Chapter03.html#cullen-and-frey-plot",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Cullen and Frey plot",
    "text": "Cullen and Frey plot\nThis is a plot of kurtosis, a measure of excessive peakedness, against the square of a measure of skew.\n\ndescdist(rangitikei$people)\n\n\nsummary statistics\n------\nmin:  4   max:  470 \nmedian:  46 \nmean:  71.72727 \nestimated sd:  86.28089 \nestimated skewness:  3.3264 \nestimated kurtosis:  17.11618 \n\n\nSymmetrical distributions such as normal (read corresponding to zero skew) are poor fits. Lognormal also does not fare well.\nWhen the observed data is a mixture from two or more distributions or contain a large number of unusual observations, no single distributional fit will be satisfactory.\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#tv-watching-time-data",
    "href": "slides/Chapter03.html#tv-watching-time-data",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "TV watching time data",
    "text": "TV watching time data\nNormal fit is supported.\n\ndescdist(tv$TELETIME)\n\n\nsummary statistics\n------\nmin:  490   max:  2799 \nmedian:  1760.5 \nmean:  1729.283 \nestimated sd:  567.913 \nestimated skewness:  -0.004088284 \nestimated kurtosis:  2.32614 \n\n\n\nSkewed continuous probability distributions"
  },
  {
    "objectID": "slides/Chapter03.html#which-distribution-is-most-appropriate",
    "href": "slides/Chapter03.html#which-distribution-is-most-appropriate",
    "title": "Chapter 3:Probability Concepts & Distributions",
    "section": "Which distribution is most appropriate?",
    "text": "Which distribution is most appropriate?\n\nRemember, theoretical distributions aren’t real—they’re just models—but they can be useful. Keep your purpose in mind.\nChoose the simplest distribution that provides an adequate fit.\nData may be best served by a mixture of two or more distributions rather than a single distribution.\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter05.html#tables-and-frequencies",
    "href": "slides/Chapter05.html#tables-and-frequencies",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Tables and frequencies",
    "text": "Tables and frequencies\n\nWe often have hypotheses regarding the frequencies of levels of a factor or group of factors.\n\nFor a single two-level factor (e.g., male vs female, survived vs died), we might wish to know how likely the data are to have come from a population with equal proportions (or some other specified proportion).\nFor two factors, we might wish to know whether they are independent.\n\nIn either case, we can specify a null hypothesis and test it using data.\nTo do so, compare observed counts with expected counts, where the expected counts are derived from our null model about the population.\nWe are employing the Chi-squared statistic with data which consists of integers. That is, the data are discrete rather than continuous."
  },
  {
    "objectID": "slides/Chapter05.html#examples-with-one-factor",
    "href": "slides/Chapter05.html#examples-with-one-factor",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Examples with one factor",
    "text": "Examples with one factor\nExample 1\n\nA dataset contains 40 males and 50 females.\nHow plausible is the null model of these counts coming from a population with 50% males and 50% females?\nThe expected counts in this case would be 45 males and 45 females.\n\nExample 2\n\n\n\nDoes the distribution of rejects of metal castings by causes in a particular week vary from the long-term average counts?\nTreat the long-term average counts as the expected counts.\nCompare observed counts with expected counts.\n\n\n\n\n\nCauses of rejection\nRejects during the week\nLong-term average\n\n\n\n\nsand\n90\n82\n\n\nmisrun\n8\n4\n\n\nshift\n16\n10\n\n\ndrop-\n8\n6\n\n\ncorebreak\n23\n21\n\n\nbroken\n21\n20\n\n\nother\n5\n8"
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-test",
    "href": "slides/Chapter05.html#goodness-of-fit-test",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit test",
    "text": "Goodness of fit test\n\nCompare observed frequencies with expected frequencies under some specified null hypothesis.\nMake a hypothesis about the population, what would we expect the frequency to be under that hypothesis?\nFor example:\n\nWish to compare the frequency of occurence of different phenotypes in an organism with the frequencies we would expect under Mendel’s laws of inheritance.\nWish to compare the distribution of a observation with expected count we would obtain from a Poisson distribution.\n\n\nHypotheses of this sort can be tested using the Chi-squared test statistic (\\(\\chi ^ 2\\) = Ki Sq.)"
  },
  {
    "objectID": "slides/Chapter05.html#example",
    "href": "slides/Chapter05.html#example",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\nA survey of voters included 550 males and 450 females. Will you call this survey as a biased one?\nHere the null hypothesis is that the ratio of males to females is 1:1.\nEquivalent: the proportion of males is equal to the proportion of female in the population\n\n\n\nGender\n\\(O\\)\n\\(E\\)\n\\((O-E)^2/E\\)\n\n\n\n\nmale\n550\n500\n\\((550-500)^2/ 500=5\\)\n\n\nfemale\n450\n500\n\\((450-500)^2/ 500=5\\)\n\n\nsum\n1000\n1000\n10\n\n\n\n\\[\n\\chi ^{2} = \\sum _{1}^{c}\\frac{\\left( O-E\\right)^{2}}{E} =\n\\frac{(550-500)^2}{500} + \\frac{(450-500)^2}{500} = 10\n\\]"
  },
  {
    "objectID": "slides/Chapter05.html#mendels-experiment-see-study-guide",
    "href": "slides/Chapter05.html#mendels-experiment-see-study-guide",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Mendel’s experiment (see Study Guide)",
    "text": "Mendel’s experiment (see Study Guide)\n\n\nMendel discovered the principles of heredity by breeding garden peas. In one Mendel’s trials ratios of various types of peas (dihybrid-crosses) were 9:3:3:1\nThe observed results are very close to expected results. This results in a small chi squared value. Were experimental results fudged or was there a confirmation bias?"
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-for-distributions",
    "href": "slides/Chapter05.html#goodness-of-fit-for-distributions",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit for distributions",
    "text": "Goodness of fit for distributions\n\nTreat class intervals as categories and obtain the actual counts (O)\nThe assumed distribution gives the expected counts (E)\nPerform a goodness of fit and validate the assumed theoretical distribution\nAdjust the degrees of freedom (df) for the number of estimated parameters of the theoretical distribution\n\nFor example, assume that you have 10 class intervals and test for normal distribution, which has 2 parameters.\n\nSo the df for this test will be 10-1-2=7."
  },
  {
    "objectID": "slides/Chapter05.html#contingency-table",
    "href": "slides/Chapter05.html#contingency-table",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Contingency table",
    "text": "Contingency table\n\nGiven a two way table of frequency counts, we test whether the row and column variables are independent\nHypotheses:\n\nNull: the two factors are independent\nwritten another way: the row (or column) distributions are the same\n\nThe expected count for cell \\((i,j)\\) is given by \\(E_{ij}\\) = \\((T_i \\times T_j)/n\\) where\n\\(~~~~~T_i\\), the total for row \\(i\\);\n\\(~~~~~T_j\\), the total for column \\(j\\)\n\\(~~~~~n\\), the overall total count\nTest statistic : \\(\\chi ^{2} =\\sum _{{i=1}}^{r}\\sum _{{j=1}}^{{\\rm c}}\\frac{\\left({ O}_{{ ij}} { -E}_{{ij}} \\right)^{{ 2}} }{{ E}_{{ij}} }.\\)\ndegrees of freedom: \\((r-1)(c-1)\\)"
  },
  {
    "objectID": "slides/Chapter05.html#example-3",
    "href": "slides/Chapter05.html#example-3",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\n\n\ndf= (2-1)=1\nAt 5% level (\\(\\alpha =0.05\\)), the critical value is only 3.84. So the sample is a biased one."
  },
  {
    "objectID": "slides/Chapter05.html#computations",
    "href": "slides/Chapter05.html#computations",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Computations",
    "text": "Computations\n\n\n\n\nPorcine Stress Syndrome (PSS) data\n\n\n\nHalothane.positive\nHalothane.negative\nTotals\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nTotals\n32\n311\n343"
  },
  {
    "objectID": "slides/Chapter05.html#inference",
    "href": "slides/Chapter05.html#inference",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Inference",
    "text": "Inference\n\nThe tabulated Chisq with (4-1)x(2-1) = 3 d.f are\n\n7.81 at 5% level; 16.27 at 1% level.\n\n\nConclusion: There is a statistical evidence that the breed is not independent of the result of the Halothane test.\n\n\nNote that large counts in the second column (Halothane negative) lead to large expected values but low contributions to chi-squared statistic.\nThis is because of the division by the appropriate expected value. On the other hand, the small observations of first column lead to small expected values but large contributions to the \\(\\chi ^{2}\\)."
  },
  {
    "objectID": "slides/Chapter05.html#significant-cell-contribution",
    "href": "slides/Chapter05.html#significant-cell-contribution",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Significant cell contribution",
    "text": "Significant cell contribution\n\nCounts follow Poisson distribution for which mean = variance\nHence \\(\\chi^{2}=\\sum \\frac{\\left({\\rm residual}\\right)^{{\\rm 2}} }{{\\rm variance}} =\\sum \\left(\\frac{{\\rm residual}}{{\\rm std\\; dev}} \\right)^{2}.\\)\nIndividual cell contribution is similar to standardized residual. Any standardized residual greater than 2 is regarded as significant.\nSo \\(2^2 = 4\\) is treated as a significant contribution to the \\(\\chi ^{2}\\) statistic."
  },
  {
    "objectID": "slides/Chapter05.html#warnings",
    "href": "slides/Chapter05.html#warnings",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Warnings",
    "text": "Warnings\n\nUse only frequency counts. Use of percentages in place of counts may lead to incorrect conclusions.\nCheck for small expected values. An expected value of less than 5 may lead to concern and a very small value of less than 1 is a warning. Sometimes, you can merge/combine categories in case of small expected counts.\nIf the chi-squared statistic is small enough to be not significant, there is no problem.\nIf chi-squared statistic is significant, check the contributions to each cell. If cells with large expected value (&gt;5) contribute a large amount to chi-squared statistic, again there is no problem.\nIf cells with expected values less than 5 lead large contributions to chi-squared statistic, the significance of the chi-squared statistic should be treated with caution."
  },
  {
    "objectID": "slides/Chapter05.html#simpsons-paradox",
    "href": "slides/Chapter05.html#simpsons-paradox",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\nGroup 1\n\n\n     [,1] [,2]\n[1,]   80  120\n[2,]   30   80\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group1\nX-squared = 4.4809, df = 1, p-value = 0.03428\n\n\nGroup 2\n\n\n     [,1] [,2]\n[1,]   20   75\n[2,]   25   20\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  group2\nX-squared = 15.122, df = 1, p-value = 0.0001008\n\n\nAfter amalgamation of both groups\n\n\n     [,1] [,2]\n[1,]  100  195\n[2,]   55  100\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  all\nX-squared = 0.053808, df = 1, p-value = 0.8166"
  },
  {
    "objectID": "slides/Chapter05.html#permutation-test",
    "href": "slides/Chapter05.html#permutation-test",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Permutation test",
    "text": "Permutation test\nThis test is done maintaining the marginal totals.\nData: Smoking Status vs. Staff  Groupings\n\n\n\nSmoking Status vis-a-vis Staff Groupings\n\n\n\nNone\nModerate\nHeavy\nTotals\n\n\n\n\nJunior employees\n18\n57\n13\n88\n\n\nJunior managers\n4\n10\n4\n18\n\n\nSecretaries\n10\n13\n2\n25\n\n\nSenior employees\n25\n22\n4\n51\n\n\nSenior managers\n4\n5\n2\n11\n\n\nTotals\n61\n107\n25\n193\n\n\n\n\n\n\n\nPermutation test\n\nset.seed(12321)\nchisq.test(tabledata, simulate.p.value = TRUE)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  tabledata\nX-squared = 15.672, df = NA, p-value = 0.04948\n\n\nRegular Chi-square test\n\nchisq.test(tabledata)\n\n\n    Pearson's Chi-squared test\n\ndata:  tabledata\nX-squared = 15.672, df = 8, p-value = 0.04733"
  },
  {
    "objectID": "slides/Chapter05.html#log-linear-model",
    "href": "slides/Chapter05.html#log-linear-model",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Log-linear model",
    "text": "Log-linear model\nThis model is built based on the assumption that the log of the count, which is predicted or modelled by categorical factors, is normally distributed.\nEasy to fit using the MASS package but the data must be in the tall format.\n\n\nRows: 15\nColumns: 3\n$ Smoke_status &lt;chr&gt; \"None\", \"Moderate\", \"Heavy\", \"None\", \"Moderate\", \"Heavy\",…\n$ Employ       &lt;chr&gt; \"Junior employees\", \"Junior employees\", \"Junior employees…\n$ Counts       &lt;dbl&gt; 18, 57, 13, 4, 10, 4, 10, 13, 2, 25, 22, 4, 4, 5, 2\n\n\n\nlibrary(MASS)\nMASS::loglm(Counts~ Smoke_status+Employ, smk)\n\nCall:\nMASS::loglm(formula = Counts ~ Smoke_status + Employ, data = smk)\n\nStatistics:\n                      X^2 df   P(&gt; X^2)\nLikelihood Ratio 15.52691  8 0.04967430\nPearson          15.67163  8 0.04732869\n\n\n\nIn addition to the usual Pearson Chi-square statistic, a different test statistic based on the likelihood principle is also given.\nUseful to model multi-way tables with more than two factors.\nGeneralised linear models are preferred to log-linear models where the count response can be assumed to follow Poisson. Logistic models can be built to binomial counts. These models are covered in the applied linear models course."
  },
  {
    "objectID": "slides/Chapter05.html#correspondence-analysis-not-examined-in-depth",
    "href": "slides/Chapter05.html#correspondence-analysis-not-examined-in-depth",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Correspondence Analysis | (not examined in depth)",
    "text": "Correspondence Analysis | (not examined in depth)\n\nCA is for assessing interdependence of categorical variables presented in the form of a contingency table\nData: Smoking Status vs. Staff Groupings\n\n\n\n                 None Moderate Heavy\nJunior employees   18       57    13\nJunior managers     4       10     4\nSecretaries        10       13     2\nSenior employees   25       22     4\nSenior managers     4        5     2\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tabledata\nX-squared = 15.672, df = 8, p-value = 0.04733"
  },
  {
    "objectID": "slides/Chapter05.html#row-mass-row-profiles",
    "href": "slides/Chapter05.html#row-mass-row-profiles",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Row mass & row profiles",
    "text": "Row mass & row profiles\n\nrow profiles are found dividing the cell counts by the corresponding row total\nrow mass is found dividing the row totals by the grand total\n\n\n\n                  None Moderate  Heavy\nJunior employees 0.205    0.648 0.1477\nJunior managers  0.222    0.556 0.2222\nSecretaries      0.400    0.520 0.0800\nSenior employees 0.490    0.431 0.0784\nSenior managers  0.364    0.455 0.1818\nrowmass          0.316    0.554 0.1295\n\n\n\nSimilarly column profiles & column masses can be found"
  },
  {
    "objectID": "slides/Chapter05.html#graphical-display-of-row-profiles",
    "href": "slides/Chapter05.html#graphical-display-of-row-profiles",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Graphical display of row profiles",
    "text": "Graphical display of row profiles\n\n\nNo clear patterns seen"
  },
  {
    "objectID": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping",
    "href": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Symmetric plots to find subgrouping",
    "text": "Symmetric plots to find subgrouping\n\nlibrary(\"FactoMineR\")\nCA(tabledata) |&gt; summary()\n\n\n\nCall:\nCA(X = tabledata) \n\nThe chi square of independence between the two variables is equal to 15.7 (p-value =  0.0473 ).\n\nEigenvalues\n                       Dim.1   Dim.2\nVariance               0.074   0.008\n% of var.             90.570   9.430\nCumulative % of var.  90.570 100.000\n\nRows\n                   Iner*1000    Dim.1    ctr   cos2    Dim.2    ctr   cos2  \nJunior employees |    26.268 | -0.235 34.372  0.962 | -0.047 12.934  0.038 |\nJunior managers  |     8.784 | -0.236  7.042  0.590 |  0.197 47.082  0.410 |\nSecretaries      |     5.618 |  0.195  6.670  0.873 | -0.074  9.301  0.127 |\nSenior employees |    37.894 |  0.379 51.521  1.000 |  0.004  0.045  0.000 |\nSenior managers  |     2.636 |  0.071  0.394  0.110 |  0.203 30.638  0.890 |\n\nColumns\n                   Iner*1000    Dim.1    ctr   cos2    Dim.2    ctr   cos2  \nNone             |    49.186 |  0.394 66.705  0.997 |  0.020  1.688  0.003 |\nModerate         |    15.679 | -0.157 18.619  0.873 | -0.060 25.941  0.127 |\nHeavy            |    16.335 | -0.289 14.676  0.661 |  0.207 72.371  0.339 |"
  },
  {
    "objectID": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping-1",
    "href": "slides/Chapter05.html#symmetric-plots-to-find-subgrouping-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Symmetric plots to find subgrouping",
    "text": "Symmetric plots to find subgrouping"
  },
  {
    "objectID": "slides/Chapter05.html#summary",
    "href": "slides/Chapter05.html#summary",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Summary",
    "text": "Summary\n\nGoodness of fit is for testing whether the observed counts are from the hypothesised population groups.\n\nFor \\(c\\) categories (groups), the test involves \\(c-1\\) df.\n\nContingency Table (\\(r\\) rows and \\(c\\) columns) data are tested for the independence.\n\nFor \\(r\\times c\\) cells the test involves \\((r-1)(c-1)\\) df\n\n\\(\\chi ^{2}\\) test works well if \\(E&gt; 5\\). Some could be as low as 1.\nDo correspondence analysis (symmetric plots) when independence is rejected.\n\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides/Chapter07.html#learning-objectives",
    "href": "slides/Chapter07.html#learning-objectives",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\n\nUnderstand and describe a multiple linear regression, the difference from a simple linear regression, and when each are appropriate\nFit and display a multiple regression\nUnderstand and interpret different types of Sums of Squares\nUnderstand and test for multicollinearity and other assumptions"
  },
  {
    "objectID": "slides/Chapter07.html#what-is-a-multiple-regression",
    "href": "slides/Chapter07.html#what-is-a-multiple-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "What is a multiple Regression?",
    "text": "What is a multiple Regression?\n\nIn a simple regression, there is only one predictor.\nMultiple regression modelling involves many predictors."
  },
  {
    "objectID": "slides/Chapter07.html#when-to-use-multiple-predictors",
    "href": "slides/Chapter07.html#when-to-use-multiple-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "When to use multiple predictors",
    "text": "When to use multiple predictors\n\nStatistical control of a confound: controlling treatment for some unwanted variability\nMultiple causation: multiple things are thought to cause changes in the outcome variable\nInteractions: we are interested in how two variables may combine to change our outcome (will cover this in the next chapter)"
  },
  {
    "objectID": "slides/Chapter07.html#multiple-regression",
    "href": "slides/Chapter07.html#multiple-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nWhere to start:\n\nQuestion and hypothesis + domain knowledge\n\nWhat are we interested in testing?\nWhat data is collected? do those variables make sense?\nWhat kind of data are my response and predictor variables?\n\nPerform EDA first\n\nA scatter plot matrix can show nonlinear relationships\nA correlation matrix will only show the strength of pairwise linear relationships\n\nLook for the predictors having the largest correlation with response\n\nLook for inter-correlations between the predictors and choose the one with high correlation with response variable but uncorrelated with the rest."
  },
  {
    "objectID": "slides/Chapter07.html#data-example",
    "href": "slides/Chapter07.html#data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Data example",
    "text": "Data example\nBasketball team summary for 2020 regular season\n\nWe have 2118 data points which equates to 1059 games (2 teams per game), 2020 was a short season\nEach team in a standard season plays 82 games. That means in total, an NBA season is comprised of 1,230 games.\n\n\n  Team     Game Spread Outcome MIN PTS       P2p       P3p       FTp OREB DREB\n1  ATL 21900014     17       W  48 117 0.6000000 0.3548387 0.8571429    8   34\n2  ATL 21900028      4       W  48 103 0.6296296 0.3000000 0.5333333    9   43\n3  ATL 21900043     -2       L  48 103 0.4736842 0.3333333 0.6875000    8   37\n4  ATL 21900052    -15       L  48  97 0.5454545 0.2820513 0.6666667    9   24\n5  ATL 21900066     -9       L  48  97 0.5370370 0.2058824 0.6923077   16   34\n6  ATL 21900099      8       W  48 108 0.5333333 0.3666667 0.6875000    9   39\n  AST TOV STL BLK PF\n1  27  13   9   2 15\n2  22  18   5   9 26\n3  23  21  12   3 25\n4  28  20  14   7 29\n5  20  16   5   5 15\n6  22  18   9   5 20\n\n\nSpread: Point difference between winning and losing team\nPTS: Total points scored by a team in a game\nP2p: Percent of 2-pointers made\nP3p: Percent of 3-pointers made\nFTp: Percent of free-throws made\nOREB: Offensive rebounds\nDREB: Defensive rebounds\nAST: Assists\nSTL: Steals\nBLK: Blocks"
  },
  {
    "objectID": "slides/Chapter07.html#data-example-1",
    "href": "slides/Chapter07.html#data-example-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Data example",
    "text": "Data example\n\noptions(digits=3)\ncor(teams[,c(3, 6:12, 14:15)])\n\n        Spread     PTS      P2p      P3p     FTp     OREB     DREB     AST\nSpread  1.0000  0.5730  0.36338  0.42719  0.1063 -0.00240  0.46470  0.3477\nPTS     0.5730  1.0000  0.48713  0.55094  0.1897 -0.01487  0.14131  0.5103\nP2p     0.3634  0.4871  1.00000 -0.00601 -0.0105 -0.28585  0.02484  0.3633\nP3p     0.4272  0.5509 -0.00601  1.00000  0.0297 -0.18736  0.00164  0.4054\nFTp     0.1063  0.1897 -0.01053  0.02967  1.0000 -0.08960 -0.01724 -0.0297\nOREB   -0.0024 -0.0149 -0.28585 -0.18736 -0.0896  1.00000  0.03882 -0.0996\nDREB    0.4647  0.1413  0.02484  0.00164 -0.0172  0.03882  1.00000  0.0468\nAST     0.3477  0.5103  0.36333  0.40540 -0.0297 -0.09960  0.04681  1.0000\nSTL     0.1246  0.0352  0.01897 -0.04479  0.0115 -0.00389 -0.18027  0.0518\nBLK     0.1837  0.0623  0.03757  0.00540 -0.0185  0.00311  0.20700  0.0411\n            STL      BLK\nSpread  0.12460  0.18367\nPTS     0.03525  0.06234\nP2p     0.01897  0.03757\nP3p    -0.04479  0.00540\nFTp     0.01153 -0.01849\nOREB   -0.00389  0.00311\nDREB   -0.18027  0.20700\nAST     0.05178  0.04112\nSTL     1.00000  0.03267\nBLK     0.03267  1.00000"
  },
  {
    "objectID": "slides/Chapter07.html#inter-relationships",
    "href": "slides/Chapter07.html#inter-relationships",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Inter-relationships",
    "text": "Inter-relationships"
  },
  {
    "objectID": "slides/Chapter07.html#full-regression",
    "href": "slides/Chapter07.html#full-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Full regression",
    "text": "Full regression\nPlaces all of the predictors in the model\nEquivalent of throwing everything in and hoping something sticks\n\n\n\nCall:\nlm(formula = Spread ~ ., data = teams_forlm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-32.29  -5.72  -0.15   5.66  30.36 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -142.4223     2.8728  -49.58  &lt; 2e-16 ***\nPTS            0.0624     0.0269    2.32     0.02 *  \nP2p           74.1170     3.8391   19.31  &lt; 2e-16 ***\nP3p           74.0109     3.3449   22.13  &lt; 2e-16 ***\nFTp           15.4686     2.0265    7.63  3.4e-14 ***\nOREB           0.7191     0.0623   11.54  &lt; 2e-16 ***\nDREB           1.2033     0.0367   32.81  &lt; 2e-16 ***\nAST           -0.0350     0.0479   -0.73     0.47    \nSTL            1.0685     0.0677   15.79  &lt; 2e-16 ***\nBLK            0.3503     0.0784    4.47  8.2e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.79 on 2108 degrees of freedom\nMultiple R-squared:  0.622, Adjusted R-squared:  0.62 \nF-statistic:  385 on 9 and 2108 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#residuals",
    "href": "slides/Chapter07.html#residuals",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Residuals",
    "text": "Residuals\nJust like with a simple regression we examine residuals to look for patterns\n\nThese look great, probably because we have a ton of data in this example."
  },
  {
    "objectID": "slides/Chapter07.html#multicollinearity",
    "href": "slides/Chapter07.html#multicollinearity",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity is where at least two predictor variables are highly correlated.\nMulticollinearity does not affect the residual SD very much, and doesn’t pose a major problem for prediction.\nThe major effects of multicollinearity are:\n\nIt changes the estimates of the coefficients.\nIt inflates the variance of the estimates of the coefficients. That is, it increases the uncertainty about what the slope parameters are.\nTherefore, it matters when testing hypotheses about the effects of specific predictors."
  },
  {
    "objectID": "slides/Chapter07.html#multicollinearity-1",
    "href": "slides/Chapter07.html#multicollinearity-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nThe impact of multicollinearity on the variance of the estimates can be quantified using the Variance Inflation Factor (VIF &lt; 5 is considered ok).\nThere are several ways to deal with multicollinarity, depending on context. We can discard one of highly correlated variable, perform ridge regression, or think more carefully about how the variables relate to each other."
  },
  {
    "objectID": "slides/Chapter07.html#multicollinearity-2",
    "href": "slides/Chapter07.html#multicollinearity-2",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nIn R we can examine Multicollinearity using the function vif()\nRemember our basketball regression:\n\nlibrary(car)\nvif(full_reg)\n\n PTS  P2p  P3p  FTp OREB DREB  AST  STL  BLK \n3.07 2.14 2.24 1.14 1.37 1.14 1.50 1.06 1.05 \n\n\nThese values are all small (VIF &lt; 5) so we can continue on in interpreting the regression."
  },
  {
    "objectID": "slides/Chapter07.html#tidy-summary",
    "href": "slides/Chapter07.html#tidy-summary",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Tidy summary",
    "text": "Tidy summary\n\n\n\nt-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-142.422\n2.873\n-49.575\n0.000\n\n\nPTS\n0.062\n0.027\n2.320\n0.020\n\n\nP2p\n74.117\n3.839\n19.306\n0.000\n\n\nP3p\n74.011\n3.345\n22.126\n0.000\n\n\nFTp\n15.469\n2.027\n7.633\n0.000\n\n\nOREB\n0.719\n0.062\n11.538\n0.000\n\n\nDREB\n1.203\n0.037\n32.809\n0.000\n\n\nAST\n-0.035\n0.048\n-0.729\n0.466\n\n\nSTL\n1.069\n0.068\n15.785\n0.000\n\n\nBLK\n0.350\n0.078\n4.470\n0.000"
  },
  {
    "objectID": "slides/Chapter07.html#model-summary-measures",
    "href": "slides/Chapter07.html#model-summary-measures",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model Summary Measures",
    "text": "Model Summary Measures\n\n\n\nModel summary measures\n\n\nr.squared\n0.62\n\n\nsigma\n8.79\n\n\nstatistic\n384.83\n\n\np.value\n0.00\n\n\nAIC\n15227.82\n\n\nBIC\n15290.06"
  },
  {
    "objectID": "slides/Chapter07.html#variance-explained",
    "href": "slides/Chapter07.html#variance-explained",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Variance Explained",
    "text": "Variance Explained\n\\(R^2\\) is the proportion of variance in \\(y\\) explained by \\(x\\) .\n\nsummary(full_reg)\n\n\nCall:\nlm(formula = Spread ~ ., data = teams_forlm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-32.29  -5.72  -0.15   5.66  30.36 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -142.4223     2.8728  -49.58  &lt; 2e-16 ***\nPTS            0.0624     0.0269    2.32     0.02 *  \nP2p           74.1170     3.8391   19.31  &lt; 2e-16 ***\nP3p           74.0109     3.3449   22.13  &lt; 2e-16 ***\nFTp           15.4686     2.0265    7.63  3.4e-14 ***\nOREB           0.7191     0.0623   11.54  &lt; 2e-16 ***\nDREB           1.2033     0.0367   32.81  &lt; 2e-16 ***\nAST           -0.0350     0.0479   -0.73     0.47    \nSTL            1.0685     0.0677   15.79  &lt; 2e-16 ***\nBLK            0.3503     0.0784    4.47  8.2e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.79 on 2108 degrees of freedom\nMultiple R-squared:  0.622, Adjusted R-squared:  0.62 \nF-statistic:  385 on 9 and 2108 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#variance-explained-1",
    "href": "slides/Chapter07.html#variance-explained-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Variance Explained",
    "text": "Variance Explained\n\\(R^2=\\frac{SS~regression}{SS~Total}\\)\n\nSS &lt;- anova(full_reg) |&gt; \n   tidy() |&gt; \n  select(term:sumsq) |&gt; \n  janitor::adorn_totals()\nSS\n\n      term   df    sumsq\n       PTS    1 141194.6\n       P2p    1   4001.7\n       P3p    1  14465.4\n       FTp    1    583.8\n      OREB    1   7082.2\n      DREB    1  78359.3\n       AST    1     10.1\n       STL    1  20082.0\n       BLK    1   1542.2\n Residuals 2108 162702.8\n     Total 2117 430024.0\n\n\n\n\\(R^2_{adj}\\) is adjusted to remove the variation that is explained by chance alone\n\\(R^2_{adj}=1-\\frac{MS~Error}{MS~Total}\\)\nwhere:\n\\(MS~Error\\) = \\(frac{SSE}{n-k-1}\\)\n\\(MS~Total\\) = \\(frac{SST}{n-1}\\)\ntherefore \\(R^2_{adj}\\) can also be written as: \\(R^2_{adj}=1-\\frac{(1-R^2)*(n-1)}{(n-k-1)}\\)"
  },
  {
    "objectID": "slides/Chapter07.html#additional-variation-explained",
    "href": "slides/Chapter07.html#additional-variation-explained",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Additional variation explained",
    "text": "Additional variation explained\n\nVariation in \\(Y\\) is separated into two parts SSR and SSE.\n\nThe shaded overlap of two circles represent the variation in \\(Y\\) explained by the \\(X\\) variables.\n\nThe total overlap of \\(X_1\\) and \\(X_2\\), and \\(Y\\) depends on\n\nrelationship of \\(Y\\) with \\(X_1\\) and \\(X_2\\)\ncorrelation between \\(X_1\\) and \\(X_2\\)"
  },
  {
    "objectID": "slides/Chapter07.html#sequential-addition-of-predictors",
    "href": "slides/Chapter07.html#sequential-addition-of-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Sequential addition of predictors",
    "text": "Sequential addition of predictors\n\nAddition of variables decreases SSE and increases SSR and \\(R^2\\).\n\\(s^2\\) = MSE = SSE/df decreases to a minimum and then increases since addition of variable decreases SSE but adds to df."
  },
  {
    "objectID": "slides/Chapter07.html#significance-of-type-i-or-seq.ss",
    "href": "slides/Chapter07.html#significance-of-type-i-or-seq.ss",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Significance of Type I or Seq.SS",
    "text": "Significance of Type I or Seq.SS\n\nThe Type I SS is the SS of a predictor after adjusting for the effects of the preceding predictors in the model.\n\nSometimes order matters, particularly with unequal sample sizes\n\nFor unbalanced data, this approach tests for a difference in the weighted marginal means. In practical terms, this means that the results are dependent on the realized sample sizes. In other words, it is testing the first factor without controlling for the other factor, which may not be the hypothesis of interest.\nF test for the significance of the additional variation explained\nR function anova() calculates sequential or Type-I SS"
  },
  {
    "objectID": "slides/Chapter07.html#type-ii",
    "href": "slides/Chapter07.html#type-ii",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Type II",
    "text": "Type II\n\nType II SS is based on the principle of marginality.\n\nEach variable effect is adjusted for all other appropriate effects.\n\nequivalent to the Type I SS when the variable is the last predictor entered the model.\n\nOrder matters for Type I SS but not for Type II SS"
  },
  {
    "objectID": "slides/Chapter07.html#type-iii-ss",
    "href": "slides/Chapter07.html#type-iii-ss",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Type III SS",
    "text": "Type III SS\n\nType III SS is the SS added to the regression SS after ALL other predictors including an interaction term.\nThis type tests for the presence of a main effect after other main effects and interaction. This approach is therefore valid in the presence of significant interactions.\nIf the interaction is significant SS for main effects should not be interpreted"
  },
  {
    "objectID": "slides/Chapter07.html#ss-types-in-action",
    "href": "slides/Chapter07.html#ss-types-in-action",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "SS types in action",
    "text": "SS types in action\nConsider a model with terms A and B:\nType 1 SS:\nSS(A) for factor A.\nSS(B | A) for factor B.\nType 2 SS:\nSS(A | B) for factor A.\nSS(B | A) for factor B.\nType 3 SS:\nSS(A | B, AB) for factor A.\nSS(B | A, AB) for factor B.\n\nWhen data are balanced and the design is simple, types I, II, and III will give the same results.\nSS explained is not always a good criterion for selection of variables"
  },
  {
    "objectID": "slides/Chapter07.html#summary-so-far",
    "href": "slides/Chapter07.html#summary-so-far",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Summary so far",
    "text": "Summary so far\n\nDefined multiple regression\nWhy we might run a multiple regression\nHow to perform a multiple regression, examine residuals, multicollinearity\nVariation and R squared\nSums of Squares types"
  },
  {
    "objectID": "slides/Chapter07.html#learning-objectives-1",
    "href": "slides/Chapter07.html#learning-objectives-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\n\nCautionary tales: when correlation misleads us\nHow to make models\nComparing models\nPrinciples of model selection"
  },
  {
    "objectID": "slides/Chapter07.html#does-waffle-houses-cause-divorce",
    "href": "slides/Chapter07.html#does-waffle-houses-cause-divorce",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Does Waffle Houses cause divorce??",
    "text": "Does Waffle Houses cause divorce??"
  },
  {
    "objectID": "slides/Chapter07.html#or-is-it-butter",
    "href": "slides/Chapter07.html#or-is-it-butter",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Or is it butter?",
    "text": "Or is it butter?\n\nAnd if you want more to impress your friends at a BBQ, the source is: http://www.tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "slides/Chapter07.html#spurious-association",
    "href": "slides/Chapter07.html#spurious-association",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Spurious association",
    "text": "Spurious association\n\nYou’ve heard that before: correlation does not imply causation. BUT it doesn’t discard it either\nHope you are seated: causation does not imply correlation.\nCausation implies conditional correlation (up to linearity issues).\nWe need more than just statistical models to answer causal questions."
  },
  {
    "objectID": "slides/Chapter07.html#how-do-we-deal-with-spurious-associations",
    "href": "slides/Chapter07.html#how-do-we-deal-with-spurious-associations",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "How do we deal with spurious associations?",
    "text": "How do we deal with spurious associations?\n\nDomain knowledge: you know that waffles and butter don’t cause divorce so why might they be correlated?\n\nIs there another predictor that would be better?\n\nMultiple regressions can disentangle the association between two predictors and an outcome\n\nStatistical control of a confound"
  },
  {
    "objectID": "slides/Chapter07.html#masked-associations",
    "href": "slides/Chapter07.html#masked-associations",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Masked associations",
    "text": "Masked associations\n\nAssociation between a predictor variable and an outcome can be masked by another variable.\nYou need to observe both variables to see the “true” influence of either on the outcome.\nHow do we account for the masking variable (seen as a nuisance)?"
  },
  {
    "objectID": "slides/Chapter07.html#masking-situations-tend-to-arise-when",
    "href": "slides/Chapter07.html#masking-situations-tend-to-arise-when",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Masking situations tend to arise when:",
    "text": "Masking situations tend to arise when:\n\nBoth predictors are associated with one another.\nHave opposite relationships with the outcome."
  },
  {
    "objectID": "slides/Chapter07.html#milk-and-brain-data-example",
    "href": "slides/Chapter07.html#milk-and-brain-data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Milk and brain data example",
    "text": "Milk and brain data example"
  },
  {
    "objectID": "slides/Chapter07.html#primate-milk-data",
    "href": "slides/Chapter07.html#primate-milk-data",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Primate milk data",
    "text": "Primate milk data"
  },
  {
    "objectID": "slides/Chapter07.html#first-we-will-consider-simple-regressions",
    "href": "slides/Chapter07.html#first-we-will-consider-simple-regressions",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "First we will consider simple regressions",
    "text": "First we will consider simple regressions"
  },
  {
    "objectID": "slides/Chapter07.html#multiple-regression-1",
    "href": "slides/Chapter07.html#multiple-regression-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Multiple regression",
    "text": "Multiple regression\n\n\n\nTemperature Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.00\n0.064\n0\n1\n\n\ntemp\n0.77\n0.064\n12\n0\n\n\n\n\n\n\n\n\nShark Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.083\n0.00\n1\n\n\nshark\n0.557\n0.084\n6.64\n0\n\n\n\n\n\n\n\n\nTemperature and Sharks\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.064\n0.000\n1.000\n\n\ntemp\n0.776\n0.094\n8.217\n0.000\n\n\nshark\n-0.008\n0.094\n-0.084\n0.933"
  },
  {
    "objectID": "slides/Chapter07.html#masking-situation",
    "href": "slides/Chapter07.html#masking-situation",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Masking situation",
    "text": "Masking situation\nTend to arise when\n\nBoth predictors are associated with one another.\nHave different relationships with the outcome"
  },
  {
    "objectID": "slides/Chapter07.html#how-do-we-deal-with-this",
    "href": "slides/Chapter07.html#how-do-we-deal-with-this",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "How do we deal with this?",
    "text": "How do we deal with this?\n\nStatistically there is not really an answer\nThe answer lies in the causes and the causes are not in the data; Shark attacks dont cause ice cream sales or vice versa\nRemember that interpreting the (regression) parameter estimates always depends upon what you believe the causal model"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection",
    "href": "slides/Chapter07.html#model-selection",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model Selection",
    "text": "Model Selection\n\nThe first step before selection of the best subset of predictors is to study the correlation matrix\nWe then perform stepwise additions (forward) or subtractions (backward) from the model and compare them\n\nBUT…\n\nWe saw with the illustration of SS how the significance or otherwise of a variable in a multiple regression model depends on the other variables in the model\nTherefore, we cannot fully rely on the t-test and discard a variable because its coefficient is insignificant"
  },
  {
    "objectID": "slides/Chapter07.html#selection-of-predictors",
    "href": "slides/Chapter07.html#selection-of-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Selection of predictors",
    "text": "Selection of predictors\n\nHeuristic (short-cut) procedures based on criteria such as \\(F\\), \\(R^2_{adj}\\), \\(AIC\\), \\(C_p\\) etc\n\nForward Selection: Add variables sequentially\n\nconvenient to obtain the simplest feasible model\n\nBackward Elimination: Drop variables sequentially\n\nIf difference between two variables is significant but not the variables themselves, forward regression would obtain the wrong model since both may not enter the model.\n\nKnown as suppressor variables case (like masking variables discussed earlier)\n\n\n\n\nExample: (try)\n\n\n\nCall:\nlm(formula = y ~ x1, data = suppressor)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1691 -0.6791 -0.0033  0.6441  1.1299 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.98876    1.26689    9.46  3.4e-07 ***\nx1           0.00375    0.41608    0.01     0.99    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.832 on 13 degrees of freedom\nMultiple R-squared:  6.24e-06,  Adjusted R-squared:  -0.0769 \nF-statistic: 8.11e-05 on 1 and 13 DF,  p-value: 0.993\n\n\n\nCall:\nlm(formula = y ~ x2, data = suppressor)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0900 -0.6334  0.0002  0.6146  1.0403 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10.632      0.811   13.11  7.2e-09 ***\nx2             0.195      0.113    1.74     0.11    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.75 on 13 degrees of freedom\nMultiple R-squared:  0.188, Adjusted R-squared:  0.126 \nF-statistic: 3.02 on 1 and 13 DF,  p-value: 0.106\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = suppressor)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.01363 -0.00945 -0.00228  0.00863  0.01632 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.51541    0.06114   -73.8   &lt;2e-16 ***\nx1           3.09701    0.01227   252.3   &lt;2e-16 ***\nx2           1.03186    0.00368   280.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0107 on 12 degrees of freedom\nMultiple R-squared:     1,  Adjusted R-squared:     1 \nF-statistic: 3.92e+04 on 2 and 12 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#ockhams-razor",
    "href": "slides/Chapter07.html#ockhams-razor",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Ockham’s razor",
    "text": "Ockham’s razor\nNunca ponenda est pluralitas sine necesitate\n(Plurality should never be posited without necessity)\n\nProblem: fit to sample always (*multi-level models can be counter-examples) improves as we add parameters.\nDangers of “stargazing”: selecting variables with low p-values (aka ‘lots of stars’). P-values are not designed to cope with over-/under-fitting."
  },
  {
    "objectID": "slides/Chapter07.html#overfitting",
    "href": "slides/Chapter07.html#overfitting",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Overfitting",
    "text": "Overfitting\n\nOverfitting learning too much from the data, where you are almost just connecting the points rather than estimating.\nUnderfitting is the opposite, i.e. being insensitive to the data.\naka Models with fewer assumptions are to be preferred.\nIn practice, we have to choose between models that differ both in accuracy and simplicity. The razor is not really a useful guidance for this trade off.\n\nWe need tools"
  },
  {
    "objectID": "slides/Chapter07.html#all-possible-models",
    "href": "slides/Chapter07.html#all-possible-models",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "All possible models:",
    "text": "All possible models:\nAn exhaustive screening of all possible regression models can also be done using software\n\nBest Subsets: Stop at each step and check whether predictors, in the model or outside, are the best combination for that step.\n\n\ntime consuming to perform when the predictor set is large\n\n\nRemember permutations\nFor example:\nIf we fix the number of predictors as 3, then 20 regression models are possible"
  },
  {
    "objectID": "slides/Chapter07.html#what-criteria-do-we-use",
    "href": "slides/Chapter07.html#what-criteria-do-we-use",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "What criteria do we use?",
    "text": "What criteria do we use?\nMultiple options\n- R squared\n- Sums of squared (different types, for testing predictor significance)\n- Information criteria (AIC, BIC etc)\n- For prediction: MSD/MSE, MAD, MAPE\n- \\(C_p\\)\n\nRemember its about balance and what you are looking for (fit vs prediction, complexity vs generality)\n\n\nNote\n\nIf a model stands out, it will perform well in terms of all summary measures.\nIf a model does not stand out, summary measures will contradict."
  },
  {
    "objectID": "slides/Chapter07.html#when-r2-becomes-absurd",
    "href": "slides/Chapter07.html#when-r2-becomes-absurd",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "When \\(R^2\\) becomes absurd",
    "text": "When \\(R^2\\) becomes absurd"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection-1",
    "href": "slides/Chapter07.html#model-selection-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model selection",
    "text": "Model selection\n\nResidual SD depends on its degrees of freedom\n\nSo comparison of models based on Residual SD is not fully fair\n\nThe following three measures are popular prediction modelling and similar to residual SD\nMean Squared Deviation (MSD): mean of the squared errors (i.e., deviations) (also called MSE)\n\n\\[\\frac{\\sum \\left({\\rm observation-fit}\\right)^{{\\rm 2}} }{{\\rm number~of~ observations}}\\]\n\nMean Absolute Deviation (MAD)\n\n\\[\\frac{\\sum \\left|{\\rm observation-fit}\\right| }{{\\rm number~of~observations}}\\]\n\nMean Absolute Percentage Error (MAPE)\n\n\\[\\frac{\\sum \\frac{\\left|{\\rm observation-fit}\\right|}{{\\rm observation}} }{{\\rm number~of~observations}} {\\times100}\\]"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection-continued",
    "href": "slides/Chapter07.html#model-selection-continued",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model selection (continued)",
    "text": "Model selection (continued)\n\nAvoid over-fitting.\nSo place a penalty for excessive model parameters\nAkaike Information Criterion (AIC; smaller is better)\n\n\\[AIC  =  n\\log \\left(\\frac{SSE}{n} \\right) + 2p\\] - Bayesian Information Criterion (BIC) places a higher penalty that depends on, the number of observations.\n\nAs a result BIC fares well for selecting a model that explains the relationships well while AIC fares well when selecting a model for prediction purposes.\nOther variations: WAIC, AICc, etc (we will not cover them)"
  },
  {
    "objectID": "slides/Chapter07.html#model-selection-continued-1",
    "href": "slides/Chapter07.html#model-selection-continued-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Model selection (continued)",
    "text": "Model selection (continued)\n\nWe can also benchmark a small model with the full regression\nMallow’s \\(C_p\\) (look for \\(C_p\\) just less than \\(p\\) or equal)\n\n\\[C_{p} =\\; \\frac{{\\rm SS\\; Error\\; for\\; Smaller\\; Model}}{{\\rm Mean\\; Square\\; Error\\; for\\; full\\; regression}} -(n-2p)\\] - If unimportant variables are added to the model, then the variance of the fitted values will increase. Similarly if important variables are added, then the bias of the fitted values will decrease\n\nThe \\(C_{p}\\) index, balances the variance and bias"
  },
  {
    "objectID": "slides/Chapter07.html#software",
    "href": "slides/Chapter07.html#software",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Software",
    "text": "Software\n\nIn \\(R\\), lm() and step() function will perform the tasks\n\nleaps() and HH packages contain additional functions\ndredge() in MuMIn will produce all the subset models given a full model\nAlso MASS, car, caret, and SignifReg R packages\n\nR base package step-wise selection is based on \\(AIC\\) only."
  },
  {
    "objectID": "slides/Chapter07.html#cross-validation",
    "href": "slides/Chapter07.html#cross-validation",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validation",
    "text": "Cross validation\nIn sample error vs prediction error\n\nFor simpler models, increasing the number of parameters improves the fit to the sample.\nBut it seems to reduce the accuracy of the out-of-sample predictions.\nMost accurate models trade off flexibility (complexity) and overfitting\n\nGeneral idea: - Leave out some observations. - Train the model on the remaining samples; score on those left out. - Average over many left-out sets to get the out-of-sample (future) accuracy."
  },
  {
    "objectID": "slides/Chapter07.html#cross-validated-selection-data-example",
    "href": "slides/Chapter07.html#cross-validated-selection-data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validated selection: Data example",
    "text": "Cross validated selection: Data example\nConsider the pinetree data set which contains the circumference measurements of pine trees at four positions (First is bottom)"
  },
  {
    "objectID": "slides/Chapter07.html#cross-validated-selection",
    "href": "slides/Chapter07.html#cross-validated-selection",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validated selection",
    "text": "Cross validated selection\n\nModel selection can be done focusing on prediction\n\nmethod = “leapForward” & method = “leapBackward” options\n\n\n\nlibrary(caret);  library(leaps)\nset.seed(123)\nfitControl  &lt;-  trainControl(method  =  \"repeatedcv\",\n                             number  =  5,  repeats  =  100)\nleapBackwardfit  &lt;-  train(Top  ~  .,  data  =  pinetree[, -1],\ntrControl  =  fitControl,  method  =  \"leapBackward\")\nsummary(leapBackwardfit)\n\nSubset selection object\n3 Variables  (and intercept)\n       Forced in Forced out\nThird      FALSE      FALSE\nSecond     FALSE      FALSE\nFirst      FALSE      FALSE\n1 subsets of each size up to 2\nSelection Algorithm: backward\n         Third Second First\n1  ( 1 ) \" \"   \" \"    \"*\"  \n2  ( 1 ) \"*\"   \" \"    \"*\""
  },
  {
    "objectID": "slides/Chapter07.html#polynomial-models",
    "href": "slides/Chapter07.html#polynomial-models",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nA polynomial model includes the square, cube of predictor variables as additional variables.\nHigh correlation (multicollinearity) between the predictor variables may be a problem in polynomial models, but not always."
  },
  {
    "objectID": "slides/Chapter07.html#polynomial-models-data-example",
    "href": "slides/Chapter07.html#polynomial-models-data-example",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Polynomial models: Data example",
    "text": "Polynomial models: Data example\nWe can fit a simple linear regression using the Pine tree data\n\npine1 &lt;- lm(Top ~ First, data = pinetree) \nsummary(pine1)\n\n\nCall:\nlm(formula = Top ~ First, data = pinetree)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.854 -0.881 -0.195  0.630  3.176 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -6.334      0.765   -8.28  2.1e-11 ***\nFirst          0.763      0.024   31.78  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.29 on 58 degrees of freedom\nMultiple R-squared:  0.946, Adjusted R-squared:  0.945 \nF-statistic: 1.01e+03 on 1 and 58 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#look-closer",
    "href": "slides/Chapter07.html#look-closer",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Look closer",
    "text": "Look closer\n\nLooks non-linear"
  },
  {
    "objectID": "slides/Chapter07.html#polynomial-models-data-example-1",
    "href": "slides/Chapter07.html#polynomial-models-data-example-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Polynomial models: Data example",
    "text": "Polynomial models: Data example\n\n\n                                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                         44.121      7.039    6.27        0\npoly(First, degree = 3, raw = T)1   -3.972      0.695   -5.71        0\npoly(First, degree = 3, raw = T)2    0.142      0.022    6.39        0\npoly(First, degree = 3, raw = T)3   -0.001      0.000   -5.95        0\n\n\n```         \n- For the pinetree example, all the slope coefficients are highly significant for the cubic regression\n- Not so for the quadratic regression\n```\n\n\n                                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                           3.85      2.450   1.569    0.122\npoly(First, degree = 2, raw = T)1     0.10      0.155   0.646    0.521\npoly(First, degree = 2, raw = T)2     0.01      0.002   4.319    0.000\n\n\n\nRaw polynomials do not preserve the coefficient estimates but orthogonal polynomials do.\n\n\n\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 17.40      0.146  119.26        0\npoly(First, degree = 2)1    41.01      1.130   36.29        0\npoly(First, degree = 2)2     4.88      1.130    4.32        0\n\n\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                 17.40      0.115  151.03        0\npoly(First, degree = 3)1    41.01      0.892   45.96        0\npoly(First, degree = 3)2     4.88      0.892    5.47        0\npoly(First, degree = 3)3    -5.31      0.892   -5.95        0"
  },
  {
    "objectID": "slides/Chapter07.html#residual-diagnostics",
    "href": "slides/Chapter07.html#residual-diagnostics",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Residual diagnostics",
    "text": "Residual diagnostics\n\nFor multiple regression fits, including polynomial fits, examine the residuals as usual to-\n- Validate the model assumptions\n- Look for model improvement clues\nQuadratic regression for pinetree data is not satisfactory based on the residual plots shown below:"
  },
  {
    "objectID": "slides/Chapter07.html#categorical-predictors",
    "href": "slides/Chapter07.html#categorical-predictors",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nModels can include categorical predictors such as Area in the pinetree dataset\nMake sure that you use the factor() function when numerical codes are assigned to categorical variables.\nArea effect on Top circumference is clear from the following plot"
  },
  {
    "objectID": "slides/Chapter07.html#indicator-variables",
    "href": "slides/Chapter07.html#indicator-variables",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nFactors are employed in a multiple regression using indicator variables which are simply binary variables taking either zero or one\nFor for males and females, indicator variables are defined as follows:\n\nIndicator variable of males: \\(~~~~~~~~\\begin{array}{cccc} I_{\\text {male}} & = & 1 & \\text{for males}\\\\ & & 0& \\text{for females} \\end{array}\\)\nIndicator variable of females \\(~~~~~~~~\\begin{array}{cccc} I_{\\text{female}} & = & 1 & \\text{for females}\\\\ & & 0& \\text{for males} \\end{array}\\)\n\nThere are three different areas of the forest in the pinetree dataset. So we can define three indicator variables.\nOnly two indicator variables are needed because there is only 2 degrees of freedom for the 3 areas."
  },
  {
    "objectID": "slides/Chapter07.html#regression-output",
    "href": "slides/Chapter07.html#regression-output",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Regression output",
    "text": "Regression output\n\n\n\nRegression of Top Circumference on Area Indicator Variables\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n20.02\n1.11\n17.98\n0.00\n\n\nI2\n-1.96\n1.57\n-1.24\n0.22\n\n\nI3\n-5.92\n1.57\n-3.76\n0.00\n\n\n\n\n\n\n\n\nThe y-intercept is the mean of the response for the omitted category\n\n20.02 is the mean Top circumference for the first Area\n\nslopes are the difference in the mean response\n\n-1.96 is the drop in the mean top circumference in Area 2 when compared to Area 1 (which is not a significant drop)\n-5.92 is the drop in the mean top circumference in Area 3 when compared to Area 1 (which is a highly significant drop)\n\n\nAnalysis of Covariance model employs both numerical and categorical predictors (covered later on).\n\nWe specifically include the interaction between them"
  },
  {
    "objectID": "slides/Chapter07.html#summary",
    "href": "slides/Chapter07.html#summary",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Summary",
    "text": "Summary\n\nRegression methods aim to fit a model by least squares to explain the variation in the dependent variable \\(Y\\) by fitting explanatory \\(X\\) variables.\nMatrix plots (EDA) and correlation coefficients provide important clues to the interrelationships.\nFor building a model, the additional variation explained is important. Summary criterion such as \\(AIC\\) is also useful.\nA model is not judged as the best purely on statistical grounds.\n\n\n\n\n\n\n\n\n\n161250 Data Analysis"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "161250 Slides",
    "section": "",
    "text": "Chapter 1:Data Collection\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2:Exploratory Data Analysis (EDA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3:Probability Concepts & Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4:Introduction to Statistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5:Tabulated Counts\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6:Models with a Single Predictor\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7:Models with Multiple Predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 8:Analysis of Variance (ANOVA) and Analysis of Covariance (ANCOVA)\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studyguide/2-eda.html",
    "href": "studyguide/2-eda.html",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "“Data has no meaning apart from its context.” — Walter A Shewhart"
  },
  {
    "objectID": "studyguide/2-eda.html#error-checking",
    "href": "studyguide/2-eda.html#error-checking",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Error checking",
    "text": "Error checking\nThe data are assumed to be in some sort of electronic form; e.g. text file, spreadsheet, database, etc. Before analysing the data, it is prudent to check for mistakes in the data. If one has access to the raw data (e.g. returned questionnaires), one can check observations by hand. If the raw data are not available, then the best one can do is to look for observations whose values are out of allowable ranges, or are very suspicious. A common problem occurs with the coding of missing information. Missing values are commonly recorded as a blank, *, -1, -999, or NA. Only one type of coding should be used. The use of NA is preferred since this is easier to spot. The use of blanks is particularly dangerous when one is exporting data from a spreadsheet to a text file. When the data from the text file is read into a computer package, a missing value may be skipped and the value from the next observation read in its place. When the data set is large, looking at a data set one observation at a time may not be feasible. In this case, plotting the data may show any observations with illegal values."
  },
  {
    "objectID": "studyguide/2-eda.html#understanding-data",
    "href": "studyguide/2-eda.html#understanding-data",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Understanding data",
    "text": "Understanding data\nAfter any data errors have been found and corrected, the data understanding stage commences. This stage involves tabulating, summarising, and plotting the data in many ways to gain insight into the data set. We typically want to:\n\nexamine the distributions, or shapes, of individual variables;\ncompare different groups of data, with special emphasis on location and scale;\nand discover any trends and relationships exhibited between pairs of variables.\n\nAgain, don’t fall into the trap of data snooping, where you develop and test hypotheses with the same data. For instance, suppose that boxplots of the income of individuals in different age groups show that youngest and oldest age groups are the most different. If one elects to perform the hypothesis test for the difference between these two groups based on observations made during EDA, the resulting \\(p\\)-value is not valid.\nIn the sections below, we briefly review many common tools used to visualise data during EDA."
  },
  {
    "objectID": "studyguide/2-eda.html#bar-chart",
    "href": "studyguide/2-eda.html#bar-chart",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Bar chart",
    "text": "Bar chart\nThe rules for constructing a bar chart are very simple and perhaps they can be reduced to a single rule and that is to make the height of each bar proportional to the quantity displayed. Consider the dataset Snodgrass available in the R package archdata. This dataset contains information on the size, location and contents of 91 house pits at the Snodgrass site which was occupied between about CE 1325-1420. The variable names and their description are given below:\n\nEast: East grid location of house in feet (excavation grid system)\nSouth: East grid location of house in feet (excavation grid system)\nLength: House length in feet\nWidth: House width in feet\n\nSegment: Three areas within the site 1, 2, 3\n\nInside: Location within or outside the “white wall” Inside, Outside\n\nArea: Area in square feet\n\nPoints: Number of projectile points\n\nAbraders: Number of abraders\n\nDiscs: Number of discs\n\nEarplugs: Number of earplugs\n\nEffigies: Number of effigies\n\nCeramics: Number of ceramics\n\nTotal: Total Number of artifacts listed above\n\nTypes: Number of kinds of artifacts listed above\n\nThe data from 91 house pits at the Snodgrass site were reported by Price and Giffin in 1979. The layout of the houses follows a grid pattern with the long axis oriented north-east surrounded by a fortification trench. There is also evidence of an interior wall that may have separated the houses inside that wall from those outside the wall. Price and Griffin use differences in house size and artifacts composition to suggest that those distinctions may have reflected rank differences between the occupants of the two areas.\nThe distribution of number of Ceramics found can be displayed in the form of a bar chart; see Figure 1.\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n\n\n\nCode\ndata(Snodgrass, package = \"archdata\")\n\nSnodgrass |&gt; \n  ggplot() + \n  aes(x = Ceramics) + \n  geom_bar()\n\n\n\n\n\nFigure 1: Bar Chart of Ceramics counts\n\n\n\n\nThe bar graph illustrates clearly that no ceramics were found in many houses but a lot were found in a few houses. This may be due to status and wealth of the occupants.\nNote that we are treating number of Ceramics as ordinal in the above graph. For nominal data software tend to order the categories alphabetically.\nHowever, a graph is not always the best way to display a small set of numbers. Graphs do have some disadvantages: they take up a lot of space, and they do not usually allow the recovery of the exact observations. For these reasons, particularly for a small data sets, a table is sometimes more effective. There were only three segment categories; and their counts can be simply displayed as a table.\n\n\nCode\ntable(Snodgrass$Segment)\n\n\n\n 1  2  3 \n38 28 25 \n\n\nIt would be interesting to see whether the discovery of ceramics is Segment dependent. In other words, we would like to explore two factors in display. This can be done in many ways (Figure 2).\n\n\nCode\ndata(Snodgrass, package = \"archdata\")\n\np1 &lt;- ggplot(Snodgrass) +\n  aes(x = Ceramics, fill = Segment) + \n  geom_bar() + \n  ggtitle(\"Bar plot with colour grouping\")\n\np2 &lt;- ggplot(Snodgrass) + \n  aes(x = Ceramics) + \n  geom_bar() + \n  facet_grid(vars(Segment)) + \n  ggtitle(\"Segment-wise Bar plots\")\n\np3 &lt;- ggplot(Snodgrass) +\n  aes(x = Ceramics, fill = Segment) +\n  geom_bar(position = \"dodge\") + \n  ggtitle(\"Clustered Barplot\")\n\np4 &lt;- ggplot(Snodgrass) +\n  aes(x=Ceramics, fill = Segment) + \n  geom_bar(position = \"dodge\") + \n  ggtitle(\"Clustered Bar plot - flipped\") +\n  scale_fill_grey() + coord_flip()\n\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol=2)\n\n\n\n\n\nFigure 2: Bar Chart of Ceramics counts by Segment\n\n\n\n\nIt is easy to spot that the Ceramics findings were largely from the first segment. This EDA conclusion can be drawn from all of the above plots. However, these bar plots do not rate the same as a “presentation style graphic”.\nTufte (2001) argues in favour of certain principles of graphical integrity for presentation style graphics. These are:\n\nThe representation of numbers in a graph should be directly proportional to the numbers themselves;\nClear and detailed labels should be used to prevent ambiguity;\nProvide explanations and important events pertaining to data on the graphic itself;\nThe graph should display data variation, not design variation—visual changes in the graph should be due to the data, not changes in way the data are drawn;\nThe number of dimensions used in the graph should not exceed the number of dimensions in the data;\nGraphs must not quote data out of context; for example, a monthly drop in the murder rate of a city should be plotted within the context of the murder rate over the last few years, or murder rates in comparable cities over the same period.\n\nA final point concerns the use of colour. In many sorts of graphs, data types are differentiated by the use of colour. This practice is perfectly legitimate, but one should be aware that a surprising proportion of people have impaired colour perception, such as being unable to distinguish between red and green (see here for colourblind-friendly pallettes). A remedy to these problems is to use different plotting symbols, line types, or shading, in conjunction with different colours. One should always make sure that the colours selected are clearly visible in the medium in which they are presented. Colours on a computer monitor can appear quite different when projected onto a screen in a lecture theatre, or when printed onto paper.\nAccording to Tufte (2001), a good graphical display should:\n\nDisplay the data;\nEncourage the viewer to concentrate on the substance of the data, in the context of the problem, not on the artistic merits of the graph;\nAvoid distorting, intentionally or unintentionally, what the data have to say;\nHave a high data to ink ratio; that is, convey the greatest amount of information in the shortest time with the least ink in the smallest space;\nEnable one to easily compare different pieces of data;\nReveal overall trends and patterns in the data that are not obvious from viewing the raw numbers;\nReveal detailed information secondary to the main trend; for example, the location of outliers, variability of the response.\n\nThis Chapter is largely concerned with EDA for discovery of patterns and peculiarities in a dataset. So we will not be too worried if the graphs produced are not aesthetically pleasing or does not meet a particular standard for a presentation style graph.\nThe end goal of most analyses is of course some form of written report or presentation. In either case, the findings of the study, should be supported by tables and graphs created during the EDA; these should clarify and support the final conclusions, not deceive or confuse the audience. One should refrain from showing all tables and plots that were created, but rather a careful selection that reinforces the main findings. The remaining figures can, if necessary, be placed in an appendix of the written report. The figures and tables that are shown should be well labelled and annotated. Abbreviations and cryptic variable names should be replaced by sensible labels and descriptions. The emphasis should be on conveying information rather than producing an eye catching plot. Special care should be taken to ensure that the table or graph cannot be easily misinterpreted.\nIn addition, a poorly made graph can be deceptive. This is a problem both in the understanding the data phase, and the presenting the data phase. One common mistake (but by no means the only one) in creating a graph is to use more dimensions to represent the data than there are data dimensions. For example, it is not uncommon to see a bar chart where the bars are drawn as three dimensional rectangular prisms. The false perspective actually obscures the differences in the heights of the bars.\nAnother pitfall is to use a clever graphic rather than one that clearly presents the data. The classic example of this is a pictogram, where pictures of varying size represent some quantities. Pictograms are often drawn dishonestly: to show a two-fold difference, each side of the picture is increased by a factor of two, so the area is in fact increased by a factor of four. Dishonesty in data presentation is clearly unacceptable. But even an honest pictogram is usually not the best way to present data. It is difficult to accurately judge differences between areas; which is what a pictogram asks the viewer to do."
  },
  {
    "objectID": "studyguide/2-eda.html#pie-charts",
    "href": "studyguide/2-eda.html#pie-charts",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Pie charts",
    "text": "Pie charts\nPie charts are a popular, though usually rather poor, way of presenting data. Like bar charts, they frequently use a large figure to represent just a few numbers. Pie charts also have a poor accuracy of decoding. That means people’s impression of the difference in size between two pie slices is often quite different from the actual difference in size. Pie charts use angles to represent percentages; people tend to underestimate the size of acute angles and overestimate the size of obtuse angles (Stevens, 1957). The exploded pie chart makes it more difficult to accurately compare pie slices: the exploded piece always appears bigger. The three dimensional pie chart is even worse. Bar charts, dot charts, or tables should be preferred to pie charts.\nCount summaries of two categorical variables can be displayed in the form of a mosaic plot.\n\n\nCode\ntable(Snodgrass$Inside, Snodgrass$Earplugs)\n\n\n         \n           0  1  2  3  4\n  Inside  21 10  5  1  1\n  Outside 50  2  0  1  0\n\n\nFigure 3 shows the above tabulated counts graphically. Evidently, greater number of earplugs were found outside than inside.\n\n\nCode\ntab &lt;- table(Snodgrass$Inside, Snodgrass$Earplugs)\nmosaicplot(tab, main=\"Inside*Earplugs counts\", ylab=\"Earplugs\")\n\n\n\n\n\nFigure 3: A mosaic plot\n\n\n\n\nThe R package vcd can produce many other types of plots for categorical variables but these plots require an understanding of models fitted to categorical data. Figure 4 displays the counts in a different way (and ignore the P value displayed in the plot). The test of association topic is covered later on.\n\n\nCode\nlibrary(vcd)\n\n\nLoading required package: grid\n\n\nCode\nassoc(tab, shade=TRUE, legend=TRUE) \n\n\n\n\n\nFigure 4: Association plot\n\n\n\n\nSpine plots are a special cases of mosaic plots that mimics stacked bar plots; Try-.\n\n\nCode\nspine(tab) \n\n\n\n\n\nFigure 5: Spine plot\n\n\n\n\nA doubledecker plot is used to handle many factors. These plots become harder to interpret when the number of categorical factor levels becomes large; Try-\n\n\nCode\ndoubledecker(Effigies~Inside+Segment,  data=Snodgrass)\n\n\n\n\n\nFigure 6: Doubledecker plot"
  },
  {
    "objectID": "studyguide/2-eda.html#dotplot",
    "href": "studyguide/2-eda.html#dotplot",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Dotplot",
    "text": "Dotplot\nOne of the simplest kinds of graph for a numerical variable is the single line (or one dimensional) graph. Each observation is represented by a point (or a star * or a dot .) on the line. These graphs can be useful to see the rough distribution and gaps, if any, in the data. Figure 7 shows a dotplot of Length of the house pits for the Inside and Outside groupings. Evidently privileged tribes lived inside in general, and the length of their dwellings is longer.\n\n\nCode\nggplot(Snodgrass) +\n  aes(x=Inside, y=Length) + \n  geom_dotplot(binaxis='y', dotsize=.5) + \n  coord_flip()\n\n\n\n\n\nFigure 7: A typical dot plot for two groups\n\n\n\n\nOn reflection, the one dimensional dotplot gives rise to a second dimension wherever observations are identical or nearly the same. In a dotplot such points are usually stacked. As our eyes tend to pick up trends very easily, they tend to be distracted by the vertical changes. The main problem here is that some of the values which coincide by chance may be taken to represent real increases in density. To overcome this we try ‘jiggling’ or jittering the data, which is equivalent to plotting a random number on the vertical axis against the data values on the horizontal axis. It is suggested that the vertical jiggle be restricted to a small range so that the overall effect is that of a one-dimensional spread with greater, or lesser, density of points along the graph. Figure 8 shows a jiggled dotplot. You can decide for yourself which strategy is better.\nClearly, the use of jiggling is much more relevant for larger sets of data. Notice that with jiggling we are interested in the relative density of the graph in different places rather than in trends.\nAs the vertical reading is determined at random our eyes may be distracted by a seeming trend which is mainly due to this randomness. For this reason, it is helpful to try more than one plot with different jiggles. This is in the spirit of EDA, in which data is viewed from as many angles as is feasible. The other side of this coin is that conclusions from one display of data should be treated with some scepticism. The availability of computers allows data to be plotted, tabulated and transformed quickly, so we should look at it several ways to better appreciate the peculiarities inherent in our data.\nIn practice, one tends to avoid jiggling except in (two-variable) scatter plots where there are many repeated \\((x, y)\\) points. Graphs with a jiggle are mainly needed when the data are discrete or grouped in some way, or there are a large number of points to be plotted.\n\n\n\n\n\nFigure 8: A jittered dot plot"
  },
  {
    "objectID": "studyguide/2-eda.html#histograms",
    "href": "studyguide/2-eda.html#histograms",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Histograms",
    "text": "Histograms\nThe histogram is a standard method of showing the distribution of a quantitative variable. A histogram is made by dividing the range of a variable into “bins” and counting the number of data points that fall within each bin. It is like a bar chart after forcing a quantitative variable into bins. It can be used for discrete ungrouped data, but a bar chart is generally more suitable for discrete data because histograms can mislead the reader into thinking that values exist other than at the centre of the intervals.\nFigure 9 shows the histogram of Length. Clearly the distribution is far from symmetric and left skewed. The bimodal pattern also suggests subgrouping.\n\n\nCode\nggplot(Snodgrass) + \n  aes(x = Length) + \n  geom_histogram(bins=12, \n                 color=\"black\", \n                 fill=\"white\")\n\n\n\n\n\nFigure 9: A typical histogram\n\n\n\n\nNotice that a different shape of histogram may result depending on the class intervals (bins) used (that is by choosing different class widths or different midpoints of the classes). This suggests that it may be wise to draw more than one histogram for a given data set particularly if statements are to be made about modes or gaps in the data. The guideline for the maximum number of intervals, \\(L\\), can follow the same formula as for stem-and-leaf displays. Some software use the Sturges (1926) formula namely \\[\\texttt{bin width} = \\frac {\\texttt{range}(x)} {\\log_2(n)+1}.\\] For fixing the width of the bins, the range of the data is divided by the sum of one and the base two logarithm of the sample size.\nThere are a few ways to scale the y-axis of a histogram, as shown in Figure 10:\n\nFrequency histograms show the counts in each bin; the heights of the bars sum to \\(n\\).\nRelative frequency histograms show the proportions or percent of data in each bin—the counts in each bin divided by the total; the heights sum to 1 or 100%.\nDensity histograms show the densities; the areas (heights \\(\\times\\) widths) of the bars sum to 1.\n\n\n\nCode\np1 &lt;-  Snodgrass |&gt; \n  ggplot() + \n  aes(x = Length) +\n  geom_histogram(bins = 20) +\n  ylab(\"\") + \n  ggtitle(\"(a) Frequency histogram\", \n          \"Heights of the bars sum to n\")\n\np2 &lt;-  Snodgrass |&gt; \n  ggplot() + \n  aes(x = Length) +\n  geom_histogram(\n    bins = 20, \n    mapping = aes(y = after_stat(count / sum(count)))\n    ) + \n  ylab(\"\") +\n  ggtitle(\"(b) Relative frequency histogram\",\n          \"Heights sum to 100%\") + \n  scale_y_continuous(labels = scales::percent)\n\np3 &lt;-  Snodgrass |&gt; \n  ggplot() + \n  aes(x = Length) +\n  geom_histogram(\n    bins = 20, \n    mapping = aes(y = after_stat(density))) +\n  ylab(\"\") + \n  ggtitle(\"(c) Density histogram\",\n          \"Heights x widths sum to 1\")\n\nlibrary(patchwork)\n\np1 + p2 + p3\n\n\n\n\n\nFigure 10: Three types of histograms\n\n\n\n\nIf, instead of drawing blocks over the class intervals as in a histogram, we join the mid points of the tops of the bars, we obtain a display known as a frequency polygon . If we use relative frequencies, it is a relative frequency polygon; see Figure 11.\n\n\nCode\nggplot(Snodgrass) + \n  aes(x = Length, y = after_stat(density)) +\n  geom_freqpoly( bins = 12, color = \"black\")\n\n\n\n\n\nFigure 11: A typical frequency polygon\n\n\n\n\nFigure 12 shows a kernel density plot, which is a method that produces a smoother density line.\n\n\nCode\nSnodgrass |&gt; \n  ggplot() + \n  aes(Length) + \n  geom_density() + \n  geom_rug(alpha = .3, colour = \"darkorange2\")\n\n\n\n\n\nFigure 12: A typical smoothed density curve\n\n\n\n\nKernel Density Estimation (KDE) is generated by dropping a little mini-distribution, usually a normal curve, on each individual point and summing them up (Figure 13).\n\n\n\nFigure 13: A demonstration of kernel density estimation\n\n\nThese smooth plots are also useful to see the type of skewness involved, multimodality (several peaks) and unusual observations (or outliers). The disadvantage is that we may over-smooth and mask some of the subgrouping patterns; see Figure 14. Even though the two distributions appear to be similar, the length distribution for the Inside group has a larger mean/median etc (called the location parameters).\n\n\nCode\nSnodgrass |&gt; \n  ggplot() + \n  aes(x = Length, \n      colour = Inside) + \n  geom_density()\n\n\n\n\n\nFigure 14: Identification of Subgrouping"
  },
  {
    "objectID": "studyguide/2-eda.html#theoretical-distributions",
    "href": "studyguide/2-eda.html#theoretical-distributions",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Theoretical Distributions",
    "text": "Theoretical Distributions\nA histogram gives a general picture of the frequencies or relative frequencies of the data. If an observation is taken at random from the population from which the sample was drawn, the chance of it falling in any one of the class intervals should be approximately proportional to the area of the block over that interval (not exactly proportional because the histogram was only based on a sample from the population, not the population itself).\nIf we could draw a histogram for the whole population, then the probabilities would be exact. The relative frequencies form a probability distribution and if the class intervals are small enough the tops of the bars of the histogram would often approximate a smooth curve such as the density plot. In many cases this smooth curve is almost symmetrical with a rounded peak at the centre. One particular shape, fitting a certain mathematical formula, is a very good approximation to many sets of data. For instance, heights of people, weights of animals or yields of wheat all follow this distribution fairly well (although theoretically the distribution has an infinite range of possible values). One of the widely used theoretical distribution is called the Normal or Gaussian distribution but we will not bore you with its formula. However, most software can compute the tail areas (probabilities) under theoretical distributions. We shall study this distribution in greater detail in a Chapter 3. For the moment let us note that our EDA techniques should provide us with information about the parameters of the distribution, and we also want graphical techniques that display the continuous nature of the data.\nDiscrete data also have probability distributions and the bar graph gives a general picture of its shape. One commonly occurring discrete theoretical distribution is known as the Poisson distribution and approximates reasonably well the numbers of road accidents over a fixed time period. The main feature of a Poisson distribution is that there is a peak at the nearest integer below or equal to the average, and then a steady falling off. The mean is a parameter of the model, so we want our exploratory techniques to tell us about the mean, and to help us decide whether or not the Poisson model is appropriate for a given set of data. Theoretically, possible values are any whole positive numbers, but large numbers of accidents are very unlikely. Again, any graphical method we use must properly display the discrete nature of Poisson data. The other commonly adopted discrete distribution is the binomial distribution. This distribution is valid when the individual outcomes are classified into two categories such as Success and Failure."
  },
  {
    "objectID": "studyguide/2-eda.html#symmetry-plots",
    "href": "studyguide/2-eda.html#symmetry-plots",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Symmetry plots",
    "text": "Symmetry plots\nA symmetry plot is based on the distance of left and right side points. If the underlying distribution is symmetric, the points on the left and right side would be roughly equidistant from the median. For the length data, we obtain the symmetry plot after writing a little R function as follows:\n\n\nCode\nsymmetryplot &lt;- function(x)\n{\n  require(ggplot2)\n  \n  x &lt;- na.omit(x)\n  m &lt;- length(x) %/% 2\n  sx &lt;- sort(x)\n  dfm &lt;- tibble( \n    X = median(x) - sx[1:m], \n    Y = rev(sx)[1:m] - median(x)\n    )\n  \n  dfm |&gt; \n    ggplot() + \n    geom_point(mapping = aes(x=X, y=Y)) + \n    geom_line(mapping = aes(x=X, y=X)) + \n    theme(aspect.ratio=1) + \n    xlab(\"Distance below median\") + \n    ylab(\"Distance above median\")\n}\n\n\n\n\nCode\nsymmetryplot(Snodgrass$Length)\n\n\n\n\n\nThe points do not follow the \\(45^\\circ\\) line where \\(Y=X\\); divergence from this line indicates a lack of symmetry.\nIf each half of the distribution is further divided in halves, we obtain the fourths or( hinges), as the medians for the lower and upper halves. The end points, the minimum and the maximum are called the lower and upper extreme values, respectively.\nThe R function obtains the set (minimum, lower hinge, median, upper hinge, maximum) in its function fivenum.\n\n\nCode\n# Five number summary\nfivenum(Snodgrass$Length)\n\n\n[1]  4.00 13.00 14.50 17.75 21.00"
  },
  {
    "objectID": "studyguide/2-eda.html#box-plots",
    "href": "studyguide/2-eda.html#box-plots",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Box plots",
    "text": "Box plots\nA boxplot (or sometimes a box-and-whisker plot) is a visual display of the five-number summary, which can be used to assess the skew in the data. The components of a boxplot are explained in Figure 15.\n\n\n\n\n\nFigure 15: Annotated boxplot\n\n\n\n\nThe box shows the positions of the first and third quartiles, or “hinges”, and the thick middle line shows the median. As 50% of the observations fall between the first and third quartiles, the position of the box is important and is the first thing that catches our eyes. If the median is in the centre of the box, this suggests that the distribution may be symmetric; if the median is near one end, the distribution is skewed. In particular, if the median is in the right side of the box, it implies that the middle 50% of the data are skewed to the left. For right skewed data, the median will be placed at the left end of the box (as against in the centre of the box).\nAfter having considered the ‘centre’ of a distribution, which we have measured by the median, and looked at where the bulk of the data lies, measured by the quartiles; then we are particularly interested in the strange, unusual values. They could be obvious mistakes (incorrectly coded, or decimal places in the wrong place, transposition of digits such as 91 instead of 19) or incorrect measurements or recording. These last two possibilities often occur before the data gets to the statistician so that it may be difficult to check whether these mistakes have occurred. Apart from obvious errors, we can only point to ‘far out’ values as being strange and we term them ‘outliers’. To help decide this matter, we define a few more terms, of which the most common one is the interquartile range or IQR, which is the distance between the first and third quartiles ( \\(Q3-Q1\\) ). Any observation which is more than \\(1.5 \\times IQR\\) (sometimes called the “step”) less than the first or more than the third quartile can be considered a possible outlier (Figure 15.\nBoxplots are ideal for comparing a few groups of data because they give a visual summary of certain main features of the distributions but they are not as cluttered as other displays. Common sense must prevail, for there is a limit to the amount of information that the eye can take in. Up to 10 or 12 boxplots can be plotted, preferably on the same scale; more than 12 could be confusing.\nShown in Figure 16 are the boxplots of Length for the Inside-Outside groupings. Note that the boxplots have been constructed on the same scale; that is, they have common (Length) axis.\n\n\nCode\nggplot(Snodgrass) + \n  aes(y = Length, \n      x = Inside, \n      col = Inside) + \n  geom_boxplot() + \n  coord_flip()\n\n\n\n\n\nFigure 16: Boxplots\n\n\n\n\nThe boxplots in Figure 16 show that the general shapes of the distributions of Length are broadly similar. In each case, the median is near the centre of the box. The box of the Outside group is entirely to the right of the box of the Inside group, indicating that the distribution of Length is higher for the Outside group. We can also see that the Inside group may have greater spread than the Outside group."
  },
  {
    "objectID": "studyguide/2-eda.html#letter-value-plots",
    "href": "studyguide/2-eda.html#letter-value-plots",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Letter-value plots",
    "text": "Letter-value plots\nFor large datasets, a summary based on five numbers may be overly simple, showing too many ‘outliers’ and obscuring some structure in the more extreme range of the data. Hofmann, Wickham, and Kafadar (2017) describe an extension to the boxplot called the “letter-value plot”.\nLike boxplots, letter-value plots include the median and the two middle fourths of the data. However, the whiskers are replaced by smaller boxes, representing ever-decreasing proportions of the data, such as eighths, sixteenths, etc., represented by letter values. The extent of this extra boxing depends on the quantity and shape of the data.\nLetter values are defined by their depths, or distances from the nearer edge of the ordered data.\nmedian: found at depth \\(d(M) = (n + 1)/2\\) where \\(n\\) is the number of observations\nfourths: found at depth \\(d(F) = \\{\\verb\"int\"[d(M)] + 1\\}/2\\) where \\(\\verb\"int[]\"\\) refers to the integer part function.\neighths: found at depth \\(d(E) = \\{\\verb\"int\"[d(F)] + 1\\}/2\\)\nSimilarly if we had enough data, we could extend further\nsixteenths: found at depth \\(d(D) = \\{\\verb\"int\"[d(E)] + 1\\}/2\\)\nthirty-seconds: found at depth \\(d(C) = \\{\\verb\"int\"[d(D)] + 1\\}/2\\)\nand thus define points \\(B, A, Z, Y, X,\\) and so on.\nNote the M, F and E could be termed measures of location. It is also helpful to have some measures of spread which indicate the variability in the distribution. One such measure, the F-spread, is the distance between the two F (fourths) values; that is, the inter-quartile range (IQR). Finally, the range is the distance between the two extreme points.\nAnother useful measure of location is obtained by averaging the two hinges which is termed the mid-fourth (or mid-F). If the distribution were perfectly symmetrical, the mid-F would be the same as the median. If mid-F is smaller (larger) than the median, the distribution is skewed to the left (right). Note that the mid values can be found at any depths.\nSee here for more details.\nLetter values are displayed as a graph called letter value plot; see Figure 17. This type of plot is a better display than a boxplot for big data which tend to show many unusual points. Boxplots are suitable to moderate size datasets only. For large datasets, the skew may not be consistent within the data. The LV plot will show such anomalies in the skew patterns.\n\n\nCode\nlibrary(lvplot)\nLVboxplot(Snodgrass$Length, xlab = \"Length\")\n\n\n\n\n\nFigure 17: Letter-value plot for length data\n\n\n\n\nA letter value plot showing the eighths is shown in Figure 18. Note that this LV plot does not have a whisker, but the points falling outside the eighths are tagged.\n\n\nCode\nLVboxplot(Snodgrass$Length ~ Snodgrass$Inside, xlab = \"Length\")\n\n\n\n\n\nFigure 18: Letter-value plots of Length by Inside\n\n\n\n\nIf we compare Figure 16 with Figure 18, we can see that the whiskers have been replaced with more ‘outliers’ and two extra boxes. The middle boxes are the same; they show the first and third quartiles. The smaller, darker boxes in Figure 18 show the first and seventh eighths.\nFigure 19 shows a ggplot version of an LV plot for a big random data set.\n\n\nCode\nset.seed(123)\n\nlibrary(lvplot)\n\ndata.frame(\n  y = c( rlnorm(n=1e4, mean = 10) , \n         rnorm(n=1e4, mean = 5e5, sd = 1e5) \n         ),\n  x = rep(c(\"Log-normal(10,1)\",\"Normal(500k,100k)\"), each=1e4) |&gt; factor()\n) |&gt; \n  ggplot() + \n  aes(x=x, y=y, fill=after_stat(LV)) +\n  geom_lv() + \n  scale_fill_lv() +\n  coord_flip() +\n  theme_bw() +\n  xlab(\"\")\n\n\n\n\n\nFigure 19: A ggplot letter-value plot"
  },
  {
    "objectID": "studyguide/2-eda.html#quantile-quantile-q-q-plot",
    "href": "studyguide/2-eda.html#quantile-quantile-q-q-plot",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Quantile-Quantile (Q-Q) Plot",
    "text": "Quantile-Quantile (Q-Q) Plot\nAt times, two groups may have different numbers of observations but they may be similar in some ways and it may be appropriate to compare them. This comparison can be done in several ways but Q-Q plot is once such graph useful for such a comparison. The quantiles of the first batch is plotted against the quantiles of the second batch. A 45 degree \\((Y=X)\\) line is also drawn. If the points on a quantile-quantile plot fall along, or close to, the 45 degree line, then the two variables follow the same distribution; if they fall on a line parallel to the 45 degree line, the two variables follow similar distributions but one has a larger location parameter (say mean, or median) than the other; if the slope differs from the 45 degree line the variables have a similar distribution but have different scale parameters (say, standard deviations or F-spreads); if the plot is linear then the distributions of \\(Y\\) and \\(X\\) are different.\n\n\nCode\nset.seed(12344)\nX &lt;- rnorm(100)\nY &lt;- rnorm(100)\nnq &lt;- 201\n\np &lt;- seq(1 , nq, length.out = 50) / nq - 0.5 / nq\n\np1 &lt;- ggplot() + \n  aes(x = quantile(X, p), y = quantile(Y, p)) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"X & Y roughly follow\\nthe same distributions\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\nX2 &lt;- rnorm(100)\nY2 &lt;- rnorm(100, 1)\n\np2 &lt;- ggplot() + \n  aes(x = quantile(X2, p), \n      y = quantile(Y2, p)\n      ) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"The distribution of Y has\\na larger location (mean/median)\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\nX3 &lt;- rnorm(100, 0, 1)\nY3 &lt;- rnorm(100, 0, 2)\n\np3 &lt;- ggplot() + \n  aes(x = quantile(X3, p), \n      y = quantile(Y3, p)\n      ) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"The distribution of Y has\\na larger scale (SD, F-spread etc)\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\nX4 &lt;- rbeta(100, 1, 4)\nY4 &lt;- rnorm(100, 0, 2)\n\np4 &lt;- ggplot() + \n  aes(x = quantile(X4, p), y = quantile(Y4, p)) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  ggtitle(\"The distributions of  X and Y \\nare different\") + \n  xlab(\"X quantiles\") + ylab(\"Y quantiles\")\n\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol=2)\n\n\n\n\n\nFigure 22: Quantile-Quantile plot patterns\n\n\n\n\nIn Figure 22, different kinds of quantile-quantile plots are shown; note the following three cases in particular.\n\n\\(Y\\) and \\(X\\) follow similar distributions but the quantiles of \\(Y\\) are a constant amount greater than the quantiles of \\(X\\).\n\\(Y\\) and \\(X\\) follow similar distributions but the standard deviation of \\(Y\\) is greater than that of \\(X\\).\n\\(Y\\) and \\(X\\) follow different distributions.\n\nFigure 23 gives Q-Q plot comparing the Length of house pits for Inside and Outside groups. This Q-Q plot suggests that the two distributions are similar but the average Length is greater for the Inside dwellings.\n\n\nCode\nnq &lt;- length(Snodgrass$Length)\np &lt;- seq(1 , nq, length.out = 20) / nq - 0.5 / nq\n\nX &lt;- Snodgrass |&gt; \n  filter(Inside == \"Inside\") |&gt; \n  pull(Length)\n\nY &lt;- Snodgrass |&gt; \n  filter(Inside == \"Outside\") |&gt; \n  pull(Length)\n\nggplot() + \n  aes(x = quantile(X, p), \n      y = quantile(Y, p)\n      ) + \n  geom_point() + \n  geom_abline(slope=1, intercept=0) + \n  xlab(\"Length quantiles for Inside Group\") + \n  ylab(\"Length quantiles for Outside Group\")\n\n\n\n\n\nFigure 23: Quantile-Quantile plot of Length for Inside-Outside groups\n\n\n\n\nNote that the distribution of two variables \\(X\\) and \\(Y\\) can be compared in a number of ways using boxplots, overlaid ECDF plots etc."
  },
  {
    "objectID": "studyguide/2-eda.html#exploring-relationships",
    "href": "studyguide/2-eda.html#exploring-relationships",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Exploring relationships",
    "text": "Exploring relationships\nConsider data which can be written as pairs (\\(X\\), \\(Y\\)). The two groups of data (\\(X\\) and \\(Y\\)) need not be in the same units but they are assumed to be related in some way. Scatterplots are useful in exploring the type of relationship between \\(X\\) and \\(Y\\).\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/testmarks.RData\", \n  destfile = \"testmarks.RData\")\n\nload(\"testmarks.RData\")\n\n\n\n\nCode\nggplot(testmarks) +\n  aes(y=English, x=Maths) + \n  geom_point() + \n  coord_equal()\n\n\n\n\n\nFigure 26: Scatter plot of English vs Maths scores\n\n\n\n\nThe data in testmarks.txt consists of the records of 40 students, at a school, on standard tests of comprehension of English and Mathematics. The test marks data can be graphed plotting English test scores against Mathematics test scores. The resulting graph, known as the scatterplot, is shown as Figure 26. This plot enables us to see if there is a relationship between \\(X\\) and \\(Y\\).\nHere, the scatterplot shows a clear upward trend – as the Mathematics mark increases so does the English mark, in general.\nThere are a number of useful features that can be added to a scatterplot. The individual distributions of \\(X\\) and \\(Y\\) can be shown by adding appropriate graphical displays (e.g. boxplots) along the axes. If a number of the students have the same marks then it may be advisable to add a jitter as we discussed earlier.\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- ggplot(testmarks) + \n  aes(y=English, x=Maths) +\n  geom_point() + \n  coord_equal()\n\nggMarginal(p1, type=\"boxplot\")\n\n\n\n\n\nFigure 27: English vs Maths scores\n\n\n\n\nIn Figure 27, it can be seen (with some effort) that each set of scores seems to be uniformly distributed along its axis, although the Maths marks exhibit gaps which may indicate different groups within the class.\nFinally, does the variability in English marks change as the Maths mark changes, or is it fairly constant, i.e. does the vertical height of the scatter change from left to right? In this case the variability appears to be fairly constant (we will see why this is important later).\n\n\nCode\nggplot(rangitikei) +\n  aes(y = people, x = vehicle, col = loc, shape = loc) + \n  geom_point()\n\n\n\n\n\nFigure 28: Scatterplot with a grouping variable\n\n\n\n\nIf the data is known to be grouped, then this can be shown on the scatterplot by using different symbols for each group. The data file Rangitikei.txt gives the number of people making recreational use of the Rangitikei River and the number of vehicles seen at a particular location and time. The data were collected at two locations (location being a categorical variable). The effect of location, if any, on the number of people and vehicles can be seen on a scatterplot if we identify each plotted point with a different symbol (or colour) depending on the location (see Figure 28). There appears to be little effect of location on the number of people and vehicles seen.\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- rangitikei |&gt; \n  ggplot() + \n  aes(y=people, x=vehicle) + \n  geom_point() +\n  ggtitle(\"(A): All of the data\")\n\np2 &lt;- rangitikei |&gt; \n  # remove the rows with highest value of 'people'\n  filter(people &lt; max(people)) |&gt; \n  ggplot() + \n  aes(y=people, x=vehicle) + \n  geom_point() + \n  ggtitle(\"(B): Without observation #26\")\n\ngridExtra::grid.arrange(\n  ggMarginal(p1, type=\"boxplot\"), \n  ggMarginal(p2, type=\"boxplot\"), \n  ncol=1)\n\n\n\n\n\nFigure 29: People vs vehicle plots\n\n\n\n\nFigure 30 shows the number of people against the number of vehicles seen as well as the distribution of each variable along the axes using boxplots (these are often referred to as marginal distributions). The first plot in Figure 30 (A) shows the scatter of all points, whereas the second one (B) shows the scatter with the very large value (Observation #26) removed. In the first plot the outlier shows up quite clearly in both boxplots. In the second plot the boxplot associated with the variable ‘people’ still shows one point as an outlier, though this doesn’t appear to be as extreme as the one in the first plot, and in the scatterplot the points fall close to a line which has a positive slope as is the case of Figure 30 (A). However, the distribution of each variable is skewed to the right.\nNote that the case which produced the outlier in both the marginal distributions did not produce a scatterplot value that was in any way unusual. While this might not surprise us (the relationship between people and vehicles might be expected to stay the same even for a location with large numbers of each), it follows that an outlier with respect to the underlying relationship between two variables will be different from an outlier in the marginal distribution sense. And such a value may not be an outlier in either marginal distribution. Look again at Figure 27 - there is one student with a high Maths mark but a low English mark. While neither mark is unusual in itself, the point stands out from the rest of the scatterplot because it is unusually high or low (i.e. up or down) on the plot. In general an outlier with respect to the relationship between \\(X\\) and \\(Y\\) is one whose \\(Y\\) value is unusually large or small compared to other cases with similar \\(X\\) values.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/horsehearts.RData\",\n  destfile = \"horsehearts.RData\")\n\nload(\"horsehearts.RData\")\n\n\n\n\nCode\nlibrary(ggExtra)\n\np1 &lt;- ggplot(horsehearts) + \n  aes(y=WEIGHT, x=EXTDIA) + \n  geom_point()\n\np2 &lt;- p1 + geom_smooth(se=FALSE)\n\ngridExtra::grid.arrange(\n  ggMarginal(p1, type=\"density\"), \n  ggMarginal(p2, type=\"density\"), \n  ncol=1)\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nFigure 30: Weight vs Exterior Widths\n\n\n\n\nAnother example of a scatterplot is shown in Figure 30 which displays the weight of horses’ hearts against their exterior widths in the systole phase (that is the contracted phase)(data set horsehearts.txt). The scatterplot indicates that a straight line would not fit the data well. On the other hand, there is some evidence the weights fall into one of two subgroups (with a value around 2.5kg being the cut off point). Scatterplots are particularly useful in pinpointing possible groupings of points so that the structure of the data is revealed which might otherwise go undetected in descriptive statistics calculated for the full data set.\nOne way of quickly assessing whether a straight line fit is reasonable is to smooth the points in the plot using a LOWESS (LOcally WEighted Scatterplot Smoother) smoothed line. The lowess (also called loess) procedure is based on local sets of points (instead of the whole data set) at a given \\(x\\) range, and fits a smoothed line to the data. The second scatterplot in Figure 30 shows the lowess smoother which confirms that there is curvature in the underlying relationship- possibly a cubic curve would fit this.\n\nScatterplots and Correlation Coefficients\nWe will use the data set horsehearts.txt of the weights of horses’ hearts in relation to other heart measurements to illustrate the measurement of correlation. As we have seen before, scatterplots provide a good starting point for exploring the type of relationship between two variables. In Figure 31, the weight, \\(Y\\), is plotted against the exterior width \\(X\\) (diastole phase in column 7 of horsehearts.txt). It is helpful to divide the scatterplot into four quarters, called quadrants, by drawing a line horizontally at the average \\(Y\\) value or weight (2.235kg) and vertically at the average \\(X\\) value (or diastole exterior width 14.13mm). The first quadrant is the top right hand side quarter. We move anti-clockwise for numbering the quadrants, and the bottom left hand side quadrant is called the third quadrant.\n\n\n\n\n\nFigure 31: Four quadrants of a scatter plot\n\n\n\n\nIn Figure 31, most of the points fall in the third and first quadrants which indicates that there is an increasing relationship between the two variables (as the length increases, the weight tends to increase which is not too surprising!).\nAlthough this scatterplot indicates a nonlinear relationship would fit better than a linear relationship, we will continue to use it for illustration. It is useful to have an objective measure of how strong a linear relationship is (as we have seen, stretching or compressing one or both axes can give different impressions). This measure is called the Pearsonian correlation coefficient between \\(X\\) and \\(Y\\) and is denoted by \\(r_{xy}\\) or \\(r\\) for short:\n\\[\n\\begin{aligned}\nr_{xy} &= \\frac{{\\text {sum}(x - \\bar x)(y - \\bar y)}}{{\\text{squareroot}[\\text {sum}(x - \\bar x)^2  {\\text{sum}}(y - \\bar y)^2 ]}} \\\\\n\\\\\n&= \\frac{\\sum (x-\\bar{x})(y-\\bar{y}) }{\\sqrt{\\sum (x-\\bar{x})^{2}  \\sum (y-\\bar{y})^{2}  } } \\\\\n\\\\\n&=\\frac{S_{xy} }{\\sqrt{S_{xx} S_{yy} } }\n\\end{aligned}\n\\]\nHere \\(S_{xx}=S_x^2\\) is the variance of \\(X\\), \\(S_{yy}=S_y^2\\) is the variance of \\(Y\\), and \\(S_{xy}\\) is the covariance between \\(X\\) and \\(Y\\).\nThe (Pearsonian) correlation (coefficient) between WEIGHT and EXTDIA equals 0.759. Perfect correlation (+l) would occur if all points fell on a straight line in the first and third quadrants. Assuming a bivariate normal distribution, the null hypothesis that the true correlation coefficient \\(\\rho _{xy} =0\\) can be tested using the test statistic \\[t = \\frac{{r_{xy} }}{{\\sqrt {\\frac{{1 - r_{xy}^2 }}{{n - 2}}} }}\\] Statistical software programs directly give the \\(p\\)-value for this test. The estimated correlation coefficient of 0.759 for \\((n-2)=44\\) df is significant (\\(p\\)-value is close to zero). Testing of hypothesis related topics are covered in the next Chapter.\nThe scatterplot indicates that there may be two groups of horses; whose hearts weigh about 2.3 kg or more and whose heart weights are less than 2.3 kg. If possible, one should try to establish whether there are two groups for some reason (perhaps age, sex or breed). If the points tended to concentrate in quadrants 2 and 4, this would indicate that the correlation between \\(Y\\) and \\(X\\) is negative (as \\(X\\) increases, \\(Y\\) tends to decrease). If the points were evenly scattered among the four quadrants, the correlation coefficient would be close to zero.\n\n\n\n\n\nFigure 32: Limitations of the correlation coefficient\n\n\n\n\nFigure 32 shows a number of possible plots of \\(Y\\) against \\(X\\) and their associated correlation coefficients. Notice that the correlation coefficient may be close to zero even though there is a strong pattern present. This is because the correlation coefficient only measures the strength of the linear relationship between two variables. In the presence of subgroups the correlation coefficient can become spurious or even be close to zero when there is indeed the correlation is high within the subgroup. Simpson (1951) demonstrated that relationships noted within a population may not hold and could be the opposite within all subgroups. This amalgamation paradox can happen to most summary statistical measures including the correlation coefficient.\nThe other situation that is not shown in Figure 32 is that the correlation coefficient can be positive and a line would be a reasonable fit although the scatter diagram indicates that both \\(Y\\) and \\(X\\) have skewed distributions suggesting that transformations should be applied.\n\n\n\nFigure 33: Sign of the correlation coefficient\n\n\nTo see why the sign of \\(r\\) indicates the direction of the relationship, consider Figure 33.\n\nRegardless of which quadrant the point is in, the contribution to \\(S_{xx}\\) and \\(S_{yy}\\) is positive, as these are sums of squares.\nIn the first quadrant the contribution of each point to the cross product \\(S_{xy}\\) is \\((+) \\times (+)\\), that is a positive amount. In the third quadrant, each contribution is \\((-)\\times (-)\\), that is a positive amount. This is the reason that the correlation coefficient is positive if most points are in the first and third quadrants.\nPoints in the second and fourth quadrant contribute negative amounts, \\((-)\\times (+)\\) or \\((+)\\times (-)\\), to the cross product \\(S_{xy}\\). If there are a considerable number of points in quadrants 2 and 4, the correlation coefficient will tend to be negative.\n\n\n\nScatterplot and correlation Matrices\nWhen there are more than two variables of interest, it is possible to draw scatterplots of each pair of variables but this becomes a bewildering exercise as the number of variables increases. One way to keep track of these plots is to set them out in a scatterplot matrix as in Figure 34 which is based on the data set pinetree.txt containing the circumference measurements of pine trees at four positions (Top, Third, Second and First) and in three areas of a forest. This plot also shows the simple correlation coefficients on the upper diagonal and smoothed density plots on the main diagonal of the matrix. This plot clearly shows that the variables are strongly related to each other. In particular, the Top circumference can be predicted reasonably well based on the bottom circumference. Even though the correlations are high, a quadratic model may fit better. Figure 34 seems to show a non-random pattern in the way points appear on the plot. This may be due to the Area effect because the circumference data were collected from three different areas. Figure 34 confirms that this is indeed the case.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/pinetree.RData\", \n  destfile = \"pinetree.RData\")\nload(\"pinetree.RData\")\n\n\n\n\nCode\nlibrary(GGally)\n\nggpairs(pinetree, columns = 2:5)\n\n\n\n\n\nFigure 34: Four quadrants of a scatter plot\n\n\n\n\nIt is also possible to use the colour option as well as the subgroup-wise correlation coefficients. Try\nggpairs(\n  pinetree, \n  columns = 2:5,\n  mapping = aes(colour = Area)\n  )\n\n\nExploring bivariate distributions\nThe two-dimensional or the bivariate probability distribution of \\(X\\) and \\(Y\\) can be displayed as a 2-d smoothed density plot or as a probability contour plot. Figure 35 shows such plots. This figure rather suggests that there may be two separate joint probability distributions of Maths and English scores. This clue is not obtained in the scatter plot of English score against Maths score but only from the other three plots.\n\n\nCode\ny = testmarks$English\nx = testmarks$Maths\ndef.par = par(mfrow = c(2, 2))\n\nplot(x, y, xlim = c(0,100), ylim = c(0,100),\n     xlab = \"Maths Marks\",\n     ylab = \"English Marks\",\n     main = \"Scatter plot of English Scores vs Maths Scores\")\n\nf1 = kde2d(x, y, n = 50, lims = c(0, 100, 0, 100))\n\nimage(f1,\n      xlab = \"Maths Marks\",\n      ylab = \"English Marks\",\n      col=gray((0:32)/32), main = \"Image Plot\")\n\ncontour(f1, \n        xlab = \"Maths Marks\",\n        ylab = \"English Marks\",\n        main = \"Density contourPlot\")\n\nop = par(mar=c(0,0,2,0)+.1)\n\npersp(f1, phi = 45, theta = 30,\n      xlab = \"Maths Marks\", ylab = \"English Marks\", zlab = \"density\",\n      col = \"gray\", shade = .5, border = NA,\n      main = \"Perspective plot (density estimation)\")\n\npar(op)\n\npar(def.par)\n\n\n\n\n\nFigure 35: Exploring bivariate distributions\n\n\n\n\n\n\n\n\n\n\n\nIn summary, scatterplots may reveal many properties and peculiarities about the relationship of \\(Y\\) to \\(X\\), namely\n\nTrends: the overall trend may be positive or negative. It may be appropriate to fit a straight line or a curve.\nOutliers: points which appear to be unusual, or outliers, may be evident. In a scatterplot, the outlier need not be in the extreme scale of both \\(X\\) and \\(Y\\), and anything appearing peculiar or as a rogue point must be investigated.\nGaps may be seen in the data which may indicate that different groups are present. On the other hand, if it is known that points belong to different groups, the points can be tagged with different symbols. The basic idea is to see whether different groups follow the same pattern or different patterns.\nVariability of \\(Y\\) change with \\(X\\), or may be fairly constant. Clustering of points with smaller variability when compared to rest of the points may also be revealed in some scatterplots.\n\nBoth joint and marginal distributions of \\(X\\) and \\(Y\\) are important. We also consider the distribution of each variable separately, and explore whether they have symmetric distributions or not."
  },
  {
    "objectID": "studyguide/2-eda.html#higher-dimensional-data",
    "href": "studyguide/2-eda.html#higher-dimensional-data",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Higher dimensional data",
    "text": "Higher dimensional data\nA short-cut to explore three numerical variables is to obtain a contour plot. In Figure 35, the contours were the joint density of Maths and English marks. Instead, we can just use three numerical variables and show the third variable (Z) as a contour on a scatter plot of Y vs. X. What this means is that several combinations of X and Y can lead to the same Z. So we have to use some smoothing method or a model to obtain the contours and form a grid. The R package plotly is rather simple and powerful to obtain dynamic or interactive plots. The plots produced with this package are colour optimised for viewing and hence they may not be the best for printing but good for EDA purposes. We can also hover over the graph and view the graphed data. After loading the pine tree data, try-\n\n\nCode\nlibrary(plotly)\nplot_ly(type = 'contour', \n        x=pinetree$First, \n        y=pinetree$Second, \n        z=pinetree$Top)\n\n\n\n\n\n\nFor the Snodgrass data, the contour plot of East (X) vs, South (Y) vs. Area (Z) is shown in Figure 36. This plot shows that bigger house pits are inside the X and Y axis show the geolocations.\n\n\n\nFigure 36: A contour plot for the Snodgrass data\n\n\nA bubble plot is another option where a scatter plot shows the third variable as the size of the bubble on a scatter plot. This type of plot can be restrictive for some datasets depending on the variability involved in the third variable. Figure 37 shows the location of the house pits and total Area. This plot not only gives the spatial view of the tribal area but indirectly reveals the clustering of house sizes.\n\n\nCode\nggplot(Snodgrass) + \n  aes(x=East, y=South, colour=Area, size=Area) + \n  geom_point()\n\n\n\n\n\nFigure 37: A bubble plot for the Snodgrass data\n\n\n\n\nUsing the appropriate software, it is possible to explore three variables using a 3-D scatter plot. We explore the pinetree data in Figure 38 which clearly shows the strong relationships and the Area effect. The other forms of exploring 3 variables include 3-D surface plots.\n\n\nCode\nlibrary(lattice)\ncloud(Top ~ First+Second, group=Area, data=pinetree)\n\n\n\n\n\nFigure 38: A 3-D plot\n\n\n\n\nR packages such as rgl and plotly allow rotations of axes for graphical exploration of the relationships among variables, detection of peculiar points etc. It is easy to miss the form of relationships if proper rotational exploration is not done. Try:\n\n\nCode\nlibrary(rgl)\nplot3d(Snodgrass$East, Snodgrass$South, Snodgrass$Area)\n\n\nplotly is great for interactive 3d plots (though they only work with HTML, not PDF).\n\n\nCode\nplot_ly(Snodgrass, x = ~East, y = ~South, z = ~Area) |&gt; \n  add_markers()\n\n\n\n\n\n\nIf we wish to examine the relationships between (three or more) variables we could do so with a conditioning plot, otherwise known as a coplot. A coplot is a scatterplot matrix where each panel in the matrix is a scatterplot of the same two variables, whose observations are restricted by the values of a third (and possibly a fourth) variable.\n\n\nCode\ncoplot(Top~ First | Second*Area, data = pinetree)\n\n\n\n\n\nFigure 39: A co-plot\n\n\n\n\nThe coplot in Figure 39 shows that pine trees are not maturing well in Area 3, given that fewer points are on the right hand side of the graph when compared to Areas 1 and 2.\nObviously we cannot to visualise several dimensions easily but there are few shortcuts for multivariate data visualisation such as the Chernoff or cartoon faces plot. This type of plot is based on our ability to recognise features in human faces. This type of plot is suitable for small datasets and identify unusual observations, for example observation #46 in horsehearts data shown in Figure 40.\n\n\neffect of variables:\n modified item       Var       \n \"height of face   \" \"INNERSYS\"\n \"width of face    \" \"INNERDIA\"\n \"structure of face\" \"OUTERSYS\"\n \"height of mouth  \" \"OUTERDIA\"\n \"width of mouth   \" \"EXTSYS\"  \n \"smiling          \" \"EXTDIA\"  \n \"height of eyes   \" \"WEIGHT\"  \n \"width of eyes    \" \"INNERSYS\"\n \"height of hair   \" \"INNERDIA\"\n \"width of hair   \"  \"OUTERSYS\"\n \"style of hair   \"  \"OUTERDIA\"\n \"height of nose  \"  \"EXTSYS\"  \n \"width of nose   \"  \"EXTDIA\"  \n \"width of ear    \"  \"WEIGHT\"  \n \"height of ear   \"  \"INNERSYS\"\n\n\n\n\n\nFigure 40: A Cartoon Faces plot\n\n\n\n\nExploration of higher dimensional data requires further theory. Reduction of dimensions and display of crucial information using newly generated variables based on the theory of multivariate statistics are the natural way to explore higher dimensional data. Many such approaches are taught mainly in 300 level papers. A couple of such methods are explained below:\nOne of the tricks to handle multidimensional data is to reduce the number of dimensions using principal components. These are simply linear combinations of the original variables and may not have physical meaning. In some cases, we may be able to identify a latent variable that represents such a linear combination of variables. For example, physical endurance is latent but can be measured indirectly using many variables such as time taken to run 100m, maximum weight lifted etc. The first principal component (PC) is found in such a way that it accounts for the most variation in the data. The second principal component is found independent of the first in such a way that it accounts for the most variation that is not explained by the first principal component. The following output shows that the first two components are sufficient to explain most (&gt;98%) of the variation in the four numerical variables in the pinetree dataset.\n\n\nCode\nfit &lt;- prcomp(pinetree[, -1], scale. = TRUE)\nsummary(fit)\n\n\nImportance of components:\n                          PC1     PC2     PC3    PC4\nStandard deviation     1.9579 0.32825 0.19678 0.1415\nProportion of Variance 0.9584 0.02694 0.00968 0.0050\nCumulative Proportion  0.9584 0.98532 0.99500 1.0000\n\n\nCode\n# Try autoplot(fit)\n\n\nA biplot aims to show both the observations and variables in the same plot formed by the first two principal components. Figure 41 shows the biplot for the pinetree variables and data.\n\n\nCode\nlibrary(ggfortify)\nautoplot(fit, loadings = TRUE, loadings.label = TRUE)\n\n\n\n\n\nFigure 41: A biplot\n\n\n\n\nFor interpreting the biplot, consider the following patterns:\n\nIf points that are close to each other, it means observations with similar values. So you can spot the odd ones.\nThe correlation between variables is indicated by the cosine of the angle between vectors displayed for the variables. Highly correlated variables will point in the same direction. If variables are uncorrelated, they will be at right angles to each other. For the pine tree data, we notice high correlation. This has implications in model building. We may not need all of the three predictor variables of Top. More will follow in later Chapters.\nThe cosine of the angle between a vector and an axis indicates the contribution of the corresponding variable to the axis dimension. Look particularly the X-axis that represents the first principal component. The variables contribute somewhat the same.\n\nNote that the biplot is not useful if the first two principal components account for only a small part of the overall variation. We can also examine relationship between the categorical factors and the principal components but this topic is skipped in this course."
  },
  {
    "objectID": "studyguide/2-eda.html#autocorrelation",
    "href": "studyguide/2-eda.html#autocorrelation",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nTime series data are not often independent and there often exists a relationship between the current observation \\(X_t\\) at time \\(t\\) with the past observations \\(X_j\\) \\((j=t-1, t-2, \\ldots, 1)\\). The term first order lag refers to \\(X_{j-1}\\) for \\(X_j\\). In general the \\(k^\\text {th}\\) lag refers to a past observation of \\(k\\) periods back. We may explore the time related linear dependency in time series data using lagplots. Figure 44 is a lag plot for the NZ $20 series.\n\n\nCode\ngglagplot(NZnotes20)\n\n\n\n\n\nFigure 44: Lag plot of NZD 20 notes series\n\n\n\n\nThis plot suggests that the time related dependency is rather strong for lower order \\(k\\) but the lag effect diminishes when \\(k\\) becomes large . This is a good thing because we can use recent lagged data as predictors in our models.\nBy the term autocorrelation we mean the simple correlation between the observations but lagged back one or more time periods. For a given lag \\(k\\), the sample autocorrelation function (SACF or simply ACF) is defined as\n\\[r_{k} = \\frac{ \\sum \\limits_{t=k+1}^N (y_{t}-\\bar{y})(y_{t-k}-\\bar{y})} {\\sum \\limits_{t=1}^T (y_{t}-\\bar{y})^2}\\]\nwhere \\(N\\) is the length of the time series. SACFs can be used to make a tentative assessment of the terms or order of the model. Figure 45 is the plot of SACF values (called ACF plot or correlogram) for the NZ $20 series.\n\n\nCode\nggAcf(NZnotes20)\n\n\n\n\n\nFigure 45: ACF plot of NZD 20 notes series\n\n\n\n\nThis graph shows that the autocorrelations decay to zero (implying that the value of $20 notes in public hands positively depend on the values of $20 notes held in the immediate past rather than the distant past). The significance of autocorrelations may be judged from the 95% confidence interval band. More on this later.\nWe also plot the partial autocorrelations instead of autocorrelations. Partial Autocorrelation Function (PACF) at a given time is without the linear effect of other lags. Figure 46 shows the PACF plot for the ‘$20 Notes’ series. Only the first PACF is significant and not the rest. This means that the first lag is sufficient and the rest may not be needed for modelling.\n\n\nCode\nggPacf(NZnotes20)\n\n\n\n\n\nFigure 46: PACF plot of NZD 20 notes series\n\n\n\n\nBoth ACF and PACF plots are useful to assess the time series components which is discussed next."
  },
  {
    "objectID": "studyguide/2-eda.html#time-series-components-of-variation",
    "href": "studyguide/2-eda.html#time-series-components-of-variation",
    "title": "Chapter 2: Exploratory Data Analysis (EDA)",
    "section": "Time Series Components of Variation",
    "text": "Time Series Components of Variation\nThe nature of structural variations in a time series are broadly classified under three major headings:\n\nTrend\nThe trend component is to represent long term positive (upward) or negative (downward) movement. See Figure 47 below which lists some of the parametric models for fitting the trend component:\n\n\n\nFigure 47: Trend types\n\n\nWe will cover more on fitting models to capture the trend later on.\n\n\nSeasonal\nThe seasonal component is to account for the periodic behaviour happening within a block (say Christmas time) of a given time period (say in a calendar year) but this periodic behaviour will repeat fairly regularly over time (say year after year). Seasonality is often present in business related time series data. EDA tools are useful for judging the presence of seasonality. A simple scatterplot of the response variable against time may reveal seasonality directly. Figure 48 is a time series plot of monthly means of Erythemal UV, the sunburn causing UV, measured at Lauder (45.0S, 169.7E), New Zealand (uv.txt).\n\n\nCode\nlibrary(readr)\n\nuv &lt;- read_table(\"http://www.massey.ac.nz/~anhsmith/data/uv.txt\") |&gt; \n  pull(erythemal.uv) |&gt; \n  ts(start = c(1990, 01), frequency = 12)\n\nforecast::autoplot(uv, xlab=\"Year\", ylab=\"Erythemal UV\")\n\n\n\n\n\nFigure 48: Time series plot revealing seasonality\n\n\n\n\nSeasonality is more easily seen graphically when two or more grouping variables such as month is used to subset the data as has been done in Figure 49. All tools of EDA covered in this Chapter such as boxplots etc will also be useful to explore seasonal effects. The autocorrelation plot is also useful for detecting seasonality.\n\n\nCode\n# monthplot(uv) or\nggsubseriesplot(uv)\n\n\n\n\n\nFigure 49: Month plot revealing seasonality\n\n\n\n\nA different version of the plot, known as season plot, can also be used; see Figure 50 drawn for the credit balances data. This plot shows the month related seasonal effects over the years. We can also assess whether the seasonal effect is the same for various years or not using this plot.\n\n\nCode\nggseasonplot(credit.balance, year.labels=TRUE)\n\n\n\n\n\nFigure 50: Seasonal plot for credit balances series\n\n\n\n\nThe ACF plot is also useful to assess seasonality. Figure 51 shows a scallop pattern which is attributed to seasonality.\n\n\nCode\nggAcf(uv)\n\n\n\n\n\nFigure 51: ACF plot of uv series\n\n\n\n\nSpectral plot is another graphical technique for examining cyclic structure of variation in time series data. This plot is based on a sinusoidal model. The cycle frequency is expressed per unit of time where a unit of time is the distance between 2 time series points. A frequency of 0.5 means a cycle of 2 time series data points. Methods are available to determine the amplitude and the dominant frequency etc under the sinusoidal model.\n\n\nError\nThis component of the time series is to account for the random or irregular movement in the series. Ideally the plot of errors should not show any time effect; for example plotting a set of random normal data will not show any of the systematic time related components, see Figure 52. Such a series is called a white noise series.\n\n\nCode\nset.seed(123)\nwhtnoise &lt;- ts(rnorm(120), start=1, frequency = 12)\nforecast::autoplot(whtnoise, \n                   xlab=\"time\", \n                   ylab=\"N(0,1) random data\")\n\n\n\n\n\nFigure 52: Time series plot of random normal data\n\n\n\n\nThe ACF plot of our white noise series is shown in Figure 53. As you would expected, the ACFs are small in size and not significant. The ACFs also do not die out in any systematic pattern too.\n\n\nCode\nggAcf(whtnoise)\n\n\n\n\n\nFigure 53: ACF plot of random normal data\n\n\n\n\nIt is the practice to assume a typical structural interrelationship between the trend, seasonal or cyclical components of a time series. This may be done using an additive model for the observation at time \\(t\\) say \\(X_t\\) as\n\\(X_t\\) = Trend + Seasonal + Error\nor, using the multiplicative model,\n\\(X_t\\) = Trend \\(\\times\\) Seasonal + Error.\nThe multiplicative model takes the interaction between the components into account whereas the additive model assumes independence of the components. We have not explicitly considered the cyclical component of variation in the models and assume that the either the trend or the seasonal part of the model will include it.\nConsider the UV series, which is a short time series. Hence this series may not reveal secular (long term) trend or cycles. Figure 54 shows the decomposition done for the UV series. While this plot is useful in revealing the seasonality, trend component is probably over-fitted.\nThe associated standard error bars also appear on the RHS side of the plot and the trend estimation is done poorly after all.\n\n\nCode\nuv |&gt; \n  decompose(type=\"additive\") |&gt;  \n  forecast::autoplot() + \n  ggtitle(\"\")\n\n\n\n\n\nFigure 54: Classical decomposition for UV data\n\n\n\n\nThe classical decomposition done for the credit card balances data is shown in Figure 55. For this series, the trend component is strong and estimated well. The seasonal component is weaker and not estimated well at all.\n\n\nCode\ncredit.balance |&gt; \n  decompose(type=\"additive\") |&gt; \n  forecast::autoplot() + \n  ggtitle(\"\")\n\n\n\n\n\nFigure 55: Classical decomposition for credit data\n\n\n\n\nThere are many approaches to decomposing the components of a time series. Some are designed to improve forecasting. These methods are covered in higher level statistics courses.\nBy the term detrending, we mean the process of removing the trend from a time series. This can be model based. For example, we may fit a S-type logistic model and the residuals of this model is called the detrended series, which mainly contains the seasonal and error components of variation. The other approach to detrending is to apply differencing which is discussed later on. There are also nonparametric approaches to fitting trends. For example, the moving average smoothing (discussed in a later Chapter) can be treated as fit (filter) and the residuals after smoothing can be regarded as detrended data.\nBy the term deseasoning we mean the process of removing the seasonal component of the variation in a time series. For example, we may use a regression approach to deseason the series after fitting the model with seasonal indicator variables. We may also use seasonal differencing or use trigonometric functions for deseasoning.\nIt is not always easy to completely separate trends from seasonal effect but time series decomposition helps us to understand the underlying complexities.\nR packages are available for exploring spatial data, particularly for integrating with maps. Some software programs also employ audio to augment exploration of higher dimensional data. Plots for EDA are introduced at an introductory level in this Chapter but there are many dedicated R packages available for conducting EDA in a specialised area such as finance or ecology."
  },
  {
    "objectID": "studyguide/4-inference.html",
    "href": "studyguide/4-inference.html",
    "title": "Chapter 4: Statistical Inference",
    "section": "",
    "text": "“All models are wrong, but some are useful.”\n– George Box\nThis chapter provides an introduction to statistical inference. Many of the concepts in this chapter should be familiar to you because they are covered in all first-year statistics courses."
  },
  {
    "objectID": "studyguide/4-inference.html#populations-and-parameters-samples-and-statistics",
    "href": "studyguide/4-inference.html#populations-and-parameters-samples-and-statistics",
    "title": "Chapter 4: Statistical Inference",
    "section": "Populations and parameters, samples and statistics",
    "text": "Populations and parameters, samples and statistics\nStatistical inference is a fundamental concept in statistics. The vast majority of statistical analyses that you will do as an undergraduate involve statistical inference. Anything involving p-values, confidence intervals, or standard errors are a form of statistical inference.\n\n\n\n\n\n\nStatistical inference is:\n\n\n\nthe use of information from a sample to make statements about a population.\n\n\nAs discussed in previous chapters, most datasets contain information about a sample from a population, rather than the whole population of interest. For example (Figure 1), say we owned a fish farm, and we wished to know the average length of the fish in our farm. Let’s say we had 2,000 fish in our farm. It would be too time-consuming to catch and measure every single fish. Instead, we take a random sample of, say, 10 fish, measure their lengths, and calculate the mean.\nRemember, our goal here is to know something about the whole population of 2,000 fish. We don’t really care about the 10 fish in our sample. It is no use to say “Well, I’ve no idea about the average length of my whole population fish, but you see those 10 fish there? They average 36.7 cm in length.”. We only care about the 10 fish in our sample in so far as they tell us something about the broader population. We use the average length of the fish in our sample as and estimate of the average length of fish in the population. This is statistical inference: using information from a sample to make conclusions about a population.\n\n\n\nFigure 1: Statistical inference from a sample to a population of fish\n\n\nTo clarify some terminology using the example in Figure 1:\n\nThe population is all 2,000 fish in our farm.\nThe sample is the 10 fish we happened to measure.\nThe parameter of interest (often denoted \\(\\mu\\), if it is a mean, or \\(\\theta\\) more generally) is the average length of the fish in the population of 2,000. Population parameters are usually considered to be fixed and unknown values.\nThe statistic (often denoted \\(\\bar{y}\\) or \\(\\hat\\mu\\), if it is a mean, or \\(\\hat\\theta\\) more generally) is the average of the 10 lengths of the fish in our sample. Unlike population parameters, which are fixed and unknown, sample statistics are random variables.\nStatistical inference in this case is the use of the sample mean \\(\\bar{y}\\) as an estimate of the population mean \\(\\mu\\).\n\nThe fact that we’ve only measured lengths from a sample rather than the whole population necessitates statistical inference. If we’d measured every fish in the farm, we wouldn’t need statistical inference, because we’d know precisely the population parameter (assuming negligible measurement error)."
  },
  {
    "objectID": "studyguide/4-inference.html#sampling-error",
    "href": "studyguide/4-inference.html#sampling-error",
    "title": "Chapter 4: Statistical Inference",
    "section": "Sampling Error",
    "text": "Sampling Error\nThis brings us to the next important concept of statistical inference: sampling error. A consequence of having collected data from a sample rather than the whole population is that there is uncertainty in our knowledge of the population parameter. Our sample mean is an estimate of the population mean; if we wanted to know population mean with zero uncertainty, we’d have to measure all the fish. This is the trade-off of sampling. It’s a lot cheaper to sample, but we sacrifice certainty.\nThe practical application of statistical inference involves (1) making estimates and (2) quantifying the uncertainty of those estimates. Uncertainty is often quantified using standard errors, confidence intervals, and P-values. All these quantities relate to sampling error. They’re all expressions of the uncertainty of an estimate of a population parameter.\nIn the fish farm example (Figure 1), sampling error is the hypothetical variation in the means of the lengths of samples of fish, with a sample size of \\(n\\) = 10. That is, if we were to (hypothetically) repeat the scientific process (i.e., randomly select 10 fish, measure their lengths, and calculate the sample mean), over and over again, how much would those sample means vary? That variation of sample statistics is sampling variation, or sampling error. And understanding sampling variation is the key to understanding most of undergraduate statistics.\nSo, when we do our study (i.e., randomly select 10 fish, measure their lengths, and calculate the sample mean), we are drawing one value of the sample mean, \\(\\bar y\\), from a random variable, \\(\\bar Y\\), which is the distribution of sample means that we could hypothetically draw.\nGiven this random sampling variation, here are some explanations for some commonly used measures of uncertainty:\n\nA standard error is simply the standard deviation of a statistic under repeated sampling–that is, how much it would vary (hypothetically) from sample to sample.\nA confidence interval is a pair of numbers that contain the true value of the population parameter with 95% confidence. It is a simple function of the sample statistic and its standard error.\nA p-value is the probability of obtaining a sample statistic as or more extreme than the one observed, given a particular hypothesised value (usually zero) of the population parameter.\n\nDon’t worry if those definitions aren’t completely clear to you right now, but I encourage you to refer back to this section again and again as you learn about them in more detail during the rest of this course.\nKeep sampling error front of mind whenever you see a standard error, a confidence interval, or a p-value.\nNow, we’ll introduce some specific inference methods."
  },
  {
    "objectID": "studyguide/4-inference.html#example-rangitikei",
    "href": "studyguide/4-inference.html#example-rangitikei",
    "title": "Chapter 4: Statistical Inference",
    "section": "Example: rangitikei",
    "text": "Example: rangitikei\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_minimal())\n\n\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/rangitikei.RData\",\n  destfile = \"rangitikei.RData\")\n\nload(\"rangitikei.RData\")\n\n\n\n\nCode\nshapiro.test(rangitikei$people)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  rangitikei$people\nW = 0.65346, p-value = 1.382e-07\n\n\nThe low \\(p\\)-value here indicates a significant departure from normality. We conclude that there is very strong evidence against the null hypothesis that we the population is normally distributed. (Remember, always express your conclusions by reference to the population, not the sample, or even “the data”.)\nWe can examine a Q-Q plot (Figure 2), which plots the observed values (\\(y\\)) against the theoretical values if the population were normally distributed. Departure from the diagonal line indicates departure from normality.\n\n\nCode\np1 &lt;- ggplot(rangitikei) + \n  aes(sample = people) + \n  stat_qq() +\n  stat_qq_line() \n\np2 &lt;- ggplot(rangitikei) + \n  aes(y=people, x=\"\") +\n  geom_boxplot() +\n  xlab(\"\") +\n  coord_flip()\n\ngridExtra::grid.arrange(p1, p2, ncol=1) \n\n\n\n\n\nFigure 2: Distribution of people\n\n\n\n\nThe same conclusion is drawn with the Kolmogorov-Smirnov test. Note that this test does not allow ties and can be used to test the fitting of non-normal distributions.\n\n\nCode\nks.test(rangitikei$people, \"pnorm\")\n\n\nWarning in ks.test.default(rangitikei$people, \"pnorm\"): ties should not be\npresent for the Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  rangitikei$people\nD = 0.99997, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nIt is often informative to analyse data by fitting a statistical model. The idea is to look for real patterns, “signals” amongst the “noise” of individual variation, patterns that would reoccur in other, hypothetical samples we might have drawn from the population. We often try to approximate patterns by fitting a “statistical model”. A A statistical model usually comprises a mathematical formula describing the relationships among variables, along with a probabilistic description of the variation of the data around the formula. If the statistical model is a good approximation, it serves as a neat way of describing the system that generated the data, and we can use such a model to predict future values of the variables."
  },
  {
    "objectID": "studyguide/4-inference.html#example-testmarks",
    "href": "studyguide/4-inference.html#example-testmarks",
    "title": "Chapter 4: Statistical Inference",
    "section": "Example: testmarks",
    "text": "Example: testmarks\nThe data set tv consists of the time that 46 school children spent watching television. Before fitting a model to the data, it is a good idea to see whether the data approximately follows a Normal distribution using a normal Q-Q Plot; see Figure 3. The points plotted fall pretty much along the line, suggesting at least approximate Normality.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/tv.RData\", \n  destfile = \"tv.RData\")\n\nload(\"tv.RData\")\n\n\n\n\nCode\nP.val &lt;- tv$TELETIME |&gt; \n  shapiro.test() |&gt; \n  pluck('p.value') |&gt; \n  round(digits = 3)\n\np1 &lt;- ggplot(tv) + \n  aes(sample = TELETIME) + \n  stat_qq() +\n  stat_qq_line() +\n  labs(caption = paste(\"Shapiro Test P value\", P.val))\n\np2 &lt;- ggplot(tv) + \n  aes(y=TELETIME, x=\"\") +\n  geom_boxplot() +\n  xlab(\"\") +\n  coord_flip()\n\ngridExtra::grid.arrange(p1, p2, ncol=1) \n\n\n\n\n\nFigure 3: Distribution of TV viewing times\n\n\n\n\nFigure 3 shows the boxplot of the data. The boxplot again suggests a very mild skew to the left but the middle 50% data show right skewness. However the whiskers are about the same length and there are no outliers. There is a difference of 31 between the mean and median, suggestive of a slight skew to the lower values. However this difference is small given the overall variability (standard deviation is 567.9, and the range is 2309) so we can probably ignore the observed skew. However we will look for any further evidence of skewness, since this could invalidate any inference we make based on the Normal distribution (at least it would if we had a smaller sample). The TV viewing time data also passes normality tests such as Shapiro-Wilk test. All told, we conclude that the normal model describes the distribution of these data fairly well.\nAs indicated earlier, a large number of naturally occurring measurements, such as height, appear to follow a Normal distribution so that a considerable amount of theory has been built on this distribution.\nTo recapitulate, Normal (or Gaussian) curves are determined by just two numbers, one indicating the location and the other the spread. Although there are an infinite number of Normal curves, their shapes are similar and, of course, the area under each curve is 1. Indeed, the location and spread parameters are the only differences between curves.\nIt is usual to take the measure of location as the mean, denoted by \\(\\mu\\), and the measure of spread as the standard deviation, denoted by \\(\\sigma\\). If a variable, Y, follows a Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we write Y \\(\\sim\\) N(\\(\\mu\\), \\(\\sigma\\)) where \\(\\sim\\) means is distributed as (The squiggly symbol \\(\\sim\\) is known as tilde).\nNote, again, that the normal distribution, like all statistical distributions, is a theoretical concept and no naturally occurring measurement will exactly follow a probability distribution model. For one thing, any measurement is finite whereas the Normal curve is continuous in the interval \\(\\left(-\\infty ,\\infty \\right)\\). Also, the curve is asymptotic to the \\(X\\)-axis so that any range of values of \\(Y\\) however large or small will have a certain probability according to the Normal distribution, but in practice there will be limitations such as that a person’s blood pressure must be greater than zero.\nSuppose we didn’t know that \\(\\mu\\) = 80 and \\(\\sigma\\) = 12. The obvious estimator of \\(\\mu\\), based solely on the sample, is the sample mean \\(\\bar{y}\\), and the obvious estimator of \\(\\sigma\\) is the sample standard deviation \\(S\\). Note that by estimator we don’t mean the observed value based on the particular sample. Rather the word estimator means the mathematical formula or procedure that we use to produce our estimates, namely \\(\\bar{y}={\\frac{1}{n}} \\sum y\\) and \\(S=\\sqrt{{\\frac{1}{n-1}} \\sum _{i=1}^{n}(y_{i} -\\bar{y})^{2} }\\).\nThe point is that there can be several alternative procedures for estimating the same parameters \\(\\mu\\) and \\(\\sigma\\), and in particular samples the actual computed estimates may be the same or different. For example, since the mean and median are the same for Normal data, we could estimate \\(\\mu\\) by the sample median, namely 81.313 for our example. This different procedure has given rise to a different number, and if we didn’t know the answer we would not know which estimate to use. The median estimate has a lot of attraction, since the median is robust, that is, is not affected by outliers.\nClearly all these estimates are close to the true parameter values, but not the same. A statistical question relates to how close estimates are to their true values in general. We usually can’t answer this question about the actual estimates (observed numbers) since we usually don’t know the correct answer. One approach, which is often used to test procedures in research, is to try a number of simulations and see which approach produces the closest results on average. We can also give error bounds that say, for example, that 95% of the time the estimator is within such-and-such a distance of the true parameter. This leads to the idea of using probability or so-called ‘confidence’. By making probability statements about the estimators we can say something useful about how trustworthy the particular estimates are also. We use standard errors to measure the trustworthiness of the estimators. The standard error is the standard deviation of the estimator, so the smaller the standard error the better.\nWithout going into details, it turns out that \\(\\bar{y}\\) has the smallest possible standard error for any unbiased estimator of \\(\\mu\\) for normal data. (An unbiased estimator is one that is not systematically too big or too small.) While the same is not true of \\(S\\) in relation to \\(\\sigma\\), the latter does have other useful mathematical properties. So these are some reasons for using these formulae so routinely."
  },
  {
    "objectID": "studyguide/4-inference.html#hypothesis-testing-for-mean",
    "href": "studyguide/4-inference.html#hypothesis-testing-for-mean",
    "title": "Chapter 4: Statistical Inference",
    "section": "Hypothesis testing for mean",
    "text": "Hypothesis testing for mean\nFor testing the mean of a population, the fact that we have specified a parameter value in \\(H_0\\) enables us to specify a probability distribution, for example that the data are a random sample from N(\\(\\mu_0\\), \\(\\sigma\\)). This immediately implies that\n\\(t = \\frac{\\bar{y}-\\mu _{0} }{S/\\sqrt{n} } \\sim t{}_{n-1}\\)\nNow if the hypothesis \\(H_0\\) is true, we should have \\(\\mu\\cong\\mu_0\\) . The notation equal-with-squiggle, \\(\\cong\\), stands for “approximately equal to” ), so that \\(t\\) should generally be close to 0. If the hypothesis is false we should get either much less than \\(\\mu_0\\) or much greater than \\(\\mu_0\\), in which case \\(t\\) should be large, out in the tails of the \\(t_{n-1}\\) distribution. Since we have a probability distribution, we can quantify just how unlikely our particular \\(t\\) is by comparing our sample value with the distribution.\nLet’s make things more specific by considering the television example. The degrees of freedom are \\(n-1 = 45\\). We have already indicated that \\(t\\cong0\\) implies \\(\\mu\\cong\\mu_0\\), in other words that the data matches the hypothesis very closely. But suppose instead we observed \\(t= 0.68\\). Could we regard this as an unusually large value of \\(t\\), that is, as evidence against \\(H_0\\)? The answer is no! The reason is that 0.68 is the upper quartile of the \\(t_{45}\\) distribution; in other words half (50%) of the time we would see values of \\(t\\) either greater than 0.68 or less than -0.68, even if the hypothesis \\(H_0\\) is true. Now what if \\(t\\) were below -1.6794? Would that be regarded as an unusual amount of discrepancy between \\(t\\) and \\(\\mu\\), that is as evidence against \\(H_0\\)? The answer is maybe. The fact is that 10% of the time one sees \\(t\\) \\(&lt;-1.6794\\) or \\(t&gt;1.6794\\), even if \\(H_0\\) is true. So if we use this rule, we have a 10% chance of wrongly rejecting \\(H_0\\). Few New Zealanders would feel happy with a legal system that allowed a 10% chance of wrongfully convicting an innocent person. Finally, what if we observe \\(t= -2.6896\\)? Only 1% of the area under the \\(t_{45}\\) curve lies outside the interval -2.6896 to +2.6896, so we would conclude that such a value of \\(t\\) was quite unlikely. This then would be strong evidence against \\(H_0\\).\nSuppose now we test the extremely unlikely hypothesis \\(H_0:\\mu = 0\\). Since \\(S/\\sqrt n=83.7\\), we obtain \\(t = (1729.3-0)/83.7 = 20.65\\). This is far larger than could be expected by chance, so the \\(t\\) statistic provides clear evidence against \\(H_0\\).\nA more realistic test may be whether \\(H_0:\\mu = 1500\\). Perhaps a previous study found an average time of viewing of 1500 minutes per week, and we want to check this. The test statistic is: \\(t=(1729.3-1500)/83.7=2.74\\). This is just outside our 1% bounds established earlier, so we conclude the data and \\(H_0\\) do not seem to agree. We reject \\(H_0:\\mu=1500\\).\nFinally suppose we test the hypothesis \\(H_0:\\mu=1600\\). Again this hypothesis could have been prompted by a previous study. Then \\(t=(1729.3-1600)/83.7=1.54\\). This is within the interval -1.6794 to 1.6794, suggesting that 1.54 is not an unusually highly value since it is exceeded more than 10% of the time when \\(H_0\\) is true. We would not reject the hypothesis \\(H_0:\\mu = 1600\\).\nWhen statistical test of hypothesis is done using R software, we rely on the \\(p\\)-value displayed.\n\n\nCode\nt.test(tv$TELETIME, mu=1500)\n\n\n\n    One Sample t-test\n\ndata:  tv$TELETIME\nt = 2.7382, df = 45, p-value = 0.008818\nalternative hypothesis: true mean is not equal to 1500\n95 percent confidence interval:\n 1560.633 1897.932\nsample estimates:\nmean of x \n 1729.283 \n\n\nSo what is a \\(P\\)-value? Formally, A P-value is the probability of observing data as extreme or more extreme as the data you actually observed, if \\(H_0\\) is true. This sounds like a very abstract and difficult concept to grasp, but it’s in fact exactly the rule we have been using. We saw that 50% of the time, \\(t&lt;-0.68\\) or \\(t&gt;0.68\\). So the \\(p\\)-value for a \\(t=0.68\\) is 0.5. We saw that 10% of the time, \\(t&lt;-1.6794\\) or \\(t&gt;1.6794\\). So the \\(p\\)-value of \\(t= -1.6794\\) is 0.1. And we saw that 1% of the time \\(t&lt;-2.6896\\) or \\(t&gt;2.6896\\), so the \\(p\\)-value of \\(t=2.6896\\) is 0.01. What is the \\(p\\)-value of \\(t= 2.74\\)? The answer is 0.009 or just under 1%. What is the \\(p\\)-value of \\(t= 1.54\\)? Answer is 0.130.\nUsually, we reject \\(H_0\\) in favour of \\(H_{1}\\) if the \\(p\\)-value of the data is \\(&lt; 0.05\\). In this case the test is said to be significant, and the 0.05 is called the significance level of the test. Otherwise (when we don’t reject \\(H_0\\)) we call the test non-significant. Some refer to a \\(p\\)-value \\(&lt;0.01\\) as very significant or highly significant. In journal articles and published tables of results, the three case non-significant, significant, and highly significant are often abbreviated as NS, * and ** respectively. The issues of using a standard cut-off of 5% for significance level, largely insisted in journals, has had its unintended consequences. The false discovery in science can be avoided if P values are not used as the sole criterion to draw conclusions. Read the advice on P values presented next.\nAdvice on the use of P values\nUnfortunately the P-values are often misunderstood in practice. The advice issued by the American Statistical Association (https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf) is noteworthy:\n\nThe statement’s six principles, many of which address misconceptions and misuse of the p-value, are the following:\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n\nIn order to fully appreciate the reasons behind the above advice, we need learn more theory but you should be able to understand the following distinction between statistical significance and practical significance. A hypothesis test may suggest that the estimated size of the effect is not big enough compared to the effect that can occur due to errors under the assumed model. A small effect, particularly if it is known to be caused by a variable, can be of practical importance and can contribute to scientific knowledge. The opposite scenario is also possible. We may find a small difference to be statistically significant because of the large sample size but such a difference may not be of much practical significance.\nA hypothesis test has a certain power (probability) to reject the null hypothesis when it is false. This power (probability) is a function of the sample size and the unknown parameters of the probability model adopted for testing.\nAssume that we are testing the null hypothesis \\(H_0:\\mu = 0\\) using the null model \\(N(0,1)\\). The power of the one sample t test can be evaluated using the R function power.t.test() for a given \\(\\delta\\), the difference in the true mean and what was hypothesised under \\(H_0\\). For example, the power of the \\(t\\)-test for \\(n=30\\) is lower than the power when \\(n=50\\) (say) when other settings are the same. Try-\n\n\nCode\npower.t.test(n = 30, delta = 1, sd = 1, sig.level = 0.05)\npower.t.test(n = 50, delta = 1, sd = 1, sig.level = 0.05)\n\n\nThe power to detect a small change in the mean is often low. Try-\n\n\nCode\npower.t.test(n = 30, delta = .25, sd = 1, sig.level = 0.05)\npower.t.test(n = 30, delta = 1, sd = 1, sig.level = 0.05)\n\n\nThere is a trade-off between the significance level (Type I error or false positive) and Type II error or false negative (=1-power) probabilities. Try-\n\n\nCode\npower.t.test(n = 30, delta = 0.5, sd = 1, sig.level = 0.05)\npower.t.test(n = 30, delta = 0.5, sd = 1, sig.level = 0.01)\n\n\nWhen we test many hypothesis in tandem, we are more concerned on the overall or family-wise error rates. The issues of false discovery in science is discussed in a later section.\nP hacking is a phrase used when a particular test or a meta procedure is deliberately chosen either to ensure a low p-value or just to achieve a value below 0.05."
  },
  {
    "objectID": "studyguide/4-inference.html#hypothesis-tests-for-two-groups",
    "href": "studyguide/4-inference.html#hypothesis-tests-for-two-groups",
    "title": "Chapter 4: Statistical Inference",
    "section": "Hypothesis tests for two groups",
    "text": "Hypothesis tests for two groups\nThe null hypothesis is that population means of the two groups are equal (written as \\(H_0:\\mu_1 = \\mu_2\\) or \\(H_0:\\mu_1-\\mu_2=0\\)). The alternative hypothesis is that the population means of the two groups are different (i.e., \\(H_0:\\mu_1\\neq \\mu_2\\) or \\(H_0:\\mu_1-\\mu_2\\neq0\\)). Note that when the two population means are different either \\(\\mu_1&gt;\\mu_2\\) or \\(\\mu_1&lt;\\mu_2\\).\nThe difference in sample means \\(\\bar{y}_{1} -\\bar{y}_{2}\\) can be standardized to give a \\(t\\) statistic:\n\\(t\\) = ((\\(\\bar{y}_{1} -\\bar{y}_{2}\\))- expected)/e.s.e.\nThe test statistic is \\((\\bar{y}_{1} -\\bar{y}_{2})\\), which has the expected value of zero under the null hypothesis.\nThe estimated standard error (e.s.e) of (\\(\\bar{y}_{1} -\\bar{y}_{2}\\)), is obtained in two ways depending on whether it is plausible or not to make the assumption that the variances within the populations are the same, (i.e. \\(\\sigma _{1}^{2} =\\sigma _{2}^{2}\\)). Whether this assumption appears to be tenable or not can be explored using boxplots etc. For the television viewing time example, the variances of the TV viewing times do not appear to be the same for boys and girls. If the variances of the two populations are the same, then we will use a method of combining the individual variances of the groups to form a pooled variance estimate. To do this, we cannot simply average the two variances as the sample sizes may be quite different. A weighted sum is called for to give:\npooled estimate of variance, \\(S_{p}^{2} =w_{1} S_{1}^{2} +w_{2} S_{2}^{2}\\)\nwhere the weights are, \\(w_{1} =\\frac{n_{1}-1}{n_{1} +n_{2}-2}\\) and \\(w_{2} =\\frac{n_{2}-1}{n_{1} +n_{2}-2}\\). Hence the pooled estimate of the standard deviation is given by\n\\(S_p = \\sqrt{ w_1S_{1}^2 + w_2S_{2}^2 }\\) or\n\\[S_{p} =\\sqrt{\\frac{\\left(n_{1}-1\\right)S_{1}^{2} +\\left(n_{2}-1\\right)S_{2}^{2} }{n_{1} +n_{2} -2} }\\] Consequently,\nEstimated standard error (\\(\\bar{y}_{i}\\))=\\(\\frac{S_{p}}{\\sqrt{n_{i}}},~~~i = 1, 2\\)\nso that\nEstimated standard error (\\(\\bar{y}_{1}-\\bar{y}_{2}\\))=\\(S_{p} \\sqrt{1/n_{1}+1/n_{2}}\\)\nIf the variances of the two populations are not the same, then we cannot pool the variances. Hence the estimated standard error for the difference in the two sample means is given by\nEstimated standard error (\\(\\bar{y}_{1}-\\bar{y}_{2}\\))=\\(\\sqrt{\\frac{S_{1}^{2}}{n_{1}} +\\frac{S_{2}^{2}}{n_{2}}}\\)\nThe degrees of freedom for our \\(t\\)-test (called the two-sample \\(t\\) test) depends on whether estimated standard error is based on the pooled variance or not. For the variance pooled case, the \\(df\\) for the \\(t\\)-test is \\(n_{1}+n_{2}-2\\) but becomes smaller for the unpooled case to \\[df=\\frac{\\left(\\frac{S_{1}^{2}}{n_{1}} +\\frac{S_{2}^{2} }{n_{2}} \\right)^{2} }{\\frac{1}{n_{1} -1} \\left(\\frac{S_{1}^{2}}{n_{1}}\\right)^{2} +\\frac{1}{n_{2} -1} \\left(\\frac{S_{2}^{2}}{n_{2} } \\right)^{2}}\\] That is, the \\(df\\) is adjusted according to the ratio of the two variances. The \\(t\\)-test is also approximate when the variances are not pooled and hence it would be advisable to perform a transformation as later outlined in this Chapter.\nHand calculations for the two sample \\(t\\)-test are cumbersome. The test done on computer usually gives us an output which contains the \\(t\\)-statistic value and the associated \\(p\\)-value. Supplementary details such as the standard errors of the sample means, and their difference, associated \\(df\\) etc will also be contained. The R output given below shows the two-sample test results for the TV viewing data set. Here we test whether the true mean TV viewing times are the same for boys and girls.\n\n\nCode\nt.test(TELETIME~SEX, data=tv)\n\n\n\n    Welch Two Sample t-test\n\ndata:  TELETIME by SEX\nt = -0.7249, df = 40.653, p-value = 0.4727\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -462.1384  218.0514\nsample estimates:\nmean in group 1 mean in group 2 \n       1668.261        1790.304 \n\n\nBased on the EDA evidence seen in Figure 6, we may take a conservative stand and prefer the unpooled two-sample \\(t\\)-test (which is also known as Welch Two Sample t-test). The \\(t\\)-value of -0.72 is not unusual as the probability of getting such an extreme value under the null hypothesis is \\(p=0.47\\). In other words, we cannot reject the null hypothesis; we accept it until we have more evidence to the contrary. Hence the conclusion of the \\(t\\)-test is that the mean TV viewing times can be regarded as the same for the population of boys and girls. Alternatively there is no statistically significant gender effect on TV watching for boys and girls of Standards 2 to 4."
  },
  {
    "objectID": "studyguide/4-inference.html#confidence-intervals-for-the-difference-in-means",
    "href": "studyguide/4-inference.html#confidence-intervals-for-the-difference-in-means",
    "title": "Chapter 4: Statistical Inference",
    "section": "Confidence Intervals for the Difference in Means",
    "text": "Confidence Intervals for the Difference in Means\nThe 95% Confidence Interval for the difference \\(\\left(\\mu _{1} -\\mu _{2} \\right)\\) in population means is given by:\ndifference in sample means \\(\\pm t \\times\\) e.s.e .\nOr more specifically,\nInterval estimate for \\((\\mu _1-\\mu _2)\\) = \\((\\bar{y}_{1}-\\bar{y}_{2})\\pm t \\times\\) e.s.e.\\((\\bar{y}_{1} -\\bar{y}_{2})\\).\nBased on the \\(t\\) quantile value of 2.021 for 40 \\(df\\), the CI in the unpooled case is\n\\(-122 \\pm2.021\\times\\sqrt{\\frac{648^{2}}{23}+\\frac{482^{2}}{23}}\\) or \\((-462.3, 218.2)\\)\nThe \\(t\\)-test output for the null hypothesis \\(H_{0}:\\mu _1=\\mu _2\\) (or \\((\\mu _1-\\mu _2)= 0)\\) gives the confidence interval too. Notice that the CI actually includes zero as a possible value. This means that it is possible \\((\\mu_1-\\mu_2)=0\\); so we cannot reject the null hypothesis."
  },
  {
    "objectID": "studyguide/4-inference.html#paired-t-test",
    "href": "studyguide/4-inference.html#paired-t-test",
    "title": "Chapter 4: Statistical Inference",
    "section": "Paired \\(t\\) test",
    "text": "Paired \\(t\\) test\nNote that the two-sample data may be simply paired observations. For instance, a measurement may be made on the left and right eyes of the same person. If observations are paired in some way, a one-sample \\(t\\)-test on the difference \\(\\left(X_{i} ,Y_{i} \\right)\\) will suggest whether the true mean of the differences can be regarded as zero or not. Such a test will be more powerful than a two sample \\(t\\)-test because of the correlation between the paired observations. If the correlation is weak, it is desirable to ignore the pairing variable and perform a two-sample \\(t\\)-test.\nConsider the maths and English test scores of students available in the data set testmarks. These test scores are paired being the scores of the same student. The correlation or linear relationship between the maths and English scores is high; see Figure 7.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/testmarks.RData\",\n  destfile = \"testmarks.RData\")\n\nload(\"testmarks.RData\")\n\n\n\n\nCode\nlibrary(GGally)\nggpairs(testmarks)\n\n\n\n\n\nFigure 7: Relationship between Maths and English scores\n\n\n\n\nThe paired \\(t\\)-test or the one-sample \\(t\\)-test on the difference in scores gives a \\(t\\)-statistic of 0.17 (\\(p\\)-value of 0.868). This means that the true average difference in test scores can be regarded as zero or alternatively the true mean scores of maths and English can be regarded as equal.\n\n\nCode\nt.test(testmarks$Maths, testmarks$English, paired=T)\n\n\n\n    Paired t-test\n\ndata:  testmarks$Maths and testmarks$English\nt = 0.16745, df = 39, p-value = 0.8679\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -4.154646  4.904646\nsample estimates:\nmean difference \n          0.375"
  },
  {
    "objectID": "studyguide/4-inference.html#transformation-and-shape",
    "href": "studyguide/4-inference.html#transformation-and-shape",
    "title": "Chapter 4: Statistical Inference",
    "section": "Transformation and shape",
    "text": "Transformation and shape\nWhen considering data, we may decide to use the measurements as they are, or we may rescale them. For example, we may change them to percentages of the total. As a simple example, consider a town with four stores in which the weekly turnovers are one, two, four and eight thousand dollars. These could be rescaled to percentages of the total (which is 15).\n\n\n\n\n\n\n\n\n\n\nRaw data\n1\n2\n4\n8\n\n\nData in %\n1/15 \\(\\times\\) 100= 6.7%\n2/15 \\(\\times\\) 100 = 13.3%\n4/15 \\(\\times\\) 100= 26.7%\n8/15 \\(\\times\\) 100= 53.3%\n\n\n\nIf you were to compare a dotplot of the original data with a dotplot of the rescaled data you would find that the shape of the data had not changed by this rescaling. That is, the second percent is twice the first and the second weekly turnover is almost twice the first; the third percent is four times the first and twice the second and so on. This is an example of a linear transformation.\nA linear transformation is one that can be described by the formula, \\(y=a+bx\\) for certain constants \\(a\\) and \\(b\\), and where \\(x\\) is the old data and \\(y\\) is the transformed data. The key thing about linear transformations is that they do not change the shape of a dotplot, only the scale. Another linear example is converting temperature data from Fahrenheit (\\(x\\)) to centigrade \\(y = 5(x-32)/9\\). Boxplots of the temperatures would look the same even though the scale was altered.\nAnother way of rescaling the store example would be to calculate the weekly turnover of a store divided by the number of employees in that store, to give weekly turnover per employee. Even though this looks like a linear transformation it is not, since the relative positions of the four stores on a scale would change depending on the number of employees. If we have two or more variables (e.g. turnover, employees) it is often useful to look at ratios like this to seek simple explanations of the data.\n\n\nCode\nrht &lt;- data.frame(RHT=rbeta(1e3, 1,5))\n\np1 &lt;- ggplot(rht) + \n  aes(RHT) +\n  geom_histogram() +\n  xlab(\"\") + ylab(\"\") +\n  ggtitle(\"(a) Needs a Shrinking Transformation\")\n\np2 &lt;- ggplot(rht) +\n  aes(y=RHT, x=\"Right Skewed Data\") +\n  geom_boxplot() +\n  xlab(\"\") + ylab(\"\") +\n  coord_flip()\n\nlht &lt;- data.frame(LHT=rbeta(1e3, 5,1))\n\np3 &lt;- ggplot(lht) +\n  aes(LHT) +\n  geom_histogram() +\n  xlab(\"\") + ylab(\"\") + \n  ggtitle(\"(a) Needs a Stretching Transformation\")\n\np4 &lt;- ggplot(lht) +\n  aes(y=LHT, x=\"Left Skewed Data\") +\n  geom_boxplot() + \n  xlab(\"\") + ylab(\"\") +\n  coord_flip()\n\ngridExtra::grid.arrange(p1, p3, p2, p4, ncol=2) \n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 8: Transformations\n\n\n\n\nIn this chapter, we focus on transformations involving just one variable (though perhaps more than one batch of data on that variable). Our goal is to change the shape of a distribution to make it more symmetric. Consider the two distributions in Figure 8 in which (a) is skewed to the left and (b) to the right. These could be made more symmetric by stretching the large values in (a) but shrinking them in (b)."
  },
  {
    "objectID": "studyguide/4-inference.html#the-ladder-of-powers",
    "href": "studyguide/4-inference.html#the-ladder-of-powers",
    "title": "Chapter 4: Statistical Inference",
    "section": "The Ladder of Powers",
    "text": "The Ladder of Powers\nFrom now on we assume that the data are very skewed, and we wish to transform it, (or, in Tukey’s terms, re-express it) to be as symmetrical as possible. A simple approach considers the data raised to different powers, that is, if the original data are \\(x\\) and the new data are \\(y\\),\n\\[y =\\left\\{\\begin{array}{l} {\n\\text {sign} (\\lambda )x^\\lambda~~~~~ \\lambda \\ne 0} \\\\\n{\\log(x)~~~~~~~~~~~\\lambda = 0} \\end{array} \\right.\\]\nNote that the Greek letter \\(\\lambda\\) is pronounced as lambda. Here, (\\(\\text {sign}(\\lambda)\\)) is +1 if \\(\\lambda&gt;0\\), and \\(\\text {sign}(\\lambda)=-1\\) if \\(\\lambda&lt;0\\), for reasons discussed below. Some special cases of this power transformation are set out below:\n\n\n\n\n\n\n\n\n\nPOWER\nFormula\nName\nResult\n\n\n\n\n3\n\\(x^3\\)\ncube\nstretches large values\n\n\n2\n\\(x^2\\)\nsquare\nstretches large values\n\n\n1\n\\(x\\)\nraw\nNo change\n\n\n1/2\n\\(\\sqrt{x}\\)\nsquare root\nsquashes large values\n\n\n0\n\\(\\log{x}\\)\nlogarithm\nsquashes large values\n\n\n-1/2\n\\(\\frac{-1}{\\sqrt{x}}\\)\nreciprocal root\nsquashes large values\n\n\n-1\n\\(\\frac{-1}{x}\\)\nreciprocal\nsquashes large values\n\n\n\nRaising the data to the power of 1 does not change it at all; as we proceed down or up from 1, the strength of the transformation increases. The special case \\(\\lambda\\)=0 has to be handled differently since \\(x^0=1\\) for all non-zero \\(x\\). Instead we conventionally regard it as being equivalent to taking the natural logarithm because the transformation \\(\\frac{x^{\\lambda } }{\\lambda } -\\frac{1}{\\lambda }\\) is close to the logarithmic transformation if \\(\\lambda\\) is small. The ‘common’ logarithm to base 10 could be used but it just yields a constant multiple of the natural logarithm (ln = log to the base \\(e\\)). Now regarding the \\(\\text {sign}(\\lambda)\\): Notice that with two numbers, say 2 and 5, the reciprocal transformation would yield 0.5 and 0.2 so that, whereas the original numbers are increasing in size the transformed values are decreasing. To keep the order the same we take the negative of the reciprocal values, -0.5 and -0.2. These are again increasing. The same principle holds for all transformations where \\(\\lambda\\) is negative. \\(\\text {sign}(\\lambda)\\) is employed to keep the order the same as the raw data. (Alternatively we could divide by \\(\\lambda\\) which is consistent with the case of power zero that is the logarithm transformation).\n\n\nCode\np1 &lt;- ggplot(rangitikei) +\n  aes(y=vehicle^2, x=\"\") +\n  geom_boxplot() + \n  xlab(\"\") + \n  coord_flip() +\n  ggtitle(\"Square Transformation\")\n\np2 &lt;- ggplot(rangitikei) +\n  aes(y=vehicle, x=\"\") +\n  geom_boxplot() + \n  xlab(\"\") + \n  coord_flip() + \n  ggtitle(\"Raw Data\")\n\np3 &lt;- ggplot(rangitikei) +\n  aes(y=vehicle^.5, x=\"\") + \n  geom_boxplot() +\n  xlab(\"\") +\n  coord_flip() + \n  ggtitle(\"Square-root Transformation\")\n\np4 &lt;- ggplot(rangitikei) + \n  aes(y=log(vehicle), x=\"\") + \n  geom_boxplot() + \n  xlab(\"\") + \n  coord_flip() +\n  ggtitle(\"log Transformation\")\n\ngridExtra::grid.arrange(p1,p3,p2, p4, ncol=2) \n\n\n\n\n\nFigure 9: Effect of Transformations\n\n\n\n\nAs an example, the boxplots in Figure 9 represent the number of vehicles at the two Rangitikei river locations (from rangitikei). The first boxplot shows the square of the number of vehicle, which is highly right-skewed. The second boxplot shows the raw data (vehicle) which are still skewed towards the larger values. So to squash these to make the distribution more symmetric, the ladder of powers suggests we could try a square root transformation, or a stronger one such as the logarithm (or reciprocal root). These are shown in the third and fourth boxplots."
  },
  {
    "objectID": "studyguide/4-inference.html#some-words-of-caution-about-transformations",
    "href": "studyguide/4-inference.html#some-words-of-caution-about-transformations",
    "title": "Chapter 4: Statistical Inference",
    "section": "Some Words of Caution About Transformations",
    "text": "Some Words of Caution About Transformations\n\nIf the data set is small, transformations should be approached with some scepticism, for if more data were available the shape of the distribution may change.\nIt should be kept in mind that there are different levels at which data can be considered such as (a) just a collection of numbers (any reasonable transformation would suffice); (b) referring to physical quantities (certain transformations may make physical sense and allow meaningful interpretations to be made) and (c) outcomes from a certain process (for example, frequencies or counts may often suggest a Poisson distribution for which a square root transformation is suitable). The choice of a transformation may depend on the additional information that is known about the batch of data.\nCommon sense should prevail in this area as a transformation which brings only marginal improvement to symmetry, for example, should be balanced against other drawbacks such as the difficulty of interpreting the results. For example, the logarithm function turns multiplications to additions and powers to multiplications. Hence it turns divisions to subtractions and roots to divisions. Thus the geometric mean becomes the arithmetic mean and the ratio of geometric means becomes the difference between arithmetic means. Therefore reversing the transformation implies that a confidence interval for the difference between the means of the transformed data becomes a confidence interval for the ratio of the geometric means of the two groups of raw data.\nAlthough a transformation may lead to symmetry it may be better to consider other approaches. For example, plotting the data may show that there are at least two subgroups in the data. Transformations do not really solve the problem and we may have to subdivide the data into groups.\nNote that there are several other transformation functions available. For example, transformations such as arcsine are useful for proportion data."
  },
  {
    "objectID": "studyguide/4-inference.html#box-cox-normalising-transformations",
    "href": "studyguide/4-inference.html#box-cox-normalising-transformations",
    "title": "Chapter 4: Statistical Inference",
    "section": "Box-Cox Normalising transformations",
    "text": "Box-Cox Normalising transformations\nA systematic approach to power transformations was developed by Box and Cox (1976) and Box and Cox (1982). Their method produces a log-likelihood curve of possible values for the power \\(\\lambda\\). Without going into details, the higher the curve is for a particular value of \\(\\lambda\\), the more normal the transformed data will be. The plot of the log-likelihood curve of the Box-Cox method applied to the vehicle data is shown in Figure 10.\n\n\nCode\nlibrary(MASS, exclude = 'select')\n\nboxcox(rangitikei$vehicle ~ 1)\ntitle(\"Log-likelihood curve of Box-Cox power parameter\")\n\n\n\n\n\nFigure 10: Box-Cox Transformation\n\n\n\n\nThe curve peaks near zero indicating that a log transformation would be appropriate. In addition to the curve the plot contains a 95% confidence interval for the transformation parameter \\(\\lambda\\). The two vertical dotted lines are the endpoints of the confidence interval for \\(\\lambda\\). In this case the width of the confidence interval is quite small. The wider the confidence interval, the less obvious the choice for \\(\\lambda\\). If the confidence interval contains 1, then there is no need to perform a transformation. Note that the Box-Cox transformation is a normalising transformation. The EDA done in the previous sections relate to symmetry in the data.\nThe Box-Cox method, and other power transformations, should only be applied if the data are strictly positive. If all the data are negative one can of course take the absolute value of the data and then apply the transformation. If, however, only some of the data are not positive, we may add a constant to all of the data (to make the data positive) but the estimated power will vary depending on the constant."
  },
  {
    "objectID": "studyguide/4-inference.html#ranking-and-rank-correlation",
    "href": "studyguide/4-inference.html#ranking-and-rank-correlation",
    "title": "Chapter 4: Statistical Inference",
    "section": "Ranking and rank Correlation",
    "text": "Ranking and rank Correlation\nA nonparametric approach used very frequently, especially in the social sciences, is the Spearman’s Rank Correlation. To calculate it, first rank the \\(X\\) and \\(Y\\) variable, and then obtain usual correlation (the so-called Pearson correlation) coefficient. If there are a great many ties in the ranks then various corrections or modifications to the Spearman method have been suggested, but these are beyond the scope of this course. In principle then, one could simply apply the usual data analysis techniques to the \\(W= \\text {rank}(Y)\\) data and quote the \\(p\\)-values accordingly. We don’t usually do this in simple analyses, for reasons outlined below, but let’s explore this idea for a moment. Figure 16 shows the usual Pearson (lower diagonal) and Spearman rank correlations (upper diagonal) for the trees default R dataset.\n\n\nCode\nggpairs(\n  trees, \n  upper = list(continuous = wrap('cor', method = \"spearman\")), \n  lower = list(continuous = 'cor')\n  )\n\n\n\n\n\nFigure 16: Comparison of Pearson and Spearman rank correlations\n\n\n\n\nIt can be noted that the size of the estimates differ depending on the skew and relationship between the variables. For large samples it would be quite a reasonable approach, as the distribution of \\(W\\) values is symmetrical and therefore the usual data analysis methods - relying on \\(W\\) being Normally distributed - will work pretty well. The main difference to standard hypothesis tests would be that they would need to be expressed in terms of medians, say, rather than means.\nFor example, a two-group hypothesis test would be based on computing \\(W\\) for all the \\(n\\) data values together, and then comparing the mean of the \\(n_1\\) ranks in the first group of observations to the mean of the \\(n_2\\) ranks for the second group of observations. If there is no difference in population medians for the two groups, then we should not be able to reject the hypothesis that the mean \\(W\\) values are the same.\nIn practice there are a couple of complications. One is that for small samples the distribution of \\(W=\\text {rank}(Y)\\) is not Normal because it is discrete, taking only integers or averages of integers. But fortunately mathematical statisticians have long since worked out the exact distribution of \\(W\\) for many simple situations including two-sample tests, one-way ANOVA, two-way ANOVA with balanced numbers, correlation coefficients (e.g. the correlation between \\(\\text {rank}(Y)\\) and \\(\\text {rank}(X)\\)) and some others. So these exact distributions can be used for hypothesis tests, and are available. The second complication is that these exact distributions for \\(W= \\text {rank}(Y)\\) usually depend on the assumption of no ties, i.e. no equal ranks. Since ties do often occur in practice (if only because data are not measured exactly enough) then we need to use methods that are modified or corrected to handle ties. Fortunately the use of a software handles such issues as a matter of course in many cases, so it doesn’t take any extra time or effort on our behalf."
  },
  {
    "objectID": "studyguide/4-inference.html#wilcoxon-signed-rank-test",
    "href": "studyguide/4-inference.html#wilcoxon-signed-rank-test",
    "title": "Chapter 4: Statistical Inference",
    "section": "Wilcoxon signed rank test",
    "text": "Wilcoxon signed rank test\nFor the nonparametric equivalent of a one-sample \\(t\\)-test for \\(H_0:\\mu= \\mu_0\\), we use the Wilcoxon signed rank test for \\(H_0: \\eta=\\eta_0\\) where \\(\\eta\\) (Greek letter ‘eta’) is the population median. Effectively this test is based on rank \\((|Y-\\eta_0|)\\), where the ranks for data with \\(Y&lt;\\eta_0\\) are compared to the ranks for data with \\(Y&gt;\\eta_0\\). If the \\(\\eta_0\\) is in about the right place, then the distances to points above \\(\\eta_0\\) will tend to rank approximately the same as the distances to points below \\(\\eta_0\\). But if median is assumed too low, say, then the distances above \\(\\eta_0\\) will tend to be bigger (ranked higher) than the distances to points below \\(\\eta_0\\). A statistical test (Wilcoxon test) and the associated \\(p\\)-value follow. In theory, this test assumes a continuous symmetric distribution, but a correction is available in the case of ties. The following output shows the two-sample \\(t\\) test and Wilcoxon test results for testing the equality of median number of people for the time of day groups (morning & afternoon).\n\n\nCode\nwilcox.test(rangitikei$people ~ rangitikei$time, conf.int=T)\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact confidence intervals with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  rangitikei$people by rangitikei$time\nW = 30, p-value = 0.007711\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -88.99996 -10.00005\nsample estimates:\ndifference in location \n             -36.46835 \n\n\nThe t.test(rangitikei$people~rangitikei$time) test also gives the same conclusion for the equality of means."
  },
  {
    "objectID": "studyguide/4-inference.html#sign-test",
    "href": "studyguide/4-inference.html#sign-test",
    "title": "Chapter 4: Statistical Inference",
    "section": "Sign test",
    "text": "Sign test\nThere is an additional one-sample test available called the one-sample sign test, which is based on replacing \\(Y\\) not by \\(\\text {rank}(Y)\\) but simply by the sign of \\(Y-\\eta_0\\), i.e. whether it is positive or negative. This replacement represents an additional loss of detail in the data, but also requires no assumptions. The resulting test is based on a binomial distribution. For example, consider the television viewing time data. Suppose we wish to test the hypothesis that children watch 4 hours of television per day on average (1680 minutes per week). The one-sample sign test output follows:\n\n\nCode\nwilcox.test(tv$TELETIME, mu=1680, conf.int=T)\n\n\n\n    Wilcoxon signed rank exact test\n\ndata:  tv$TELETIME\nV = 588, p-value = 0.6108\nalternative hypothesis: true location is not equal to 1680\n95 percent confidence interval:\n 1557.5 1906.5\nsample estimates:\n(pseudo)median \n          1728 \n\n\nNotice the Wilcoxon test has about the same \\(p\\)-value as the Normal-based \\(t\\)-test. However as it assumes symmetry the estimated median is the same as the sample mean. The \\(p\\)-value for the sign test is similar, but not the same as the others, and the estimated median is the same. The Wilcoxon and Sign test procedure can also be used to generate approximate 95% confidence intervals for the median. Note that these are based on the sorted sample data, and so are discrete, so it is usually not possible to get exact 95% confidence intervals. Both intervals are wider than the confidence interval based on the mean. The loss of precision (longer interval) is reasonable as we are making much weaker assumptions."
  },
  {
    "objectID": "studyguide/4-inference.html#wilcoxon-rank-sum-or-mann-whitney-test",
    "href": "studyguide/4-inference.html#wilcoxon-rank-sum-or-mann-whitney-test",
    "title": "Chapter 4: Statistical Inference",
    "section": "Wilcoxon Rank-Sum or Mann-Whitney test",
    "text": "Wilcoxon Rank-Sum or Mann-Whitney test\nAs an alternative to the two-sample \\(t\\)-test is the Wilcoxon Rank-Sum test (also mathematically equivalent to a test known as the Mann-Whitney test). The assumptions of the test are that the data are continuous (or at least ordinal) from populations that have the same shape (e.g. same skewness and same variance) but just (possibly) different medians. For this test, the entire set of responses is ranked together and then the ranks for the first group are compared to the ranks for the second group. The null hypothesis is that the two group medians are the same: \\(H_0: \\eta_1=\\eta_2\\). The following R output shows the Wilcoxon Rank-Sum test results.\n\n\nCode\nkruskal.test(tv$TELETIME ~ factor(tv$SCHOOL))\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  tv$TELETIME by factor(tv$SCHOOL)\nKruskal-Wallis chi-squared = 7.6389, df = 2, p-value = 0.02194\n\n\nAgain since our data are normally distributed we expect to get a similar result for the Mann-Whitney test as for the two-sample \\(t\\)-test of equal means \\(\\mu_1=\\mu_2\\). This is indeed the case."
  },
  {
    "objectID": "studyguide/4-inference.html#bootstrap-methods",
    "href": "studyguide/4-inference.html#bootstrap-methods",
    "title": "Chapter 4: Statistical Inference",
    "section": "Bootstrap methods",
    "text": "Bootstrap methods\nWhen a random sample is taken from a population, the expectation is that it is representative. So why not sample from the sample (i.e. resample) so that the quality of how well the sample is representative can be examined. We cannot gain extra information hugely by ordinary resampling and it is more like moving forward pulling the bootstrap! The methodology bootstrapping or resampling was introduced by Efron (1979). This computational intensive procedure can be implemented very mechanically and simpler. By computing the sampling distribution of a statistic of interest, issues such as its bias and the standard error can be addressed.\nR package boot has many features and several variations (types) of the bootstrap resampling method but harder to use. We will use the resample package instead because it is simpler and also includes simple permutation tests.\nUnder the simple bootstrap method, observations are resampled with replacement from the original sample to create a bootstrap sample. We can then compute a statistic such as the sample mean for this resample. This process can be repeated many times, say 10000, and we form the bootstrap distribution of the statistic. Consider the tv dataset. If we resample TELETIME, and compute the mean television viewing time for each sample, we construct the bootstrap distribution of mean. Using the resample package, we get-\n\n\nCode\nlibrary(resample)\nbootC &lt;- bootstrap(tv$TELETIME, mean)\nbootC\n\n\nCall:\nbootstrap(data = tv$TELETIME, statistic = mean)\nReplications: 10000\n\nSummary Statistics:\n     Observed       SE     Mean    Bias\nmean 1729.283 82.70197 1728.279 -1.0034\n\n\nThis output shows the observed sample mean 1729.283 and the mean of all bootstrap means which is 1729.114. The bias is the difference, which is -0.168. The main advantage of the bootstrap method is that it can quantify the bias that can occur due to sampling. Figure 17 and Figure 18, respectively, show the histogram and the normal quantile plots for the bootstrap means. Obviously the bootstrap means follow normal (due to CLT).\n\n\nCode\nhist(bootC)\n\n\n\n\n\nFigure 17: Histogram of the bootstrap means of TELETIME\n\n\n\n\n\n\nCode\nqqnorm(bootC)\n\n\n\n\n\nFigure 18: QQ plot of the bootstrap means of TELETIME\n\n\n\n\nThere are many versions of bootstrap confidence intervals depending on the way bootstrapping is done. Without going into details, the 95% confidence interval for the true mean viewing time is obtained as follows:\n\n\nCode\nCI.t(bootC)\n\n\n         2.5%    97.5%\nmean 1560.724 1897.841\n\n\nThis interval compares well with the confidence interval using t-distribution found earlier namely (1560.633, 1897.932). The same approach can be taken to construct a confidence interval for the mean of the paired differences and thereby perform a test analogous to paired t-test. See the testsmarks data example given below:\n\n\nCode\ndiffer &lt;- testmarks$Maths-testmarks$English\nbootC &lt;- bootstrap(differ, mean)\nCI.t(bootC)\n\n\n          2.5%    97.5%\nmean -4.131042 4.881042\n\n\nThe parametric (i.e. t-test based) and bootstrap results are very similar. The bootstrap method can obtain better confidence intervals for the mean when the population is skewed because resampling tends to adjust for the skew in the population when captured by the sample well. The bootstrap approach will work well only when reasonably large sample sizes are available because of the inherent uncertainty in the tails of the underlying distribution. Small samples are not sufficient to identify capture the tails of the distribution and hence certain types of inferences involving tail part of distribution will not work well. Parametric assumptions can be made to improve the bootstrap method and this approach is known as parametric bootstrapping. We will not study such methods in this course.\nThe resample package also has options to bootstrap from two vectors. Consider the television viewing times for the boys and girls groupings. We resample from the two groups to perform the two sample test.\n\n\nCode\nbootC &lt;- bootstrap2(tv$TELETIME, statistic=mean, treatment=tv$SEX)\nCI.t(bootC)\n\n\n               2.5%    97.5%\nmean: 1-2 -478.6493 234.5624\n\n\nThe bootstrap test conclusion again agrees with the Welch two-sample t-test conclusion.\nYou will not be examined on the use of permutation and bootstrap tests in the final exam. We may occasionally use this approach for assignments."
  },
  {
    "objectID": "studyguide/4-inference.html#footnotes",
    "href": "studyguide/4-inference.html#footnotes",
    "title": "Chapter 4: Statistical Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBear in mind though that no real population of values (except for synthetic ones, simulated by a computer) is normally distributed in reality. A variable can only be normally distributed in theory. The real world just is what it is. Recall Box’s adage: “All models are wrong, but some are useful”. When we test for a departure of a variable from, say, a normal distribution, we are simply testing whether the normal distribution provides an adequate model for the data.↩︎"
  },
  {
    "objectID": "studyguide/6-single.html",
    "href": "studyguide/6-single.html",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "",
    "text": "“The sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work.” — von Neumann"
  },
  {
    "objectID": "studyguide/6-single.html#displaying-interpreting-the-fitted-model",
    "href": "studyguide/6-single.html#displaying-interpreting-the-fitted-model",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Displaying & Interpreting the Fitted Model",
    "text": "Displaying & Interpreting the Fitted Model\nIt is fairly easy to display fitted simple regression line on a scatter plot; see Figure 2 and the R codes shown below:\n\n\nCode\nggplot(horsehearts) +\n  aes(x=EXTDIA, y=WEIGHT) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE)\n\n\n\n\n\nFigure 2: Simple regression line\n\n\n\n\nThe geoms geom_smooth() or stat_smooth() add the fitted regression line to the plot. For the simple regression of WEIGHT (weights of horses’ hearts) on EXTDIA (diastole exterior width), the fitted simple regression model is \\(\\hat{y}=a+bx\\) For horses heart data, the coefficient estimates are obtained as \\(a\\) = -2.0003 and \\(b\\) = 0.2996. That is, the fitted model is given by \\(fitted~~weight = -2.0003 + 0.2996\\times extdia\\) or after rounding\n\\[fitted~~weight = -2 +0.3\\times extdia\\]\nFor a given extdia \\(\\left(x\\right)\\) value, the expected weight is given by the fitted simple regression equation. For example, the exterior width (during diastole phase) for the \\(39^{th}\\) horse is 15.0mm and fitted weight is therefore\n\\[fitted~~weight =\\hat{y}_{39} = -2 + 0.3 \\times 15 = 2.5\\]\nThe observed weight of its heart \\(\\left(y_{39} \\right)\\) is 4.1kg. For this horse, we obtain the residual as\n\\[e_{39}= \\left(y_{39}-\\hat{y}_{39} \\right) = 4.1- 2.5 = 1.6.\\]\n\\(t\\)-test for Model Parameters\nIt is desirable to use the R package broom to get parts of the regression outputs. The function tidy() extracts the model and the significance testing results; see Table 1.\n\n\nCode\nlibrary(broom)\nlibrary(kableExtra)\n\nsimplereg &lt;- lm(WEIGHT~EXTDIA, data=horsehearts)\n\ntidy(simplereg)\n\n\n\n\n\n\nTable 1: t-tests for model parameters\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.00034\n0.55881\n-3.57965\n0.00085\n\n\nEXTDIA\n0.29963\n0.03877\n7.72793\n0.00000\n\n\n\n\n\n\n\n\nFor the horses hearts data, under the null hypothesis that the true (or population) slope \\(b\\) equals zero (i.e., \\(H_{0}:\\beta =0\\)), the test statistic becomes \\[t=\\frac{b}{s_{b} } =\\frac{0.29963}{0.03877} = 7.728.\\]\nThe \\(t\\)-statistic is clearly large enough to be considered significant for \\((n-2)\\) \\(df\\) and the \\(p\\)-value is close to zero. Thus, we would reject the null hypothesis \\(H_{0} :\\beta =0\\). In other words, the slope coefficient is significantly different from zero and hence the predictor variable extdia explains a significant amount of variation in the response variable weight.\nWe can also carry out a similar \\(t\\)-test for the \\(y\\)-intercept \\(\\alpha\\) based on the \\(t\\)-statistic \\[t = \\frac {a}{s_a}= \\frac{-2.00034}{0.55881}=-3.58.\\] but the main interest is in the slope parameter \\(\\beta\\). See Table 1."
  },
  {
    "objectID": "studyguide/6-single.html#model-summaries",
    "href": "studyguide/6-single.html#model-summaries",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Model Summaries",
    "text": "Model Summaries\nMany of the model summary measures can be obtained for assessing the quality of the fitted model using the glance() function from the broom package (Table 2).\n\n\nCode\nsimplereg |&gt; \n  glance() |&gt; \n  select(r.squared, sigma, statistic, p.value, AIC, BIC)\n\n\n\n\n\n\nTable 2: Model summary measures\n\n\nr.squared\n0.58\n\n\nsigma\n0.74\n\n\nstatistic\n59.72\n\n\np.value\n0.00\n\n\nAIC\n106.83\n\n\nBIC\n112.32\n\n\n\n\n\n\n\n\nHow to interpret entries appearing in the R output Table 2 is explained below. Note that the \\(R^{2}\\) statistic and the residual standard error are two common summary measures for the fitted model. The other measures such as the AIC and BIC shown in in Table 2 are useful for comparison of models, and selecting a best model, which will be discussed later on.\nResidual standard error\nThe size of the residual standard error \\(s_{e}\\), which is called sigma in Table 2, is important for many reasons. In general, we prefer to have \\(s_{e}\\) no more than 5% of \\(\\bar{y}\\) or small compared the range of \\(y\\) data. For the model fitted to horses hearts , the residual standard error is labelled as sigma in Table 2. This value of is rather large (compared to the range of \\(y\\) data), and hence the fitted model may not be good for prediction purposes.\nThe size of \\(s_{e}\\) also controls the size of the standard error of the slope estimate \\(b\\) (and hence its confidence interval).\nR-squared \\(\\left(R^{2} \\right)\\) statistic:\nThe R-Squared statistic, the proportion of the variation explained by the fitted model, is 0.58. This means that 58 percent of the total variation of the weights is explained by the exterior widths (diastole) using the fitted straight line model namely\n\\[\\text {fitted  weight = -2 +0.3}\\times \\text {extdia}\\]\nThe \\(R^{2}\\) value is also known as the coefficient of (multiple) determination. Notice that when there is only one explanatory variable \\(X\\), then the \\(R^{2}\\) is equal to the square of the \\((X,Y)\\) correlation coefficient, i.e. \\(R^{2} =\\left(r_{x,y} \\right)^{2}\\).\nWe usually require that \\(R^{2}\\) be at least 0.5 so that at least half of the variation is explained by the fit. For the Horse data the model \\(R^{2}\\) is not much better than this. However the scatterplot revealed that there may be two different groups of observations in the data and/or the curvature in the data may indicate that a transformation would be advisable so this low \\(R^{2}\\) is not really surprising.\nRemember that there is a difference between a meaningful model and a statistically significant model. An \\(R^{2}\\) of 0.5 or more indicates a meaningful model whereas the \\(t\\)-test for slope indicates a statistically significant model. A statistically significant model may not always be a meaningful model - in this case the significance is high but the \\(R^{2}\\) of 57.6% is barely adequate.\nANOVA and \\(F\\)-test\nTable 2 gives the F-statistic (labelled as just statistic) and the P-value for this F-statistic. The concept behind this statistic is explained below:\nThe variation in a data set can be measured as the sum of the squared deviation from its central value. This Sum of Squares is abbreviated as SS or SumSq in software regression outputs. For a regression model, we have\nI : total SumSq = variation in the observed values about the mean = \\(\\sum \\left(y-\\bar{y}\\right)^{2}\\).\nII : regression SumSq = Variation in the fitted values about the mean = \\(\\sum \\left(\\hat{y}-\\bar{y}\\right)^{2}\\) (Note that fit is denoted by \\(\\hat{y}\\)).\nIII : error or residual SumSq = Variation in the residuals \\(=\\sum \\left(y-\\hat{y}\\right)^{2} =\\sum e^{2}\\).\nThese sums of squares due to regression, error and total etc are usually displayed in the form of a table known as the analysis of variance table, which is usually shortened to ANOVA or anova. A typical ANOVA table for a simple regression model will appear as in Figure 3. Depending on the package used, the ANOVA table may differ slightly in style.\n\n\n\nFigure 3: A typical ANOVA table\n\n\nR does not print the last row while displaying the F-statistic and ANOVA table. The following codes can be used to obtain the ANOVA output for the regression model (Table 3).\n\n\nCode\nsimplereg |&gt; anova()\n\n# or \n\nsimplereg |&gt; anova() |&gt; tidy()\n\n\n\n\n\n\nTable 3: Analysis of Variance Table\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nEXTDIA\n1\n32.731\n32.731\n59.721\n0\n\n\nResiduals\n44\n24.115\n0.548\nNA\nNA\n\n\n\n\n\n\n\n\nEach sum of squares (source of variation) has associated with it a degrees of freedom \\(df\\)). For one explanatory variable, the regression \\(df\\) = 1. The total \\(df\\) is always one less than the sample size, that is \\(n-1.\\) In other words, residual \\(df\\) = \\(n-2\\). From the sums of squares, the variance estimates are calculated as SumSq/df which are called Mean Squares (MeanSq).\nFor the horses’ heart data, we have Regression SumSq = 32.731, Error SumSq= 24.115 and Total SumSq = 56.845. We also have regression \\(df\\) = 1, total \\(df\\) = \\(n-1 = 45\\) and by subtraction residual \\(df= 45-1= 44\\). Dividing the SumSq by the associated \\(df\\), we compute\n\nRegression Mean Sq = 32.731/1 = 32.731\n\nResidual Mean Sq = 24.115/44 = 0.548\n\nTotal Mean Sq = Total SumSq/45 = \\(\\sum(y-\\bar{y})^2/(n-1)=1.263\\)\n\nNote that the Total Mean Sq is nothing but the variance of \\(y\\), which is usually not displayed in the ANOVA table.\nIt is possible to formalise the goodness of fit of the model by carrying out an \\(F\\)-test. The \\(F\\) statistic is formed by the ratio of two estimates of variances, the regression Mean Sq or variance and the error Mean Sq or variance. The distribution of the \\(F\\) statistic is governed by the numerator \\(df\\) and the denominator \\(df\\). For the horses’ heart data, we obtain\n\\[F =\n\\frac{{\\text {regression MeanSq} }}{{\\text {residual MeanSq}}}\n= \\frac{32.731}{0.548} = 59.721 \\]\nwith 1 \\(df\\) for the numerator and 44 \\(df\\) for the denominator.\nThe null hypothesis is that the model does not fit the data well. That is, the model explains too little of the variation in the \\(y\\) values to be significant. This hypothesis is generally rejected whenever the \\(p\\)-value of the \\(F\\)-test is less than 0.05. In ANOVA table, the \\(F\\) statistic is displayed along with the \\(p\\)-value. Here the \\(p\\)-value is the probability of observing an \\(F\\) statistic larger than the computed value. For horses’ heart data, the computed \\(F\\) statistic is 59.721. \\(F\\) statistic is always positive (being the ratio of mean squares) and hence the alternative hypothesis must be one-sided. That is, the \\(p\\)-value is given by \\(\\Pr \\left(F_{1,44} &gt;59.721\\right)\\) which is very close to zero. This means that the null hypothesis is firmly rejected and we conclude that the regression model using the explanatory variable extdia explains a significantly large proportion of the variation of the response variable weight.\nNote that the \\(R^{2}\\) can also be calculated from the ANOVA table\n\\[R^{2} =\\frac{{\\text {regression SumSq}}}{{\\text {total SumSq}}} = \\frac{32.731}{56.845} = 0.5758\\] or alternatively\n\\[R^{2} = 1-\\frac{{\\text {residual SumSq}}}{{\\text {total SumSq}}} = 1 -- \\frac{24.115}{56.845} = 0.5758.\\] The degrees of freedom for residual SumSq and total SumSq are not identical; while they are very close this is not always the case (e.g. in multiple regression which we will meet in the next chapter). Hence we can adjust for this to obtain the adjusted R-Squared (\\(R_{adj}^{2}\\)):\n\\[\n\\begin{aligned}\nR_{adj}^{2}\n&= 1-\\frac{\\left(\\frac{{\\text {residual SumSq}}}{{\\text {residual df}}} \\right)}{\\left(\\frac{{\\text {totalSumSq}}}{{\\text {total df}}} \\right)\\, } \\\\\n&= 1-\\frac{24.115/44}{56.845/45} \\\\\n&=0.5661.\n\\end{aligned}\n\\]\nFor the simple regression, the \\(F\\)-test is equivalent to the previous \\(t\\)-test (and produces exactly the same \\(p\\)-value). In fact when there is only one explanatory variable the two test statistics are related by the equation \\(F\\) = \\(t^2\\). A further relationship between the two is that the residual standard error is the square root of the residual mean square.\nThe summary() function in base R gives rather a bulky output for the fitted regression model particularly when the number of predictors is large.\n\n\nCode\nsimplereg &lt;- lm(WEIGHT~EXTDIA, data=horsehearts) \nsummary(simplereg)\n\n\nThe R package lessR will get you even a bigger output of model quality and summary measures. We wont be covering all of them but only the essential ones obtained by the broom package function glance(). Try-\n\n\nCode\nlibrary(lessR)\n\n\n\nlessR 4.3.0                         feedback: gerbing@pdx.edu \n--------------------------------------------------------------\n&gt; d &lt;- Read(\"\")   Read text, Excel, SPSS, SAS, or R data file\n  d is default data frame, data= in analysis routines optional\n\nLearn about reading, writing, and manipulating data, graphics,\ntesting means and proportions, regression, factor analysis,\ncustomization, and descriptive statistics from pivot tables\n  Enter:  browseVignettes(\"lessR\")\n\nView changes in this and recent versions of lessR\n  Enter: news(package=\"lessR\")\n\nInteractive data analysis\n  Enter: interact()\n\n\n\nAttaching package: 'lessR'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    recode, rename\n\n\nCode\nreg(WEIGHT~EXTDIA, data=horsehearts)"
  },
  {
    "objectID": "studyguide/6-single.html#prediction-and-estimation",
    "href": "studyguide/6-single.html#prediction-and-estimation",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Prediction and Estimation",
    "text": "Prediction and Estimation\nRecall that the fitted value for the weight of a heart with an extdia value of 15.0mm was 2.5kg. This value can be interpreted as the predicted weight of a horse heart with extdia = 15mm, or as the estimated of all horse hearts with extdia = 15mm. The 95% confidence limits for the mean response for the mean weight of horses hearts with extdia = 15mm is given by \\(\\left(2.26kg,2.72kg\\right)\\). However the weight of any individual heart with extdia = 15mm could (with the same confidence) be as low as 0.98kg or as high as 4.00kg (the prediction interval being (0.98kg, 4.00kg). Notice that the approximate PI formula works as well here: \\(s\\) = 0.7403 and so 2.50 \\(\\pm\\) (2 \\(\\times\\) 0.74) = (1.0kg, 4.0kg). Note that manual computation of the prediction intervals is harder, and we prefer to use R for this.\n\n\nCode\n# confidence interval\npredict(simplereg, list(EXTDIA = 15), interval = \"confidence\")\n\n\n       fit      lwr      upr\n1 2.494161 2.264023 2.724299\n\n\n\n\nCode\n# prediction interval\npredict(simplereg, list(EXTDIA = 15), interval = \"prediction\")\n\n\n       fit       lwr      upr\n1 2.494161 0.9845172 4.003805\n\n\nWe can also create a dataset with the desired intervals using the augment() function from the broom package.\n\n\nCode\nhorseCI &lt;- augment(simplereg, interval = \"confidence\")\nhorsePI &lt;- augment(simplereg, interval = \"prediction\")\n\n\nLet’s visualise the confidence and prediction bands for the fitted line on a scatter plot using the geom_ribbon() function; see Figure 4.\n\n\nCode\np1 &lt;- simplereg |&gt; \n  augment(interval = \"confidence\") |&gt;\n  ggplot() +\n  aes(x = EXTDIA) +\n  geom_point(aes(y = WEIGHT)) + \n  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.5) +\n  geom_line(aes(y = .fitted)) +\n  ggtitle(\"Confidence interval\") \n\np2 &lt;- simplereg |&gt; \n  augment(interval = \"prediction\") |&gt;\n  ggplot() +\n  aes(x = EXTDIA) +\n  geom_point(aes(y = WEIGHT)) + \n  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.5) +\n  geom_line(aes(y = .fitted)) +\n  ggtitle(\"Prediction interval\") +\n  theme_light()\n\nlibrary(patchwork)\n\np1/p2\n\n\n\n\n\nFigure 4: Confidence intervals and prediction intervals\n\n\n\n\nThe R package visreg readily shows the confidence bands too. Try-\n\n\nCode\nlibrary(visreg)\nvisreg(lm(WEIGHT~EXTDIA, data=horsehearts),)"
  },
  {
    "objectID": "studyguide/6-single.html#improving-simple-regression",
    "href": "studyguide/6-single.html#improving-simple-regression",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Improving Simple Regression",
    "text": "Improving Simple Regression\nIf the fitted simple regression is rather poor, what can be done?\n(a) Use a different predictor variable\nFor this Chapter example, we could choose another one of the ultra-sound measurements to predict the weight of the horses heart.\n(b) Transform the \\(Y\\) variable\nWe could choose a transformation which makes physical sense. For example, we could argue that the weight of the heart should be closely related to the volume of the heart and volume is related to the product of three lengths or any one length cubed. Rather than cubing the \\(X\\) variable we usually transform the response variable \\(Y\\). Alternatively we might choose a transformation based on statistical grounds. Our previous discussion suggests that a shrinking transformation should be used, so suppose we take the logarithm of \\(Y\\). The distribution of weights is compared before and after transformation in Figure 13.\n\n\n\n\n\nFigure 13: Comparison of raw and log-transformed Weight data\n\n\n\n\nWe can see the effect of logarithmic transformation from the boxplot in Figure 13. Clearly the distribution has become more symmetric. In order to make the distribution even more symmetric we might also try a power transformation as shown in Figure 13. Here we have applied the negative reciprocal cubic root transformation \\(-\\frac {1}{Y^{1/3}}\\) which makes more physical sense. This yields a slightly symmetric distribution. A minimal summary output of the regression of \\(WEIGHT^{1/3}\\) on EXTDIA is shown in Table 4:\n\n\nCode\nlm(WEIGHT^(1/3)~EXTDIA, data=horsehearts) |&gt; \n  glance() |&gt; \n  select(r.squared, sigma, statistic, p.value, AIC, BIC)|&gt; \n  mutate_if(is.numeric, round,3) |&gt; \n  t() |&gt; \n  kable() |&gt; \n  kable_classic(full_width = F) \n\n\n\n\nTable 4: Cubic Root Transformed Response Model Summary\n\n\nr.squared\n0.616\n\n\nsigma\n0.128\n\n\nstatistic\n70.498\n\n\np.value\n0.000\n\n\nAIC\n-54.588\n\n\nBIC\n-49.102\n\n\n\n\n\n\n\n\nThis output shows an improvement in the \\(R^{2}\\) value. However, for technical reasons one cannot meaningfully compare \\(R^{2}\\) values for the raw and transformed data. The estimated standard deviation of the residuals is also meaningless when comparing raw and transformed data (recall that this quantity is the square root of the residual MeanSq). There is really only one way to confirm whether a transformed model is better than the original model and that is by analysing the residuals. Only if the residuals are better behaved, in that they comply more closely with the regression assumptions, can one claim that the transformed model is preferable. This is a very important point.\nIn the next Chapter, we will cover more on AIC and BIC values shown in@tbl-transweightreg but the simple thumb rule is to opt a model with the smallest AIC or BIC while we go for a model with the largest log-likelihood.\nThese values also support the model based on the negative reciprocal cubic root transformed data.\n(c) Add other explanatory variables to the model\nThis will cause the \\(R^{2}\\) to increase (although we may decide that the increase is not worth the effect of making the model more complicated). We will study multiple regression models having two or more predictors in the next chapter."
  },
  {
    "objectID": "studyguide/6-single.html#cross-validation-cv",
    "href": "studyguide/6-single.html#cross-validation-cv",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Cross Validation (CV)",
    "text": "Cross Validation (CV)\nThis technique is commonly employed for validating models for prediction purposes. The available data is split randomly into \\(k\\) (equal) folds (parts), often by resampling. A model is fitted for the \\((k-1)\\) folds of the data, and then the prediction errors are calculated for the fold that was omitted for modelling. This process can be repeated omitting one subset (out of the \\(k\\) subsets) so that all the \\(k\\) subsets contribute to the estimation of prediction accuracy. This exercise is computationally intensive, and hence we will leave it to the software package such as caret, rsample or modelr to perform the cross validation. Consider the simple regression of WEIGHT on EXTDIA done with the horsesheart data. The following R code perform the 5-fold cross validation of the model for prediction purposes and compare the root mean square errors for both the regression and robust regression models.\n\n\nCode\nlibrary(caret)\nlibrary(MASS, exclude = \"select\")\n\nset.seed(123)\n\n# Set up cross validation\nfitControl &lt;- trainControl(method = \"repeatedcv\", \n                           number = 5, \n                           repeats = 100)\n\n# lmfit\nlmfit &lt;-  train(WEIGHT ~ EXTDIA,\n                data = horsehearts, \n                trControl = fitControl, \n                method = \"lm\")\n\n# rlmfit\nrlmfit &lt;- train(WEIGHT ~ EXTDIA, \n                data = horsehearts, \n                trControl = fitControl, \n                method = \"rlm\")\n\n# Extract the RMSE scores\ndfm &lt;- tibble(\n  lm = lmfit |&gt; pluck(\"resample\") |&gt; pull(RMSE),\n  rlm = rlmfit |&gt; pluck(\"resample\") |&gt; pull(RMSE)\n) |&gt; \n  pivot_longer(cols=everything(),\n               names_to = \"Method\",\n               values_to = \"RMSE\")\n  \n# Make plot of RMSE\nggplot(dfm) +\n  aes(x=Method, y=RMSE, col = Method) + \n  geom_boxplot() +\n  coord_flip() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 20: Comparison of Residual Mean Square Error (RMSE) of lm() vs rlm() fits\n\n\n\n\nFigure 20 shows that the robust rlm() fit slightly outperforms the simple regression fit in terms of RMSE. The number of folds fixed can affect the comparison. The choice of k=5 or 10 is usually recommended.\nFigure 21 shows the cross validation RMSEs for the lm() and rlm() fits for the Rangitikei dataset based on modelr package codes. The robust model again perform slightly better but this does not mean the fitted model is the best one for prediction.\n\n\nCode\nlibrary(purrr)\nlibrary(modelr)\n\nset.seed(123)\n\n# Set up cross validation\ncv2 &lt;- crossv_mc(rangitikei, 500)\n\n# Fit the models\nlm_models &lt;- map(cv2$train, ~ lm(people ~ vehicle, data = .))\nrlm_models &lt;- map(cv2$train, ~ rlm(people ~ vehicle, data = .))\n\n# Extract the RMSE scores\ndfm &lt;- tibble(\n  lm = map2_dbl(lm_models, cv2$test, rmse),\n  rlm = map2_dbl(rlm_models, cv2$test, rmse)\n) |&gt; \n  pivot_longer(cols=everything(),\n               names_to = \"Method\",\n               values_to = \"RMSE\")\n\n# Make a plot\nggplot(dfm) +\n  aes(x=Method, y=RMSE, col = Method) + \n  geom_boxplot() +\n  coord_flip() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFigure 21: Comparison of RMSEs of lm() and rlm() fits under cross validation"
  },
  {
    "objectID": "studyguide/6-single.html#main-points",
    "href": "studyguide/6-single.html#main-points",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Main points",
    "text": "Main points\nConcepts and practical skills you should have at the end of this chapter:\n\nUnderstand and be able to perform a simple linear regression on bivariate related data sets\nUse scatter plots or other appropriate plots to visualize the data and regression line\nSummarize regression results and appropriate tests of significance. Interpret these results in context of your data\nExamine residual diagnostic plots and test assumptions, then perform appropriate transformations as necessary\nUse a regression line to predict new data and explain confidence and prediction intervals\nUnderstand and explain the concepts of robust regression modeling, Tukey Line, and cross-validation."
  },
  {
    "objectID": "studyguide/6-single.html#models-related-to-simple-regression-model",
    "href": "studyguide/6-single.html#models-related-to-simple-regression-model",
    "title": "Chapter 6: Models with a Single Continuous Predictor",
    "section": "Models Related to Simple Regression Model",
    "text": "Models Related to Simple Regression Model\nFor some applications, the true \\(y\\)-intercept \\(\\alpha\\) is known to be zero. An example is when a model is fitted with compositional characteristics as predictors such as percentage fat, protein, moisture in milk powder. The sum of these compositional characteristics is always 100% and this constraint removes the intercept due to theoretical reasons. In such cases, we will be fitting only the slope(s). When the model without the \\(y\\)-intercept is the true or correct model, both the fitted models (with and without the \\(y\\)-intercept) provide unbiased estimates of the true parameters. However the model without the intercept will provide precise estimates. This regression model without the intercept must be used with caution because a very strong assumption is made on the true \\(y\\)-intercept. The first differences of certain time series data may be analysed with a model having no intercept term.\nFor some applications such as measurement system analysis in metrology orthogonal regression models are fitted because both X and Y variables are subject to measurement errors. In the least squares method, we minimise the sum of the squared vertical distances between the actual Y and fitted Y values. For orthogonal regression, we consider the perpendicular distances instead. This approach is a subset of a theory known as principal components. The orthogonal regression approach was popularised by Deming for industrial applications and hence this fit is also known as Deming regression. R software packages (such as MethComp, deming and mcr) will fit orthogonal regression models. Figure 22 shows the Deming regression model to testmarks data but you do not have to perform this regression for this course.\n\n\nCode\nmy.pca &lt;- prcomp( ~ Maths+English, data=testmarks)\n\nslp &lt;- my.pca$rotation[2,1] / my.pca$rotation[1,1]\nintr &lt;- my.pca$center[2] - slp*my.pca$center[1]\n\nmy.caption = paste(\"Slope=\", round(slp, 2), \"  \", \"Intercept\", round(intr, 2))\n\nggplot(testmarks) + \n  geom_point(aes(x=Maths,y=English)) + \n  geom_abline(intercept = intr, slope = slp) + \n  ggtitle(\"Orthogonal Regression\") + \n  labs(caption = my.caption)\n\n\n\n\n\nFigure 22: Orthogonal regression\n\n\n\n\nA number of relationships between variables can be linearised by transformations. For example, the relationship \\(y=\\alpha x^{\\beta }\\) is linearised by taking \\(\\log\\) on both sides as \\[\\log (y)=\\log (\\alpha )+\\beta \\log (x).\\] The methods discussed in this chapter will equally apply for models which can be rewritten as a straight line equation."
  },
  {
    "objectID": "studyguide/8-anova.html",
    "href": "studyguide/8-anova.html",
    "title": "Chapter 8: Analysis of Variance (ANOVA) and Covariance (ANCOVA)",
    "section": "",
    "text": "“To consult a statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.” – Sir RA Fisher"
  },
  {
    "objectID": "studyguide/8-anova.html#another-example",
    "href": "studyguide/8-anova.html#another-example",
    "title": "Chapter 8: Analysis of Variance (ANOVA) and Covariance (ANCOVA)",
    "section": "Another example",
    "text": "Another example\nConsider the pine tree data discussed earlier. Let us model the Top circumference (numerical response) using Area variable (which is categorical). A plot of the raw data and mean circumference along with the associated 95% confidence intervals are shown in Figure 13.\n\n\nCode\ndownload.file(\n  url = \"http://www.massey.ac.nz/~anhsmith/data/pinetree.RData\",\n  destfile = \"pinetree.RData\")\n\nload(\"pinetree.RData\")\n\n\n\n\nCode\npinetree |&gt; \n  ggplot() +\n  aes(x = Area, y = Top, color = Area) + \n  geom_jitter(width = 0.15, height = 0, alpha = .6) + \n  stat_summary(fun = \"mean\", \n               geom = \"point\", \n               size = 3, \n               position = position_nudge(x = 0.3)\n               ) + \n  stat_summary(fun.data = \"mean_cl_normal\", \n               geom = \"errorbar\", \n               size = 0.75, width = 0.075, \n               position = position_nudge(x = 0.3)\n               ) \n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 13: Interval Plot\n\n\n\n\nFor the three different areas of the forest, we define the following three indicator variables,\n\\[\\begin{array}{cccc}\nI_1 & = & 1 & \\text{for Area 1}\\\\\n& & 0& \\text{for Areas 2, and 3}\n\\end{array}\\] \\[\\begin{array}{cccc}\nI_2 & = & 1 & \\text{for Area 2}\\\\\n& & 0& \\text{for Areas 1, and 3}\n\\end{array}\\] \\[\\begin{array}{cccc}\nI_3 & = & 1 & \\text{for Area 3}\\\\\n& & 0& \\text{for Areas 1, and 2}\n\\end{array}\\]\nNote that we need only any three of the above indicator variables to identify the Area category. For example, with \\(I_1\\) and \\(I_2\\) variables, Area 3 is identified when \\(I_1 = I_2 = 0\\).\nLet us regress the pine tree Top circumference on the indicator variables \\(I_2\\) and \\(I_3\\) using the following R codes.\n\n\nCode\npinetree1 &lt;- pinetree |&gt; \n  select(Area, Top) |&gt; \n  mutate(I1 = as.numeric(Area == \"1\"),\n         I2 = as.numeric(Area == \"2\"), \n         I3 = as.numeric(Area == \"3\") )\n\nmdl &lt;- lm(Top ~ I2 + I3, data = pinetree1)\n\nlibrary(broom)\n\ntidy(mdl)\n\n\n\n\nCode\npinetree1 &lt;- pinetree |&gt; \n  select(Top, Area) |&gt; \n  mutate(I1 = as.numeric(Area == \"1\"),\n         I2 = as.numeric(Area == \"2\"), \n         I3 = as.numeric(Area == \"3\") )\n\nmdl &lt;- lm(Top ~ I2 + I3, data = pinetree1)\n\ntidy(mdl)\n\n\n\n\n\n\nTable 4: Regression of Top Circumference on Area Indicator Variables\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n20.025\n1.11375\n17.979805\n0.0000000\n\n\nI2\n-1.955\n1.57508\n-1.241207\n0.2196132\n\n\nI3\n-5.925\n1.57508\n-3.761714\n0.0004002\n\n\n\n\n\n\n\n\nThe following points are to be noted on the regression model shown in Table 4:\n\nThe \\(y\\)-intercept 20.025 is the mean Top circumference for Area 1 (i.e. the mean response for the omitted predictor category).\nThe mean Top circumference 2 and 3 can be found from the fitted model. For Area 2, \\(I_1\\) =0, \\(I_2\\)=1, and \\(I_3\\) =0. Substituting these values in the fitted model, we get \\(20.025 -1.955 \\times 1 -5.925 \\times 0 = 20.025-1.955 = 18.07\\), which is mean Top circumference for Area 1. For Area 3, the mean Top circumference for Area 3 is \\(20.025 -1.955 \\times 0 -5.925 \\times 1 = 20.025= 14.1\\). In other words, the fitted coefficient of an indicator variable is the difference between the response means of the category indicated by the variable and the ‘base’ category (indicated by the omitted indicator variable).\nThe significance of the slope coefficients implies a significant difference in means. For example, the coefficient of the indicator variable \\(I_2\\) is highly significant. This means that the mean Top circumference for Area 2 is significantly different from the mean Top circumference for Area 1. The negative sign of the coefficient means that Area 2 mean is significantly lower when compared Area 1 mean. The same is true when Area 3 is compared with Area 1.\nIn the above output, a multiple comparison of means is made keeping Area 1 as the base. A comparison of the mean Top circumference for Areas 2 & 3 is not possible with the above regression. We need to omit either \\(I_2\\) or \\(I_3\\) and fit a regression for such a comparison.\n\nIn practice, you will NOT be creating any indicator variables to perform the analysis and R does it for us when we use the lm() function but we have to make sure that categorical factors are stored as factors, not numerical variables.\nThe ANOVA of the regression model shown in Table 5 has a highly significant F value which means that at least one Area is different in terms of mean Top circumference.\n\n\nCode\nlm(Top ~ Area, data=pinetree) |&gt; anova()\n\n\n\n\n\n\nTable 5: Regression ANOVA\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nArea\n2\n364.5903\n182.29517\n7.348015\n0.0014482\n\n\nResiduals\n57\n1414.0995\n24.80876\nNA\nNA"
  },
  {
    "objectID": "studyguide/references.html",
    "href": "studyguide/references.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nReferences"
  },
  {
    "objectID": "workshops/ws01.html",
    "href": "workshops/ws01.html",
    "title": "Getting Started With R",
    "section": "",
    "text": "In this course, we will be using R https://www.r-project.org/, an open-source (i.e., free) software package for data analysis. This software is available for essentially all computing platforms (e.g. Windows, Linux, Unix, Mac) is maintained and developed by a huge community of users including many of the world’s foremost statisticians.\nR is a programming language but you may not be required to do a lot of programming for your course work. R includes functions which enables us to perform a full range of statistical analyses.\nFor installing R software, please visit https://cran.stat.auckland.ac.nz/ and follow the instructions.\nNote that the R software will be sitting in the background in RStudio and you will not be using the standalone version of R in this course.\nRStudio https://www.rstudio.com/products/rstudio/ is an integrated development environment (IDE) for R. It includes a console and a sophisticated code editor. It also contains tools for plotting, history, debugging, and management of workspaces and projects. RStudio has many other features such as authoring HTML, PDF, Word Documents, and slide shows. In order to download RStudio (Desktop edition, open source), go to\nhttps://www.rstudio.com/products/rstudio/download/\nDownload the installation file and run it. Note that RStudio must be installed after installing R.\nR/RStudio can also be used using the cloud platform at https://rstudio.cloud/ after creating a free account but occasionally some of the packages covered in this course may fail to work in the cloud platform.\nIf you open RStudio, you will see something similar to the screen shot shown in Figure 1:\n\n\n\nFigure 1: An RStudio window\n\n\nRStudio has many options, such as uploading files to a server, creating documents, etc. You will be using only a few of the options. You will not be using the menus such as Build, Debug, Profile at all in this course.\nYou can either type or copy and paste the R codes appearing in this section on to the R Script window and run them."
  },
  {
    "objectID": "workshops/ws01.html#grammar-of-graphics",
    "href": "workshops/ws01.html#grammar-of-graphics",
    "title": "Getting Started With R",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nThe main idea behind the grammar of graphics of (Wilkinson 2005) is to mimic the manual graphing approach and define building blocks and combine them to create a graphical display. The building blocks of a graph are:\n\ndata\naesthetic mapping\ngeometric object\ntransformation or re-expression of data\nscales\ncoordinate system\nposition adjustments\nfaceting\n\nIf have not installed ggplot2 or tidyverse, install it with the following commands.\n\n\nCode\ninstall.packages(\"ggplot2\")\n\n\nWe can now load the ggplot2 library with the commands:\n\n\nCode\nlibrary(ggplot2)\n\n\nIn order to work with ggplot2, we must have a data frame or a tibble containing our data. We need to specify the aesthetics or how the columns of our data frame can be translated into positions, colours, sizes, and shapes of graphical elements.\nThe geometric objects and aesthetics of the ggplot2 system are explained below:"
  },
  {
    "objectID": "workshops/ws01.html#aesthetic-mapping-aes",
    "href": "workshops/ws01.html#aesthetic-mapping-aes",
    "title": "Getting Started With R",
    "section": "Aesthetic Mapping (aes)",
    "text": "Aesthetic Mapping (aes)\nIn ggplot land aesthetic means visualisation features or aesthetics. These are\n\nposition (i.e., on the x and y axes)\ncolor (“outside” color)\nfill (“inside” color)\nshape (of points)\nlinetype\nsize\n\nAesthetic mappings are set with the aes() function."
  },
  {
    "objectID": "workshops/ws01.html#geometric-objects-geom",
    "href": "workshops/ws01.html#geometric-objects-geom",
    "title": "Getting Started With R",
    "section": "Geometric Objects (geom)",
    "text": "Geometric Objects (geom)\nGeometric objects or geoms are the actual marking or inking on a plot such as:\n\npoints (geom_point, for scatter plots, dot plots, etc)\nlines (geom_line, for time series, trend lines, etc)\nboxplot (geom_boxplot, for boxplots)\n\nA plot must have at least one geom but there is no upper limit. In order to add a geom to a plot, the + operator is employed. A list of available geometric objects can be obtained by typing geom_&lt;tab&gt; in Rstudio. The following command can also be used which will open a Help window.\nhelp.search(\"geom_\", package = \"ggplot2\")\nConsider the study guide dataset rangitikei.txt (Recreational Use of the Rangitikei river). The first 10 rows of this dataset are shown below:\n\n\n   id loc time w.e cl wind temp river people vehicle\n1   1   1    2   1  1    2    2     1     37      15\n2   2   1    1   1  1    2    1     2     23       6\n3   3   1    2   1  1    2    2     3     87      31\n4   4   2    2   1  1    2    1     1     86      27\n5   5   2    1   1  1    2    2     2     19       2\n6   6   2    2   1  2    1    3     3    136      23\n7   7   1    2   2  2    2    2     3     14       8\n8   8   1    2   1  2    2    2     3     67      26\n9   9   1    1   2  1    3    1     2      4       3\n10 10   2    2   1  2    2    2     3    127      45\n\n\nThe description of the variables is given below:\nloc - two locations were surveyed, coded 1, 2\ntime - time of day, 1 for morning, 2 for afternoon\nw.e - coded 1 for weekend, 2 for weekday\ncl- cloud cover, 1 for &gt;50%, 2 for &lt;50%\nwind- coded 1 through 4 for increasing wind speed\ntemp - temperature, 1, 2 or 3 increasing temp\nriver- murkiness of river in 3 increasing categories\npeople - number of people at that location and time\nvehicle- number of vehicles at that location at that time\n\nThis dataset is downloaded from the web using the following commands.\n\n\nCode\nmy.data &lt;- read.csv(\n  \"https://www.massey.ac.nz/~anhsmith/data/rangitikei.csv\", \n  header=TRUE\n  )\n\n\n\n\nCode\nggplot(data = my.data,\n       mapping = aes(x = vehicle, y = people)\n       ) +\n  geom_point()\n\n\n\n\n\nThe aes part defines the “aesthetics”, which is how columns of the dataframe map to graphical attributes such as x and y position, colour, size, etc. An aesthetic can be either numeric or categorical and an appropriate scale will be used. After this, we add layers of graphics. geom_point layer is employed to map x and y and we need not specify all the options for geom_point.\nThe aes() can be specified within the ggplot function or as its own separate function. I prefer this format.\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point()\n\n\n\n\n\nWe can add a title using labs() or ggtitle() functions. Try-\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  ggtitle(\"No. of people vs No. of vehicles\")\n\n\nor\n\n\nCode\nggplot(my.data)+\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  labs(title = \"No. of people vs No. of vehicles\")\n\n\nNote that labs() allows captions and subtitles.\ngeom_smooth is additionally used to show trends.\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nSimilar to geom_smooth, a variety of geoms are available.\n\n\nCode\nggplot(my.data) + \n  aes(x = factor(wind), y = people) +\n  geom_boxplot()\n\n\n\n\n\nEach geom accepts a particular set of mappings;for example geom_text() accepts a labels mapping. Try-\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people) +\n  geom_point() + \n  geom_text(aes(label = w.e), \n            size = 5)\n\n\nThe faceting option allows a collection of small plots with the same scales. Try-\n\n\nCode\nggplot(my.data) +\n  aes(x=vehicle, y=people) + \n  geom_point() +\n  facet_wrap(~ river) \n\n\n\n\n\nFaceting is the ggplot2 option to create separate graphs for subsets of data. ggplot2 offers two functions for creating small multiples:\n\nfacet_wrap(): define subsets as the levels of a single grouping variable\nfacet_grid(): define subsets as the crossing of two grouping variables\n\nThe following arguments are common to most scales in ggplot2:\n\nname: the first argument gives the axis or legend title\nlimits: the minimum and maximum of the scale\nbreaks: the points along the scale where labels should appear\nlabels: the labels that appear at each break\n\nSpecific scale functions may have additional arguments. Some of the available Scales are:\n\n\n\nScale\nExamples\n\n\n\n\nscale_color_\nscale_color_discrete\n\n\nscale_fill_\nscale_fill_continuous\n\n\nscale_size_\nscale_size_manual\n\n\n\nscale_size_discrete\n\n\n\n\n\n\nscale_shape_\nscale_shape_discrete\n\n\n\nscale_shape_manual\n\n\nscale_linetype_\nscale_linetype_discrete\n\n\n\n\n\n\nscale_x_\nscale_x_continuous\n\n\n\nscale_x_log\n\n\n\nscale_x_date\n\n\nscale_y_\nscale_y_reverse\n\n\n\nscale_y_discrete\n\n\n\nscale_y_datetime\n\n\n\nIn RStudio, we can type scale_ followed by TAB to get the whole list of available scales.\nTry-\n\n\nCode\nggplot(my.data) + \n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point() + \n  scale_x_continuous(name = \"No. of Vehicles\") + \n  scale_y_continuous(name = \"No. of people\") + \n  scale_color_discrete(name = \"Temperature\")\n\n\n\n\n\nThe other coding option is shown below:\n\n\nCode\nggplot(my.data) +\n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point() + \n  xlab(\"No. of Vehicles\") + \n  ylab(\"No. of people\") + \n  labs(colour=\"Temperature\") \n\n\nNote that a desired graph can be obtained in more than one way.\nThe ggplot2 theme system handles plot elements (not data based) such as\n\nAxis labels\nPlot background\nFacet label background\nLegend appearance\n\nBuilt-in themes include:\n\ntheme_gray() (default)\ntheme_bw()\ntheme_minimal()\ntheme_classic()\n\n\n\nCode\np1 &lt;- ggplot(my.data) + \n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point()\n\n\nNote that the graph is assigned an object name p1 and nothing will be printed unless we then print the object p1.\n\n\nCode\np1 &lt;- ggplot(my.data) + \n  aes(x = vehicle, y = people, color = factor(temp)) + \n  geom_point()\n\np1\n\n\n\n\n\nTry-\n\n\nCode\np1 + theme_light()\n\n\n\n\n\n\n\nCode\np1 + theme_bw()\n\n\n\n\n\nSpecific theme elements can be overridden using theme(). For example:\n\n\nCode\np1 + theme_minimal() +\n  theme(text = element_text(color = \"red\"))\n\n\n\n\n\nAll theme options can be seen with ?theme.\nTo specify a theme for a whole document, use\n\n\nCode\ntheme_set(theme_minimal())\n\n\nMinimal graphing can be done using the qplot option that will produce a few standard formatted graphs quickly.\n\n\nCode\nqplot(people, vehicle, data = my.data, colour = river)\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nTry-\nqplot(people, data = my.data)\nqplot(people, fill=factor(river), data=my.data)\nqplot(people, data = my.data, geom = \"dotplot\")\nqplot(factor(river), people, data = my.data, geom = \"boxplot\")\nA cheat sheet for ggplot2 is available at https://www.rstudio.com/resources/cheatsheets/ (optional to download). There are many other packages which incorporate ggplot2 based graphs or dependent on it.\nThe library patchwork allows complex composition arbitrary plots, which are not produced using the faceting option. Try\n\n\nCode\nlibrary(patchwork)\n\np1 &lt;- qplot(people, data = my.data, geom = \"dotplot\")\np2 &lt;- qplot(people, data = my.data, geom = \"boxplot\")\np3 &lt;- ggplot(my.data, aes(x = vehicle, y = people)) + geom_point()\n\n(p1 + p2) / p3 + \n  plot_annotation(\"My title\", caption = \"My caption\")\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`."
  },
  {
    "objectID": "workshops/ws01.html#ggplot-builder",
    "href": "workshops/ws01.html#ggplot-builder",
    "title": "Getting Started With R",
    "section": "ggplot builder",
    "text": "ggplot builder\nA nice R package, known as esquisse is available to build few simple ggplot graphics interactively. This may help in the early stages of learning to use ggplot graphing.\nIf this package is not installed, install it first & then try.\nlibrary(esquisse)\noptions(\"esquisse.display.mode\" = \"browser\")\nesquisse::esquisser(data = iris)\nYou can also load the desired dataset within R studio and select the dataset.\nThe other option is to load a dataset from the course data web folder and then launch esquisse. Try-\nurl1 &lt;- \"https://www.massey.ac.nz/~anhsmith/data/rangitikei.RData\"\ndownload.file(url = url1, destfile = \"rangitikei.RData\")\nload(\"rangitikei.RData\")\nesquisse::esquisser(data = rangitikei, viewer = \"browser\")\nYou can also download the associated R codes or save the graph within the esquisse web app."
  },
  {
    "objectID": "workshops/ws01.html#dplyr",
    "href": "workshops/ws01.html#dplyr",
    "title": "Getting Started With R",
    "section": "dplyr",
    "text": "dplyr\nThe following six functions of dplyr are very useful for data wrangling :\n\nFor selecting columns, use select()\nFor subsetting data, use filter()\nFor re-ordering (e.g. ascending/descending), use arrange()\nFor augmenting new calculated columns, use mutate()\nFor computing summary measures, use summarise()\nFor group-wise computations (e.g. summary measures), use group_by()\n\nThere are many other functions such as transmute() which will add newly calculated columns to the existing data frame but drop all unused columns. The across() function extends group_by() and summarise() functions for multiple column and function summaries. For example, you like to report rounded data in a table, which calls for an operation across both rows and columns."
  },
  {
    "objectID": "workshops/ws01.html#piping",
    "href": "workshops/ws01.html#piping",
    "title": "Getting Started With R",
    "section": "Piping",
    "text": "Piping\n\n\n\n\n\n\nTip\n\n\n\nThe piping operation is a fundamental aspect of computer programming. The semantics of pipes is taking the output from the left-hand side and passing it as input to the right-hand side.\n\n\nThe R package magrittr introduced the pipe operator %&gt;% and can be pronounced as “then”. In RStudio windows/Linux versions, press Ctrl+Shift+M to insert the pipe operator. On a Mac, use Cmd+Shift+M.\nR also has its own pipe, |&gt;, which is an alternative to %&gt;%. I tend to use |&gt;. If you want to change the pipe inserted automatically with Ctrl+Shift+M, find on the menu Tools &gt; Global Options, then click on Code and check the box that says “Use Native Pipe Operator”.\nWe often pipe the dplyr functions, and the advantage is that we show the flow of data manipulation and subsequent graphing. This approach also helps to save memory, and dataframes are not unnecessarily created, a necessity for a big data framework.\nTry the following examples after loading the rangitikei dataset.\nselect()\n\n\nCode\nmy.data &lt;- read.csv(\"https://www.massey.ac.nz/~anhsmith/data/rangitikei.csv\", header=TRUE)\n\nnames(my.data)\n\n\n [1] \"id\"      \"loc\"     \"time\"    \"w.e\"     \"cl\"      \"wind\"    \"temp\"   \n [8] \"river\"   \"people\"  \"vehicle\"\n\n\n\n\nCode\nlibrary(tidyverse)\n\nnew.data &lt;- my.data |&gt; \n  select(people, vehicle)\n\nnames(new.data)\n\n\n[1] \"people\"  \"vehicle\"\n\n\n\n\nCode\nmy.data |&gt; \n  select(people, vehicle) |&gt; \n  ggplot() + \n  aes(x=people, y=vehicle) +\n  geom_point()\n\n\n\n\n\nWe select two columns and create a scatter plot with the above commands.\nfilter()\n\n\nCode\nmy.data |&gt; \n  filter(wind==1) |&gt; \n  select(people, vehicle) |&gt; \n  ggplot() +\n  aes(x=people, y=vehicle) +\n  geom_point()\n\n\n\n\n\nThe above commands filter the data for the low wind days and plots vehicle against people.\narrange()\n\n\nCode\nmy.data |&gt; \n  filter(wind==1) |&gt; \n  arrange(w.e) |&gt; \n  select(w.e, people, vehicle)\n\n\n  w.e people vehicle\n1   1    136      23\n2   1     50      22\n3   1    100      31\n4   1    470     122\n5   2     22      11\n\n\nmutate()\nAssume that a $10 levy is collected for each vehicle. We can create this new levy column as follows.\n\n\nCode\nmy.data |&gt; \n  mutate(levy = vehicle*10) |&gt; \n  select(people, levy) |&gt; \n  ggplot() +\n  aes(x = people, y=levy) +\n  geom_point()\n\n\n\n\n\nNote that the pipe operation was used to create a scatter plot using the newly created column.\nsummarise()\n\n\nCode\nmy.data |&gt; \n  summarise(total = n(), \n            avg = mean(people)\n            )\n\n\n  total      avg\n1    33 71.72727\n\n\nWe obtain the selected summary measures namely the total and the mean number of people. Try-\n\n\nCode\nmy.data |&gt; \n  filter(wind == 1) |&gt; \n  summarise(total = n(), \n            avg = mean(people)\n            )\n\n\n  total   avg\n1     5 155.6\n\n\ngroup_by()\nWe obtain the wind group-wise summaries below:\n\n\nCode\nmy.data |&gt; \n  group_by(wind) |&gt; \n  summarise(total=n(), \n            avg=mean(people))\n\n\n# A tibble: 3 × 3\n   wind total   avg\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     5 156. \n2     2    26  59.7\n3     3     2  19  \n\n\nThere are many more commands such as the transmute function which conserves the only the needed columns. Try\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  transmute(total=n(), \n            avg=mean(people))\n\n\n# A tibble: 33 × 4\n# Groups:   wind, w.e [6]\n    wind   w.e total   avg\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     2     1    18  72.1\n 2     2     1    18  72.1\n 3     2     1    18  72.1\n 4     2     1    18  72.1\n 5     2     1    18  72.1\n 6     1     1     4 189  \n 7     2     2     8  31.8\n 8     2     1    18  72.1\n 9     3     2     1   4  \n10     2     1    18  72.1\n# ℹ 23 more rows\n\n\nA simple frequency table is found using count(). Try-\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  count(temp)\n\n\n# A tibble: 10 × 4\n# Groups:   wind, w.e [6]\n    wind   w.e  temp     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1     1     1\n 2     1     1     3     3\n 3     1     2     3     1\n 4     2     1     1     4\n 5     2     1     2    12\n 6     2     1     3     2\n 7     2     2     2     6\n 8     2     2     3     2\n 9     3     1     2     1\n10     3     2     1     1\n\n\nCode\nmy.data |&gt; \n  group_by(wind, w.e) |&gt; \n  count(temp, river)\n\n\n# A tibble: 16 × 5\n# Groups:   wind, w.e [6]\n    wind   w.e  temp river     n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     1     1     1     1\n 2     1     1     3     3     3\n 3     1     2     3     3     1\n 4     2     1     1     1     1\n 5     2     1     1     2     1\n 6     2     1     1     3     2\n 7     2     1     2     1     3\n 8     2     1     2     2     2\n 9     2     1     2     3     7\n10     2     1     3     3     2\n11     2     2     2     1     2\n12     2     2     2     3     4\n13     2     2     3     2     1\n14     2     2     3     3     1\n15     3     1     2     2     1\n16     3     2     1     2     1\n\n\nThe count() is useful to check the balanced nature of the data when many subgroups are involved."
  },
  {
    "objectID": "workshops/ws01.html#tidyr",
    "href": "workshops/ws01.html#tidyr",
    "title": "Getting Started With R",
    "section": "tidyr",
    "text": "tidyr\nBy the phrase tidy data, it is meant the preferred way of arranging data that is easy to analyse. The principles of tidy data are:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nThe hospital admissions dataset is untidy because it does allocate many columns for a variable.\n\n\nCode\nmy.data &lt;- read.table(\n  \"https://www.massey.ac.nz/~anhsmith/data/hospital.txt\",\n  header=TRUE, sep=\",\")\n  \nhead(my.data)\n\n\n  YEAR PERI NORTH1 NORTH2 NORTH3 SOUTH1 SOUTH2 SOUTH3\n1 1980    1      0      4     27      4     16     27\n2 1980    2      6     11     31      8     18     21\n3 1980    3      6      4     25     20     16     24\n4 1980    4      1     10     31     22     17     20\n5 1980    5      4     16     22     21     30     31\n6 1980    6      3      8     28     31     20     30\n\n\nThe main response variable namely the number of admissions is allocated different columns depending on the North and South locations. This format is also called wide format which can be made into a tidy long format. Try-\n\n\nCode\nlibrary(tidyr)\n\nmy.data |&gt; \n  gather(NORTH1, NORTH2, NORTH3, \n         SOUTH1, SOUTH2, SOUTH3)\n\n\nThe command spread() does the opposite to gather(). The tidyr package many other functions such as unite(), separate() etc to deal with columns. A better approach would be to use the dplyr function pivot_longer(). Try-\n\n\nCode\nmy.data |&gt; \n  pivot_longer(cols = NORTH1:SOUTH3, \n               names_to = \"location\", \n               values_to = \"Admissions\")\n\n\nThe command pivot_wider() does the opposite to pivot_longer()\nThe dplyr package also has functions to deal with two-tables which can be joined either conditionally or unconditionally using commands such as full_join(). For a detailed notes and examples, you may visit https://dplyr.tidyverse.org/articles/two-table.html but we will be using such functions very occasionally in this course.\nThe reshape2 and data.table packages also have functions to do the same task."
  },
  {
    "objectID": "workshops/ws01.html#data-quality-checks",
    "href": "workshops/ws01.html#data-quality-checks",
    "title": "Getting Started With R",
    "section": "Data Quality Checks",
    "text": "Data Quality Checks\nIt is a good idea to check the quality of secondary data sourced from elsewhere. For example, there could be missing values in the dataset. Consider the Telomeres data downloaded from http://www.massey.ac.nz/~anhsmith/data/rsos192136_si_001.xlsx\n\n\nCode\n# #| eval: false\n# url &lt;- \"http://www.massey.ac.nz/~anhsmith/data/rsos192136_si_001.xlsx\"\n# destfile &lt;- \"rsos192136_si_001.xlsx\"\n# # \n# download.file(url, destfile)\n\n\n\n\nCode\nurl &lt;- \"http://www.massey.ac.nz/~anhsmith/data/rsos192136_si_001.xlsx\"\ndestfile &lt;- \"rsos192136_si_001.xlsx\"\n\ncurl::curl_download(url, destfile)\n\n\n\n\nCode\nlibrary(readxl)\nrsos192136_si_001 &lt;- read_excel(\"rsos192136_si_001.xlsx\")\n\n\nThe missingness of data can be quickly explored using many R packages. The downloaded Telomeres dataset contain many missing values.\n\n\nCode\nlibrary(VIM)\n\n\nLoading required package: colorspace\n\n\nLoading required package: grid\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nCode\nres &lt;- rsos192136_si_001 |&gt; \n  aggr(sortVar=TRUE) |&gt; \n  summary() |&gt; \n  pluck(\"combinations\")\n\n\n\n\n\nor\n\n\nCode\nlibrary(naniar)\ngg_miss_var(rsos192136_si_001) \n\n\nThe term Missing completely at random (MCAR) is often used to mean there is there is no pattern to the missing data themselves or alternatively the missingness is not related to any other variable or data in the dataset. In other words, the probability of missingness is the same for all units. So no bias is caused by the missing data, and we can discard cases with missing data when we fit models.\nIn practice, we often find missing data do have a relationship with other variables in the dataset but the actual missing values are random. This situation of data conditionally missing at random is called Missing at random (MAR) data. For a particular survey question, the response rate may differ depending on the respondent’s gender. In this situation, the actual missingness may be random but still related to the gender variable.\nMissing not at random (MNAR) is the pattern when missingness is related to other variables in the dataset, as well as the values of the missing data are not random. In other words, there is a predictable pattern in the missingness. So we cannot avoid the bias when missing cases are omitted.\nThere are also situations such as censoring where we just record a single value without actually measuring the variable of interest.\nImputation of data can be made except for the case of MCAR type. A number of R packages are available for data imputation; see https://cran.r-project.org/web/views/MissingData.html or https://stefvanbuuren.name/fimd/. We may occasionally cover data imputation issue in an assignment question.\nThere are also R packages to perform automatic investigation for data cleaning. Try-\n\n\nCode\nlibrary(dataMaid)\n\nmakeDataReport(rsos192136_si_001, output=\"html\", replace=TRUE)\n\n# or\nlibrary(DataExplorer)\n\ncreate_report(rsos192136_si_001)\n\n\nRule based validation is enabled in the R package validate. The R package janitor has a function get_dupes() to find duplicate entries in the dataset. Cleaner package will allow to clean the variables so that the columns are consistent in terms of the factor, date, numerical variable types. You will be largely using data that are already cleaned for your assignments but be aware that you have to perform data cleaning and perform necessary quality checks before analysis."
  },
  {
    "objectID": "workshops/ws03.html",
    "href": "workshops/ws03.html",
    "title": "Chapter 3 Workshop",
    "section": "",
    "text": "Code\nlibrary(tidyverse)"
  },
  {
    "objectID": "workshops/ws03.html#exercise-3.1",
    "href": "workshops/ws03.html#exercise-3.1",
    "title": "Chapter 3 Workshop",
    "section": "Exercise 3.1",
    "text": "Exercise 3.1\nLet \\(X\\) be a normally distributed random variable with mean of 10 and standard deviation of 4.\nDraw the normal distribution in each case, and shade the area of interest. Then, calculate the following values using R.\n\n\\(\\text{P}(X &lt; 7)\\)\n\\(\\text{P}(8.4 &lt; X &lt; 15.7)\\)\n\\(\\text{P}(X &gt; 17)\\)\nFind \\(x\\) such that \\(\\text{P}(X &lt; x) = 0.2\\) (i.e. the 0.2 quantile). Hint: use the qnorm() function.\n\nAnswers are at the end of this document."
  },
  {
    "objectID": "workshops/ws03.html#exercise-3.2",
    "href": "workshops/ws03.html#exercise-3.2",
    "title": "Chapter 3 Workshop",
    "section": "Exercise 3.2",
    "text": "Exercise 3.2\nFor a standard normal variable \\(z\\) , obtain the area between -1.8 and 2.1.\n\n\nCode\npnorm(2.1, mean=0, sd=1) - pnorm(-1.8, mean=0, sd=1)\n\n\nNote that the mean=0, sd=1 are the defaults for pnorm function, so don’t need to be specified.\n\n\nCode\npnorm(2.1) - pnorm(-1.8)"
  },
  {
    "objectID": "workshops/ws03.html#exercise-3.2-1",
    "href": "workshops/ws03.html#exercise-3.2-1",
    "title": "Chapter 3 Workshop",
    "section": "Exercise 3.2",
    "text": "Exercise 3.2\nPlot the prestige scores data as a histogram and show the theoretical normal curve fitted to the data.\n\n\nCode\nlibrary(tidyverse)\nlibrary(car)\n\nPrestige |&gt; \n  ggplot() + \n  aes(prestige) +\n  geom_histogram(aes(y=after_stat(density)), bins=10) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(Prestige$prestige),\n                sd = sd(Prestige$prestige) ), \n    geom = \"line\")\n\n\nLet’s try a square-root transformation\n\n\nCode\nlibrary(tidyverse)\nlibrary(car)\n\nPrestige |&gt; \n  ggplot() + \n  aes(sqrt(prestige)) +\n  geom_histogram(aes(y=after_stat(density)), bins=10) +\n  stat_function(\n    fun = dnorm,\n    args = list(mean = mean(sqrt(Prestige$prestige)),\n                sd = sd(sqrt(Prestige$prestige)) ), \n    geom = \"line\") +\n  ggtitle(\"Square-root prestige\")"
  },
  {
    "objectID": "workshops/ws03.html#exercise-3.3",
    "href": "workshops/ws03.html#exercise-3.3",
    "title": "Chapter 3 Workshop",
    "section": "Exercise 3.3",
    "text": "Exercise 3.3\nLet’s look at the prestige scores variable to see how well it conforms with a normal distribution.\nFirst, make a normal quantile plot.\n\n\nCode\nPrestige |&gt; \n  ggplot() + \n  aes(sample=prestige) + \n  stat_qq() + \n  stat_qq_line()\n\n\nThe x-axis are theoretical quantiles of a normal distribution; the y-axis are the quantiles of the actual data.\nIf the data conformed perfectly to a normal distribution, the points would lie perfectly along the line.\nThe above plot shows that these data conform pretty well to the normal. There is very often some departure in the ‘tails’ at either end, like there is here. Here’s a plot of data that were actually simulated from a normal distribution for comparison:\n\n\nCode\nset.seed(111)\n\ndata.frame(\n  x = rnorm(\n    n = nrow(Prestige),\n    mean = mean(Prestige$prestige), \n    sd = sd(Prestige$prestige)\n    )\n  ) |&gt; \n  ggplot() + \n  aes(sample = x) + \n  stat_qq() + \n  stat_qq_line()\n\n\n\n\n\n\n\n\n\nNow, we’ll do some tests for whether prestige scores show a “significant” departure from the normal distribution.\nThe null hypothesis is that the data came from a normal distribution. A small p-value (say, &lt; 0.05) would lead us to reject the null hypothesis and conclude that the data are unlikely to have come from a normal distribution. A large p-value (&gt; 0.05) means we have no evidence of non-normality.\nFirst, the Shapiro-Wilk test.\n\n\nCode\nshapiro.test(Prestige$prestige)\n\n\nHere, the null hypothesis is rejected, so the data are unlikely to have come from a normal.\nThe Kolmogorov-Smirnov test can also be used. It differs from the Shapiro-Wilk in that you specify the mean and SD of the distribution (here using the sample mean and SD).\n\n\nCode\nks.test(Prestige$prestige, \n        \"pnorm\", \n        mean(Prestige$prestige), \n        sd(Prestige$prestige) )\n\n\nHere we have a discrepancy. S-W rejected the null hypothesis, and K-S did not. It is well-known that S-W is generally more powerful (i.e., more likely to reject a false null hypothesis).\nAt any rate, I don’t believe in “true” distributions. Would I feel comfortable using a normal here? Possibly. It all depends on the context. Remember, there are no true models, only useful ones.\nWe can try the square-root transformed prestige with .\n\n\nCode\nshapiro.test(sqrt(Prestige$prestige))\n\n\nNo significant departure from normality for the sqrt-transformed data."
  },
  {
    "objectID": "workshops/ws05.html",
    "href": "workshops/ws05.html",
    "title": "Chapter 5 Workshop",
    "section": "",
    "text": "Dataset Toxaemia\nThis dataset is from the vcdExtra package. Two signs of toxaemia, an abnormal condition during pregnancy characterized by high blood pressure (hypertension) and high levels of protein in the urine. If untreated, both the mother and baby are at risk of complications or death. The dataset Toxaemia represents 13384 expectant mothers in Bradford, England in their first pregnancy, who were also classified according to social class and the number of cigarettes smoked per day.\nThe dataset is a 5 x 3 x 2 x 2 contingency table, with 60 observations on the following 5 variables:\nclass - Social class of mother, a factor with levels: 1, 2, 3, 4, 5\nsmoke - Cigarettes smoked per day during pregnancy, a factor with levels: 0, 1-19, 20+\nhyper - Hypertension level, a factor with levels: Low, High\nurea - Protein urea level, a factor with levels: Low, High\nFreq - frequency in each cell, a numeric vector\n\n\nExercise 5.1\nObtain relevant graphical displays for this dataset.\nBar charts-\n\n\nCode\nlibrary(tidyverse)\n\nlibrary(vcdExtra)\ndata(Toxaemia)\n\nToxaemia |&gt; \n  ggplot() + \n  aes(x=smoke, y=Freq, fill=hyper) + \n  geom_bar(stat='identity')\n\n\n\n\nCode\nToxaemia |&gt; \n  ggplot() + \n  aes(x=smoke, y=Freq, fill=hyper) + \n  geom_bar(stat='identity', \n           position = \"dodge\"\n           )\n\n\n\n\nCode\nToxaemia |&gt; \n  ggplot() + \n  aes(x=smoke, y=Freq, fill=hyper) + \n  geom_bar(stat ='identity', \n           position = \"dodge\") + \n  facet_grid(urea ~ ., scales = \"free\")\n\n\nMosaic type charts\n\n\nCode\ntab.data &lt;- xtabs(Freq ~ smoke + hyper + urea, data=Toxaemia)\n\nplot(tab.data)\n\n\nCode\nmosaic(tab.data, shade=TRUE, legend=TRUE)\n\n\nCode\nassoc(tab.data, shade=TRUE) \n\n\nCode\nstrucplot(tab.data)\n\n\nCode\nsieve(tab.data)\n\n\nThe full dataset is a 5 x 3 x 2 x 2 contingency table, with 60 observations on the following 5 variables. For this question we will focus on two categorical variables from this dataset, hyper and urea. This forms a 2 x 2 contingency table since these variables each have two levels.\n\n\nCode\n# subset the data\ntox_2 &lt;- Toxaemia |&gt; \n  dplyr::select(hyper, urea, Freq)\n\n\n\n\nCode\n# the tidyverse way\ntox_display &lt;- tox_2 |&gt; \n  pivot_wider(names_from = urea, \n              values_from = Freq,\n              values_fn = sum) |&gt;\n  column_to_rownames( var = \"hyper\") # make values of hyper column row names\n\ntox_display\n\n\n     High  Low\nHigh  665 2715\nLow   589 9415\n\n\nCode\n# xtabs() \n\n\nTwo signs of toxaemia, are high blood pressure (hypertension) and high levels of protein in the urine. We want to ask if in our sample of expectant mothers in Bradford, England, is high blood pressure related to high protein levels? If these two variables are associated this may indicate the presence of toxaemia in the sample, if they are independent toxaemia may not be present.\nWe can test this question using a Chi-squared test.\nThe null hypothesis of the chi-squared these is that the two variables are independent and the alternative hypothesis is that the two variables are not independent.\nOur null hypothesis is that Hypertension level and the Protein urea level in expectant mothers in Bradford, England are independent.\nOur alternative hypothesis that Hypertension level and the Protein urea level in expectant mothers in Bradford, England are not independent.\nSet our alpha = 0.05\n\n\nCode\nchisq.test(tox_display)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tox_display\nX-squared = 563.9, df = 1, p-value &lt; 2.2e-16\n\n\nSince our p-value is less than our alpha level we reject the null hypothesis and conclude that the two variables (hyper & urea) are not independent. We found evidence of an association between hypertension levels and protein in urine levels in our sample of expectant mothers in in Bradford, England.\nWe can see the expected counts\n\n\nCode\nchisq.test(tox_display)$expected\n\n\n         High      Low\nHigh 316.6856 3063.314\nLow  937.3144 9066.686\n\n\nCode\n# compared to our observed\ntox_display\n\n\n     High  Low\nHigh  665 2715\nLow   589 9415\n\n\nCode\n# total counts 13384\n\n\n\n\nExercise 5.2\nThe genetic information of an organism is stored in its Deoxyribonucleic acid (DNA). DNA is a double stranded helix made up of four different nucleotides. These nucleotides differ in which of the four bases Adenine (A), Guanine (G), Cytosine (C), or Thymine (T) they contain. Nucleotides combine to form amino acids which are the building blocks of proteins. Simply put, three nucleotides form an amino acid and the specific order of a combination dictates what amino acid is formed. A simple pattern that we may want to detect in a DNA sequence is that of the nucleotide at position i+1 based on the nucleotide at position i. The nucleotide positional data collected by a researcher in a particular case is given in the following table:\n\n\n\ni\\(i+1)\nA\nC\nG\nT\n\n\n\n\nA\n622\n316\n328\n536\n\n\nC\n428\n262\n204\n306\n\n\nG\n354\n294\n174\n266\n\n\nT\n396\n330\n382\n648\n\n\n\nPerform a test of association and then obtain the symmetric plot.\n\n\nCode\ntabledata &lt;- data.frame(\n  A = c(622, 428, 354, 396),\n  C = c(316, 262, 294, 330),\n  G = c(328, 204, 174, 382),\n  T = c(536, 306, 266, 648), \n  row.names = c(\"A\", \"C\", \"G\", \"T\")\n  )\n\n\n\n\nCode\nchisq.test(tabledata)$exp\n\n\n         A        C        G        T\nA 554.8409 370.5104 335.3705 541.2781\nC 369.4834 246.7328 223.3322 360.4516\nG 334.9983 223.7044 202.4879 326.8094\nT 540.6774 361.0523 326.8094 527.4608\n\n\nCode\nchisq.test(tabledata)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tabledata\nX-squared = 153.21, df = 9, p-value &lt; 2.2e-16\n\n\nCode\nchisq.test(tabledata, simulate.p.value = T)\n\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  tabledata\nX-squared = 153.21, df = NA, p-value = 0.0004998\n\n\n\n\nCode\n# if there is an association we can examine patterns \nlibrary(MASS)\ncorresp(tabledata)\n\n\nFirst canonical correlation(s): 0.1443355 \n\n Row scores:\n         A          C          G          T \n-0.1921802 -0.8894387 -1.0334109  1.4453224 \n\n Column scores:\n         A          C          G          T \n-1.1304512 -0.6952989  0.8139424  1.1304056 \n\n\n\n\nCode\nplot(corresp(tabledata, nf=2))\nabline(v=0)\nabline(h=0)\n\n\n\n\nCode\n#or\nlibrary(FactoMineR)\nCA(tabledata)\n\n\n**Results of the Correspondence Analysis (CA)**\nThe row variable has  4  categories; the column variable has 4 categories\nThe chi square of independence between the two variables is equal to 153.2146 (p-value =  1.902013e-28 ).\n*The results are available in the following objects:\n\n   name              description                   \n1  \"$eig\"            \"eigenvalues\"                 \n2  \"$col\"            \"results for the columns\"     \n3  \"$col$coord\"      \"coord. for the columns\"      \n4  \"$col$cos2\"       \"cos2 for the columns\"        \n5  \"$col$contrib\"    \"contributions of the columns\"\n6  \"$row\"            \"results for the rows\"        \n7  \"$row$coord\"      \"coord. for the rows\"         \n8  \"$row$cos2\"       \"cos2 for the rows\"           \n9  \"$row$contrib\"    \"contributions of the rows\"   \n10 \"$call\"           \"summary called parameters\"   \n11 \"$call$marge.col\" \"weights of the columns\"      \n12 \"$call$marge.row\" \"weights of the rows\"         \n\n\n\nMore R code examples are here"
  },
  {
    "objectID": "workshops/ws07.html",
    "href": "workshops/ws07.html",
    "title": "Chapter 7 Workshop",
    "section": "",
    "text": "Code\nlibrary(tidyverse)"
  },
  {
    "objectID": "workshops/ws07.html#load-data",
    "href": "workshops/ws07.html#load-data",
    "title": "Chapter 7 Workshop",
    "section": "Load data",
    "text": "Load data\n\n\nCode\nhh &lt;- read_csv(\"https://www.massey.ac.nz/~anhsmith/data/horsehearts.csv\")"
  },
  {
    "objectID": "workshops/ws07.html#pairs-plot",
    "href": "workshops/ws07.html#pairs-plot",
    "title": "Chapter 7 Workshop",
    "section": "Pairs plot",
    "text": "Pairs plot\n\n\nCode\nlibrary(GGally)\n\nggpairs(hh)\n\n\n\n\n\nThis plot reveals some very high correlation amongst predictors, particularly (not surprisingly) between the same measurements taken in the different phases (diastolic and systolic). Thus, multicollinearity is likely to be a problem when fitting a multiple regression model. The challenge will be choose the subset of variables that provides the best model fit."
  },
  {
    "objectID": "workshops/ws07.html#lm-output",
    "href": "workshops/ws07.html#lm-output",
    "title": "Chapter 7 Workshop",
    "section": "lm output",
    "text": "lm output\nLet’s start by fitting the full model with all of the available predictor variables.\nThe formula WEIGHT ~ . fits a model with WEIGHT as the response variable and all other variables in the data frame as predictor variables.\n\n\nCode\nmf &lt;- lm(WEIGHT ~ . , data = hh)\n\nsummary(mf)\n\n\n\nCall:\nlm(formula = WEIGHT ~ ., data = hh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.05051 -0.35313  0.01948  0.18674  2.09335 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -1.6311     0.4879  -3.343  0.00184 **\nINNERSYS      0.2321     0.3083   0.753  0.45617   \nINNERDIA      0.5195     0.3954   1.314  0.19654   \nOUTERSYS      0.7114     0.3288   2.164  0.03668 * \nOUTERDIA     -0.5574     0.4510  -1.236  0.22386   \nEXTSYS       -0.2996     0.1346  -2.227  0.03182 * \nEXTDIA        0.3387     0.1475   2.296  0.02716 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6006 on 39 degrees of freedom\nMultiple R-squared:  0.7525,    Adjusted R-squared:  0.7145 \nF-statistic: 19.77 on 6 and 39 DF,  p-value: 1.922e-10\n\n\n\n\n\n\n\n\nInterpreting summary.lm output\n\n\n\nThere is a lot of information in the above summary output to process. Let’s break it down.\n\nThe Call part shows us the command we used to produce the model.\nThe Residuals part gives us a five-number summary of the residuals.\nThe Coefficients table provides the estimates of the model parameters. Specifically, the Estimate column gives us the estimates of the \\(\\beta\\)-coefficients, which can be used to reconstruct the predictive formula. Here, that formula is:\n\\[\\hat y = -1.6311 + 0.2321 × INNERSYS + 0.5195 × INNERDIA + ... + 0.3387 × EXTDIA\\]\nThe Std. Error column gives the standard error for the estimates of the coefficients. That is, the expected average deviation of the estimator of the coefficient (\\(b\\) or \\(\\hat \\beta\\)) from the true population parameter (\\(\\beta\\)). If we were to take many samples (of size \\(n\\)) from this population, a coefficient’s standard error represents how much the estimate is expected to vary (due to sampling variation).\nThe t value is the estimate divided by its standard error, which can be used to test whether the effect of that variable is statistically different from zero. The p-value for this test is provided in the next column, headed Pr(&gt;|t|). If the p-value is low, the observed coefficient estimate is unlikely to have resulted by chance due to sampling variation if the null hypothesis (\\(H_0 : \\beta = 0\\)) is true. Asterisks indicate significant results, as coded by the Signif. codes: given below the table.\nThe final three lines give results for the entire model.\nThe final three lines give results for the entire model.\nThe Residual standard error is an estimate of the standard deviation of the residuals, i.e. the average absolute difference between the predicted values and the actual values. When estimating the weight of horse’s hearts using this model, we would expect to, on average, be wrong by 0.6 kg. The degrees of freedom here are the residual degrees of freedom—the number of independent pieces of information with which the residual standard error was estimated.\nNext, we have the Multiple R-squared, which is the proportion of the total variation in y that is explained by the model. Here, 75% of the variation is explained. The Adjusted R-squared is adjusted for the number of variables included in the model (see lecture slides). It cannot be interpreted in same way as the unadjusted \\(R^2\\) can, but it can be used to compare models.\nFinally, an F-statistic, associated degrees of freedom (DF) , and p-value are provided. This tests whether the model explains a significant proportion of the total variation in y. This can be thought of as testing whether any of the \\(\\beta\\) coefficients in model are non-zero. Here, the p-value is very small so we reject the null hypothesis that all of the \\(\\beta\\) coefficients in model are zero."
  },
  {
    "objectID": "workshops/ws07.html#variance-inflation-factor",
    "href": "workshops/ws07.html#variance-inflation-factor",
    "title": "Chapter 7 Workshop",
    "section": "Variance Inflation Factor",
    "text": "Variance Inflation Factor\nWe mentioned earlier that we were concerned with multicollinearity—correlation among the predictors. A consequence of multicollinearity is that it increases the uncertainty in the estimates of the coefficients—the standard errors of the coefficients are inflated. We can quantify this effect, for each coefficient, with the Variance Inflation Factor (VIF). This is given by the function car::vif()1.\n\n\nCode\ncar::vif(mf)\n\n\n INNERSYS  INNERDIA  OUTERSYS  OUTERDIA    EXTSYS    EXTDIA \n 8.772969  8.602746  7.706493  6.662813 16.046340 21.996455 \n\n\nAccording to a rule of thumb, a VIF &gt; 5 is cause for some concern. A VIF &gt; 10 is definitely problematic. So, we have a problem here.\nThe above VIF values pertain to variances. I find it more intuitive to discuss the square root of the VIF because they relate to the standard errors.\n\n\nCode\nsqrt(car::vif(mf))\n\n\nINNERSYS INNERDIA OUTERSYS OUTERDIA   EXTSYS   EXTDIA \n2.961920 2.933044 2.776057 2.581242 4.005788 4.690038 \n\n\nThese \\(\\sqrt{}\\)VIF values can be interpreted in the following way: the standard error for the effect of INNERSYS is around three times larger because of the presence of the other (correlated) variables in the model. The VIF is greatest for EXTDIA, which is consistent with this variable seeming to have the highest correlations with the other predictors."
  },
  {
    "objectID": "workshops/ws07.html#model-selection",
    "href": "workshops/ws07.html#model-selection",
    "title": "Chapter 7 Workshop",
    "section": "Model selection",
    "text": "Model selection\nStatisticians use the term “parsimonious” to describe a model that contains no more predictors than necessary to adequately model the data—a model that has the right balance of complexity.\nLet’s run a stepwise model selection process to try to find a more parsimonious model than the full model created above. The criterion we will use to assess the quality of the models is Akaike Information Criterion (AIC). Lower AIC values (i.e. closer to \\(-\\infty\\)) are better.\nWe will undertake a stepwise process in individual steps, using the drop1() function.\n\n\nCode\ndrop1(mf)\n\n\nSingle term deletions\n\nModel:\nWEIGHT ~ INNERSYS + INNERDIA + OUTERSYS + OUTERDIA + EXTSYS + \n    EXTDIA\n         Df Sum of Sq    RSS     AIC\n&lt;none&gt;                14.068 -40.500\nINNERSYS  1   0.20434 14.272 -41.836\nINNERDIA  1   0.62273 14.690 -40.507\nOUTERSYS  1   1.68862 15.756 -37.285\nOUTERDIA  1   0.55102 14.618 -40.732\nEXTSYS    1   1.78829 15.856 -36.995\nEXTDIA    1   1.90084 15.968 -36.670\n\n\nWe have taken the full model (all predictors included) and asked what the AIC values2 would be obtained if we dropped each one of the predictors (or none). The lowest AIC score is for the model with INNERSYS removed… so let’s remove it and then use drop1() again.\n\n\nCode\nm2 &lt;- update(mf, ~ . - INNERSYS)\ndrop1(m2)\n\n\nSingle term deletions\n\nModel:\nWEIGHT ~ INNERDIA + OUTERSYS + OUTERDIA + EXTSYS + EXTDIA\n         Df Sum of Sq    RSS     AIC\n&lt;none&gt;                14.272 -41.836\nINNERDIA  1   2.59025 16.862 -36.164\nOUTERSYS  1   2.11154 16.383 -37.489\nOUTERDIA  1   0.46718 14.739 -42.355\nEXTSYS    1   1.59023 15.862 -38.977\nEXTDIA    1   1.70093 15.973 -38.657\n\n\nNow we remove OUTERDIA and repeat.\n\n\nCode\nm3 &lt;- update(m2, ~ . - OUTERDIA)\ndrop1(m3)\n\n\nSingle term deletions\n\nModel:\nWEIGHT ~ INNERDIA + OUTERSYS + EXTSYS + EXTDIA\n         Df Sum of Sq    RSS     AIC\n&lt;none&gt;                14.739 -42.355\nINNERDIA  1    3.2083 17.947 -35.295\nOUTERSYS  1    2.0972 16.836 -38.235\nEXTSYS    1    1.3505 16.090 -40.322\nEXTDIA    1    1.3250 16.064 -40.395\n\n\nThis time, the best model is that with none removed, so the model selection process stops there. Note that this whole process could have been done in a single line of code.\n\n\nCode\nmstep &lt;- step(mf)\n\n\nStart:  AIC=-40.5\nWEIGHT ~ INNERSYS + INNERDIA + OUTERSYS + OUTERDIA + EXTSYS + \n    EXTDIA\n\n           Df Sum of Sq    RSS     AIC\n- INNERSYS  1   0.20434 14.272 -41.836\n- OUTERDIA  1   0.55102 14.618 -40.732\n- INNERDIA  1   0.62273 14.690 -40.507\n&lt;none&gt;                  14.068 -40.500\n- OUTERSYS  1   1.68862 15.756 -37.285\n- EXTSYS    1   1.78829 15.856 -36.995\n- EXTDIA    1   1.90084 15.968 -36.670\n\nStep:  AIC=-41.84\nWEIGHT ~ INNERDIA + OUTERSYS + OUTERDIA + EXTSYS + EXTDIA\n\n           Df Sum of Sq    RSS     AIC\n- OUTERDIA  1   0.46718 14.739 -42.355\n&lt;none&gt;                  14.272 -41.836\n- EXTSYS    1   1.59023 15.862 -38.977\n- EXTDIA    1   1.70093 15.973 -38.657\n- OUTERSYS  1   2.11154 16.383 -37.489\n- INNERDIA  1   2.59025 16.862 -36.164\n\nStep:  AIC=-42.35\nWEIGHT ~ INNERDIA + OUTERSYS + EXTSYS + EXTDIA\n\n           Df Sum of Sq    RSS     AIC\n&lt;none&gt;                  14.739 -42.355\n- EXTDIA    1    1.3250 16.064 -40.395\n- EXTSYS    1    1.3505 16.090 -40.322\n- OUTERSYS  1    2.0972 16.836 -38.235\n- INNERDIA  1    3.2083 17.947 -35.295\n\n\nWhen you run this, the whole three-step process we executed above will print onscreen and the object mstep represents the stepwise-selected model.\nLet’s examine the stepwise model.\n\n\nCode\nsummary(mstep)\n\n\n\nCall:\nlm(formula = WEIGHT ~ INNERDIA + OUTERSYS + EXTSYS + EXTDIA, \n    data = hh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.19400 -0.31530 -0.05037  0.20522  1.92298 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -1.5120     0.4681  -3.230  0.00244 **\nINNERDIA      0.7991     0.2675   2.987  0.00473 **\nOUTERSYS      0.4931     0.2042   2.415  0.02026 * \nEXTSYS       -0.2360     0.1218  -1.938  0.05950 . \nEXTDIA        0.2500     0.1302   1.920  0.06185 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5996 on 41 degrees of freedom\nMultiple R-squared:  0.7407,    Adjusted R-squared:  0.7154 \nF-statistic: 29.28 on 4 and 41 DF,  p-value: 1.554e-11\n\n\nWe still have two variables that are very highly correlated: EXTSYS and EXTDIA. Let’s see if this correlation is problematic, according to the VIF criterion.\n\n\nCode\ncar::vif(mstep)\n\n\n INNERDIA  OUTERSYS    EXTSYS    EXTDIA \n 3.950323  2.981334 13.184774 17.198270 \n\n\nWith VIF scores for these two variables still well above 5, this is certainly a problem. This illustrates an important point: you must not naively accept a model which results from a stepwise selection process. Always scrutinise a model before accepting it.\nWe will force a step where we drop either EXTSYS or EXTDIA.\n\n\nCode\ndrop1(mstep)\n\n\nSingle term deletions\n\nModel:\nWEIGHT ~ INNERDIA + OUTERSYS + EXTSYS + EXTDIA\n         Df Sum of Sq    RSS     AIC\n&lt;none&gt;                14.739 -42.355\nINNERDIA  1    3.2083 17.947 -35.295\nOUTERSYS  1    2.0972 16.836 -38.235\nEXTSYS    1    1.3505 16.090 -40.322\nEXTDIA    1    1.3250 16.064 -40.395\n\n\n\n\nCode\nmstep2 &lt;- update(mstep,  ~ . - EXTDIA)\n\n\nNow, let’s try another step.\n\n\nCode\ndrop1(mstep2)\n\n\nSingle term deletions\n\nModel:\nWEIGHT ~ INNERDIA + OUTERSYS + EXTSYS\n         Df Sum of Sq    RSS     AIC\n&lt;none&gt;                16.064 -40.395\nINNERDIA  1    5.0919 21.156 -29.729\nOUTERSYS  1    3.3053 19.369 -33.788\nEXTSYS    1    0.1091 16.173 -42.083\n\n\nIt seems that the model with EXTSYS removed, leaving only INNERDIA and OUTERSYS, is actually preferable. Once the latter two variables are included, EXTSYS does not add any strength to the model. Let’s make this model and check for further removals, and our VIFs.\n\n\nCode\nmstep3 &lt;- update(mstep2, ~ . - EXTSYS)\ndrop1(mstep3)\n\n\nSingle term deletions\n\nModel:\nWEIGHT ~ INNERDIA + OUTERSYS\n         Df Sum of Sq    RSS     AIC\n&lt;none&gt;                16.173 -42.083\nINNERDIA  1    6.2151 22.388 -29.125\nOUTERSYS  1    3.2771 19.450 -35.596\n\n\n\n\nCode\ncar::vif(mstep3)\n\n\nINNERDIA OUTERSYS \n2.471453 2.471453 \n\n\nIt seems that this model cannot be improved by dropping any further variables. The variables INNERDIA and OUTERSYS, though correlated (r = 0.77), do not exert undue influence on each other in the model.\nTo summarise, let’s see the AIC scores for all the models we’ve made so far.\n\n\nCode\nAIC(mf, m2, m3, mstep, mstep2, mstep3)\n\n\n       df      AIC\nmf      8 92.04262\nm2      7 90.70599\nm3      6 90.18764\nmstep   6 90.18764\nmstep2  5 92.14759\nmstep3  4 90.45888\n\n\nAll of these models have very similar AIC. Statisticians say that AIC scores within, say, 3 points can be considered equivalent, and so often we take the approach of choosing the simplest model (i.e. that with the fewest predictors) of all those within 3 AIC points of the lowest score. In some fields it is common to report all models within 10 AIC points or produce an ensemble model bases on AIC weight (not covered in this course). So, while the mstep3 model isn’t the absolute lowest, it is the simplest model from a bunch of models with roughly equivalent AIC scores. Also, it is a good choice because it doesn’t have the problems with severe multicollinearity found in the other models.\nDon’t worry about the fact that the two functions drop1() and AIC() give different scores. Remember the AIC is a tool for comparing models—the actual scores don’t matter. If you look at the difference in AIC scores between two models from the two functions, they are the same. It also should not be compared on models that have different data sources because it is unit less and only acts to compare the models in a specific set.\nDifference between AIC scores for mstep2 and mstep3 from the drop1(mstep2) output:\n\n\nCode\n-40.395 - (-42.083)\n\n\n[1] 1.688\n\n\nAnd from the AIC(mf, m2, m3, mstep, mstep2, mstep3) output:\n\n\nCode\n92.14759 - 90.45888\n\n\n[1] 1.68871\n\n\nSo now we can choose mstep3 and clean up.\n\n\nCode\nrm(mf, m2, m3, mstep, mstep2) # this code removes these variables\n\n\nLet’s examine the summary for our chosen model.\n\n\nCode\nsummary(mstep3)\n\n\n\nCall:\nlm(formula = WEIGHT ~ INNERDIA + OUTERSYS, data = hh)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.08663 -0.33797 -0.08511  0.32755  1.82971 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -1.4948     0.3728  -4.009 0.000238 ***\nINNERDIA      0.8797     0.2164   4.065 0.000201 ***\nOUTERSYS      0.5612     0.1901   2.952 0.005100 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6133 on 43 degrees of freedom\nMultiple R-squared:  0.7155,    Adjusted R-squared:  0.7023 \nF-statistic: 54.07 on 2 and 43 DF,  p-value: 1.833e-12\n\n\nWe are now explaining 72% of the variation in heart weights with two variables, as opposed to 75% of the variation with six variables in the original full model. Note also that, in the full model, INNERDIA was not significant and OUTERSYS was only weakly significant. In the smaller model, both these predictors were highly significant. Personally, I would definitely prefer the more parsimonious two-variable model, especially if it meant that I had only to take two, rather than six, ultrasound measurements on a thousand horses!\nBut we’re not done yet. We must use some diagnostic tools to examine whether our model meets the assumptions of linear regression before we can accept it."
  },
  {
    "objectID": "workshops/ws07.html#model-diagnostics",
    "href": "workshops/ws07.html#model-diagnostics",
    "title": "Chapter 7 Workshop",
    "section": "Model diagnostics",
    "text": "Model diagnostics\nExamine the usual four diagnostic plots.\n\n\nCode\nplot(mstep3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Residuals-vs-Fitted plot shows a slight decreasing trend in the residuals at low fitted values, but it is only a few points. It might pay, though, to bear in mind that the model is likely to overestimate lower heart weights.\nThe normal Q-Q plot is not too worrying, although there are a few higher-than-expected residuals.\nThe Scale-Location plot shows no strong evidence of heteroscedasticity—the variance appears fairly constant across fitted values.\nAnd, finally, there are no very large values of Cook’s distance or leverage.\nThere are many other diagnostic tools and graphs available, many in the car library, which we do not have time to go into here. If you’re interested, this website is a good place to start: http://www.statmethods.net/stats/rdiagnostics.html."
  },
  {
    "objectID": "workshops/ws07.html#d-plots",
    "href": "workshops/ws07.html#d-plots",
    "title": "Chapter 7 Workshop",
    "section": "3D plots",
    "text": "3D plots\nSince there are three variables involved in this model, it might be useful to examine their relationship using 3D plots. We can include the 2D plane that represents our regression model on the plot, using the following code.\n\n\nCode\nlibrary(scatterplot3d)\n\nhh3d &lt;- scatterplot3d(\n  hh$INNERDIA, \n  hh$OUTERSYS, \n  hh$WEIGHT,\n  type=\"h\", \n  highlight.3d=T,\n  pch=16\n  )\n\nhh3d$plane3d(mstep3)\n\n\n\n\n\nFinally, use plotly to create a dynamic 3D plot which you can rotate using your mouse. Don’t say I never treat you!\n\n\nCode\nlibrary(plotly)\n\nplot_ly(\n  hh, \n  x = ~INNERDIA, \n  y = ~OUTERDIA, \n  z = ~WEIGHT\n  ) |&gt; \n  add_markers()\n\n\n\n\n\n\nI do not recommend 3-D plots for print reports/publications. They are often best viewed interactively. Instead try using colors or bubbles for continuous third variables and shapes or facets for discrete third variables.\nFor example:\n\n\nCode\nhh |&gt; ggplot(aes(y=OUTERDIA, x=INNERDIA, color=WEIGHT))+\n  geom_point()"
  },
  {
    "objectID": "workshops/ws07.html#exercise-7.1",
    "href": "workshops/ws07.html#exercise-7.1",
    "title": "Chapter 7 Workshop",
    "section": "Exercise 7.1",
    "text": "Exercise 7.1\nObtain the matrix plot of the numerical variables education, income, women, and prestige.\n\n\nCode\nlibrary(car)\nlibrary(GGally)\nlibrary(tidyverse)\n\n\n\n\nCode\nPrestige |&gt; \n  select(prestige, education, income, women) |&gt; \n  ggpairs(aes(colour=Prestige$type))\n\n\nObtain their correlation matrix.\n\n\nCode\n# Old style pairs plot\nPrestige |&gt; \n  select(prestige, education, income, women) |&gt;\n  pairs()\n\n\nCode\nPrestige |&gt; \n  select(prestige, education, income, women) |&gt;\n  cor()\n\n\nFit a (full) multiple regression of prestige on education, income, & women.\n\n\nCode\nfull.reg &lt;- lm(prestige ~ education + income + women,\n               data = Prestige)\n\n\nObtain the plots for residual diagnostics. Residual plots\n\n\nCode\nlibrary(ggfortify)\n\nautoplot(full.reg, 1:6)\n\n\n\n\nCode\n# Old style plots\nplot(full.reg, 1) # the argument 1 can be changed up to 6\n\n\nCode\n# or just use\npar(mfrow=c(2,2))\nplot(full.reg)\n\n\nRegression outputs\n\n\nCode\nsummary(full.reg)\n\n\n\nCall:\nlm(formula = prestige ~ education + income + women, data = Prestige)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.8246  -5.3332  -0.1364   5.1587  17.5045 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.7943342  3.2390886  -2.098   0.0385 *  \neducation    4.1866373  0.3887013  10.771  &lt; 2e-16 ***\nincome       0.0013136  0.0002778   4.729 7.58e-06 ***\nwomen       -0.0089052  0.0304071  -0.293   0.7702    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.846 on 98 degrees of freedom\nMultiple R-squared:  0.7982,    Adjusted R-squared:  0.792 \nF-statistic: 129.2 on 3 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nanova(full.reg)\n\n\nAnalysis of Variance Table\n\nResponse: prestige\n          Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \neducation  1 21608.4 21608.4 350.9741 &lt; 2.2e-16 ***\nincome     1  2248.1  2248.1  36.5153 2.739e-08 ***\nwomen      1     5.3     5.3   0.0858    0.7702    \nResiduals 98  6033.6    61.6                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nextractAIC(full.reg)\n\n\n[1]   4.0000 424.1724"
  },
  {
    "objectID": "workshops/ws07.html#exercise-7.2",
    "href": "workshops/ws07.html#exercise-7.2",
    "title": "Chapter 7 Workshop",
    "section": "Exercise 7.2",
    "text": "Exercise 7.2\nPerform stepwise regression analysis of prestige on education, income, & women.\n\n\nCode\nfull.reg = lm(prestige ~ education + income + women,\n              data = Prestige)\n\nstep(full.reg)\n\nstep(full.reg, direction=\"backward\")\n\nstep(full.reg, direction=\"both\")\n\n\nThe function update() is handy for making adjustments to a model. For example, see try the following codes:\n\n\nCode\nm1 = update(full.reg,  . ~ . - women)\n\nsummary(m1)\n\n\nNote that . ~ . - women means that the model is fitted without the women variable.\nFurther options are available in leaps and HH packages (installation commands are given below).\ninstall.packages(\"leaps\", repos = \"https://cran.r-project.org\") install.packages(\"HH\", repos = \"https://cran.r-project.org\")\n\n\nCode\nlibrary(leaps)\n\nmodel = regsubsets(prestige ~ education + income + women, \n                   data = Prestige)\n\nlibrary(HH)\n\nsummaryHH(model)\n\nplot(summaryHH(model))"
  },
  {
    "objectID": "workshops/ws07.html#exercise-7.3",
    "href": "workshops/ws07.html#exercise-7.3",
    "title": "Chapter 7 Workshop",
    "section": "Exercise 7.3",
    "text": "Exercise 7.3\nPerform a polynomial regression of prestige on income.\n\n\nCode\n# Cubic fit\np.model &lt;- lm(prestige ~ poly(income,3),\n              data = Prestige)\n\nsummary(p.model)\n\nextractAIC(p.model)\n\nplot(p.model)\n\n\nCode\nautoplot(p.model)\n\n\n\nMore R code examples are here"
  },
  {
    "objectID": "workshops/ws07.html#footnotes",
    "href": "workshops/ws07.html#footnotes",
    "title": "Chapter 7 Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe car::vif() notation calls the function vif() from within the package car. Alternatively, you can load the package into R with the command library(car). After doing this, all functions in the car package become available, so you can call the function directly as vif(), omitting the car::. Either way, you must have the package installed, of course!↩︎\nNegative values of AIC, as seen here, are uncommon but are nothing to worry about. This occurs when the values of the response variable are small.↩︎"
  },
  {
    "objectID": "slides/Chapter05.html#analysing-frequencies",
    "href": "slides/Chapter05.html#analysing-frequencies",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Analysing frequencies",
    "text": "Analysing frequencies\n\nThis chapter focuses on data that consist of frequencies or counts of occurrences of some events\nWish to compare observed with what we would have expected\nWe will cover the \\(\\chi ^ 2\\) distribution, “Goodness-of-fit” tests, and test of independence"
  },
  {
    "objectID": "slides/Chapter05.html#chi-squared-distribution",
    "href": "slides/Chapter05.html#chi-squared-distribution",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Chi squared distribution",
    "text": "Chi squared distribution\n\n\nContinuous 0 to infinity; starts at 0 because it is a square (a square number can’t be negative)\nThe mean of this distribution is its degrees of freedom (k)\nIt is right skewed, mean greater than median and mode; variance is \\(2k\\)\nShape of the \\(\\chi ^ 2\\) distribution is determined by the degrees of freedom (k), at very high k (90 or greater) \\(\\chi ^ 2\\) distribution resembles the normal distribution\nMain purpose is hypothesis testing, not describing real-world distributions"
  },
  {
    "objectID": "slides/Chapter05.html#examples-of-data-with-one-factor",
    "href": "slides/Chapter05.html#examples-of-data-with-one-factor",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Examples of data with one factor",
    "text": "Examples of data with one factor\nExample 1\n\nA dataset contains 40 males and 50 females.\nHow plausible is the null model of these counts coming from a population with 50% males and 50% females?\nThe expected counts in this case would be 45 males and 45 females."
  },
  {
    "objectID": "slides/Chapter05.html#chi-squared-test-statistic",
    "href": "slides/Chapter05.html#chi-squared-test-statistic",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Chi-squared test statistic",
    "text": "Chi-squared test statistic\n\\[\n\\chi ^{2} =\\sum _{1}^{c}\\frac{\\left({\\rm Observed-Expected}\\right)^{{\\rm 2}} }{{\\rm Expected}}  =\\sum _{1}^{c}\\frac{\\left( O-E\\right)^{2}}{E}\n\\]\nIf the number of categories is \\(c\\), then the degrees of freedom is \\(c-1\\)."
  },
  {
    "objectID": "slides/Chapter05.html#chi-squared-test-statistic-1",
    "href": "slides/Chapter05.html#chi-squared-test-statistic-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Chi-squared test statistic",
    "text": "Chi-squared test statistic\nIf the null hypothesis were true, then the value of \\(\\chi ^{2}\\) calculated from our data is a random value from a Chi-squared distribuion: \\[\n\\chi_{0} ^{2} = \\chi_{c-1} ^{2}\n\\] Once we calculate the test statistic, we can compare our observed value with its distribution under \\(H_{0}\\) to calculate a p-value."
  },
  {
    "objectID": "slides/Chapter05.html#assumptions",
    "href": "slides/Chapter05.html#assumptions",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Assumptions",
    "text": "Assumptions\n\nThe classification of observations into groups must be independent\nNo more than 20% of categories should have expected counts less than 5"
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-for-distributions-example",
    "href": "slides/Chapter05.html#goodness-of-fit-for-distributions-example",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit for distributions example",
    "text": "Goodness of fit for distributions example\nA safety inspector monitors car accidents at a bustling intersection. The inspector enters the counts of monthly accidents.\nNull: The sample data follow the Poisson distribution. Alternative: The sample data do not follow the Poisson distribution.\nNote The Poisson distribution is a discrete probability distribution (integers) that can model counts of events or attributes in a fixed observation space. Many but not all count processes follow this distribution."
  },
  {
    "objectID": "slides/Chapter05.html#tests-of-independence",
    "href": "slides/Chapter05.html#tests-of-independence",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Tests of Independence",
    "text": "Tests of Independence\n\nIn some cases, we have counts of observations cross-classified in terms of two factors.\nWe are generally interested in determining whether or not the two factors are independent.\nIf we consider the observations falling into each category for factor 1, is this distribution consistent across all levels of factor 2? (or vice versa)"
  },
  {
    "objectID": "slides/Chapter05.html#correspondence-analysis",
    "href": "slides/Chapter05.html#correspondence-analysis",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Correspondence Analysis",
    "text": "Correspondence Analysis\nCorrespondence Analysis is an exploratory statistical technique for assessing the interdependence of categorical variables whose data are presented primarily in the form of a two-way table of frequencies\nData: Smoking Status vs. Staff Groupings\n\n\n                 None Moderate Heavy\nJunior employees   18       57    13\nJunior managers     4       10     4\nSecretaries        10       13     2\nSenior employees   25       22     4\nSenior managers     4        5     2\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tabledata\nX-squared = 15.672, df = 8, p-value = 0.04733"
  },
  {
    "objectID": "slides/Chapter05.html#examples-of-data-with-one-factor-1",
    "href": "slides/Chapter05.html#examples-of-data-with-one-factor-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Examples of data with one factor",
    "text": "Examples of data with one factor\nExample 2\n\n\n\nDoes the distribution of rejects of metal castings by causes in a particular week vary from the long-term average counts?\nTreat the long-term average counts as the expected counts.\nCompare observed counts with expected counts.\n\n\n\n\n\nCauses of rejection\nRejects during the week\nLong-term average\n\n\n\n\nsand\n90\n82\n\n\nmisrun\n8\n4\n\n\nshift\n16\n10\n\n\ndrop-\n8\n6\n\n\ncorebreak\n23\n21\n\n\nbroken\n21\n20\n\n\nother\n5\n8"
  },
  {
    "objectID": "slides/Chapter05.html#goodness-of-fit-for-distributions-example-1",
    "href": "slides/Chapter05.html#goodness-of-fit-for-distributions-example-1",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Goodness of fit for distributions example",
    "text": "Goodness of fit for distributions example\n\n\n\nAccidents\n\\(O\\)\n\\(E\\)\n\\((O-E)^2/E\\)\n\n\n\n\n0\n7\n\n\n\n\n1\n8\n\n\n\n\n2\n13\n\n\n\n\n3\n10\n\n\n\n\n&gt;=4\n12\n\n\n\n\nSum\n40\n\n\n\n\n\nConclusion:"
  },
  {
    "objectID": "slides/Chapter05.html#example-4",
    "href": "slides/Chapter05.html#example-4",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\nContext: Porcine Stress Syndrome (PSS) result in pale, soft meat in pigs and under conditions of stress- death.\n\nPresence of PPS is a positive reaction to breathing halothane.\n\nSelective breeding for reducing incidence of PSS\n\n\n\n\n\nPorcine Stress Syndrome (PSS) data\n\n\n\nHalothane.positive\nHalothane.negative\nTotals\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nTotals\n32\n311\n343"
  },
  {
    "objectID": "slides/Chapter05.html#example-5",
    "href": "slides/Chapter05.html#example-5",
    "title": "Chapter 5:Tabulated Counts",
    "section": "Example",
    "text": "Example\n\n\n\nPorcine Stress Syndrome (PSS) data\n\n\n\nHalothane.positive\nHalothane.negative\nTotals\n\n\n\n\nLarge White\n2\n76\n78\n\n\nHampshire\n3\n86\n89\n\n\nLandrace(B)\n11\n73\n84\n\n\nLandrace(S)\n16\n76\n92\n\n\nTotals\n32\n311\n343\n\n\n\n\n\n\n\n\nchisq.test(dt)\n\n\n    Pearson's Chi-squared test\n\ndata:  dt\nX-squared = 16.433, df = 8, p-value = 0.03659"
  },
  {
    "objectID": "slides/Chapter06.html#analysis-of-two-quantitiative-variables",
    "href": "slides/Chapter06.html#analysis-of-two-quantitiative-variables",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Analysis of two quantitiative variables",
    "text": "Analysis of two quantitiative variables\n\nWith paired (related) data (X,Y)\nTwo variables: one (Y) is random (response) and the other (X) is considered fixed (predictor)\nInterested in the functional relationship between these two variables \\(Y=f(X)\\) to predict Y, given X\nInterested in estimates of coefficients"
  },
  {
    "objectID": "slides/Chapter06.html#regression",
    "href": "slides/Chapter06.html#regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Regression",
    "text": "Regression\n\nStatistical Model\n\nFitted Model: \\(\\hat{Y}=a+bX\\) (True Model: \\(Y=\\alpha+\\beta X+\\epsilon\\))\nresidual error: \\(e= Y-\\hat{Y}\\) (\\(\\epsilon\\) is not the same as \\(e\\))"
  },
  {
    "objectID": "slides/Chapter06.html#regression-1",
    "href": "slides/Chapter06.html#regression-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Regression",
    "text": "Regression\n\n\nA regression equation is a function that indicates how the average value of one response variable for given values of one or more predictor variables varies with these predictor variables; that is, \\(E(Y|X_1, X_2, ..., X_k)\\)"
  },
  {
    "objectID": "slides/Chapter06.html#fitting-a-regression",
    "href": "slides/Chapter06.html#fitting-a-regression",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Fitting a regression",
    "text": "Fitting a regression\n\nWant a line that is as ‘close’ as possible to the existing data points we have\nThe method of least squares is employed to obtain the estimates \\(a\\) and \\(b\\)\n\nThe sum of squared residuals is minimized in the least squares method."
  },
  {
    "objectID": "slides/Chapter06.html#variance-explained",
    "href": "slides/Chapter06.html#variance-explained",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Variance Explained",
    "text": "Variance Explained\n\\(R^2\\) is the proportion of variance in \\(y\\) explained by \\(x\\) .\n\nsummary(ols)\n\n\nCall:\nlm(formula = Death ~ Alcohol, data = cirrhosis)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3966 -1.9639  0.2479  2.9884  4.7716 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.318      2.065  -1.123    0.282    \nAlcohol        1.405      0.202   6.954    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.942 on 13 degrees of freedom\nMultiple R-squared:  0.7881,    Adjusted R-squared:  0.7718 \nF-statistic: 48.35 on 1 and 13 DF,  p-value: 1.001e-05"
  },
  {
    "objectID": "slides/Chapter06.html#variance-explained-1",
    "href": "slides/Chapter06.html#variance-explained-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Variance Explained",
    "text": "Variance Explained\n\\(R^2=\\frac{SS~regression}{SS~Total}\\)\n\nSS &lt;- anova(ols) |&gt; \n   tidy() |&gt; \n  select(term:sumsq) |&gt; \n  janitor::adorn_totals()\nSS\n\n      term df    sumsq\n   Alcohol  1 751.4408\n Residuals 13 202.0286\n     Total 14 953.4693\n\n\n\n\\(R^2_{adj}\\) is adjusted to remove the variation that is explained by chance alone\n\\(R^2_{adj}=1-\\frac{MS~Error}{MS~Total}\\)\nwhere:\n\\(MS~Error\\) = \\(frac{SSE}{n-k-1}\\)\n\\(MS~Total\\) = \\(frac{SST}{n-1}\\)\ntherefore \\(R^2_{adj}\\) can also be written as: \\(R^2_{adj}=1-\\frac{(1-R^2)*(n-1)}{(n-k-1)}\\)"
  },
  {
    "objectID": "slides/Chapter06.html#robust-regression-models",
    "href": "slides/Chapter06.html#robust-regression-models",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust regression models",
    "text": "Robust regression models\nRobust regression methods are designed to limit the effect that violations of assumptions by the underlying data-generating process have on regression estimates.\nFor example, least squares estimates for regression models are highly sensitive to outliers: an outlier with twice the error magnitude of a typical observation contributes four (two squared) times as much to the squared error loss, and therefore has more leverage over the regression estimates."
  },
  {
    "objectID": "slides/Chapter06.html#robust-models",
    "href": "slides/Chapter06.html#robust-models",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust models",
    "text": "Robust models\nWe can fit a robust linear model using the functions MASS::rlm() robustbase::lmrob()."
  },
  {
    "objectID": "slides/Chapter06.html#robust-models-1",
    "href": "slides/Chapter06.html#robust-models-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust Models",
    "text": "Robust Models\nTo determine if a robust regression model offers a better fit to the data compared to the OLS model, we can calculate the residual standard error of each model.\nThe residual standard error (RSE) is a way to measure the standard deviation of the residuals in a regression model. The lower the value for RSE, the more closely a model is able to fit the data."
  },
  {
    "objectID": "slides/Chapter06.html#robust-models-2",
    "href": "slides/Chapter06.html#robust-models-2",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Robust Models",
    "text": "Robust Models\n\n#fit least squares regression model\nols &lt;- lm(Death~Alcohol, data=cirrhosis)\n#find residual standard error of ols model\nsummary(ols)$sigma\n\n[1] 3.942164\n\n#fit robust regression model\nrobust &lt;- rlm(Death~Alcohol, data=cirrhosis)\n#find residual standard error of robust model\nsummary(robust)$sigma\n\n[1] 3.417522"
  },
  {
    "objectID": "slides/Chapter06.html#cross-validation-cv-1",
    "href": "slides/Chapter06.html#cross-validation-cv-1",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Cross Validation (CV)",
    "text": "Cross Validation (CV)\n\nSplit the sample data randomly into (equal) k subsets (parts) by resampling.\nFit the model for \\((k − 1\\)) subsets of the data\nPredict for the omitted subsets\nCompare prediction errors\nRepeat for each subsets of the data"
  },
  {
    "objectID": "slides/Chapter06.html#cross-validation-types",
    "href": "slides/Chapter06.html#cross-validation-types",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Cross Validation Types",
    "text": "Cross Validation Types\n\nK-fold\nStratified K-fold\n\nSame as K-fold but splitting data governed by criteria so that each subset has the same proportion of obervations of a given categorical value\n\nLeave One Out (LOO)\n\nLeaves out a single data point, then uses the same process as K-fold CV\n\nR has many packages for cross validation"
  },
  {
    "objectID": "slides/Chapter06.html#residuals",
    "href": "slides/Chapter06.html#residuals",
    "title": "Chapter 6:Models with a Single Predictor",
    "section": "Residuals",
    "text": "Residuals"
  },
  {
    "objectID": "slides/Chapter07.html#full-regression-in-r",
    "href": "slides/Chapter07.html#full-regression-in-r",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Full regression in R",
    "text": "Full regression in R\nPlaces all of the predictors in the model\nEquivalent of throwing everything in and hoping something sticks\n\n\n\nCall:\nlm(formula = Spread ~ ., data = teams_forlm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-32.29  -5.72  -0.15   5.66  30.36 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -142.4223     2.8728  -49.58  &lt; 2e-16 ***\nPTS            0.0624     0.0269    2.32     0.02 *  \nP2p           74.1170     3.8391   19.31  &lt; 2e-16 ***\nP3p           74.0109     3.3449   22.13  &lt; 2e-16 ***\nFTp           15.4686     2.0265    7.63  3.4e-14 ***\nOREB           0.7191     0.0623   11.54  &lt; 2e-16 ***\nDREB           1.2033     0.0367   32.81  &lt; 2e-16 ***\nAST           -0.0350     0.0479   -0.73     0.47    \nSTL            1.0685     0.0677   15.79  &lt; 2e-16 ***\nBLK            0.3503     0.0784    4.47  8.2e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.79 on 2108 degrees of freedom\nMultiple R-squared:  0.622, Adjusted R-squared:  0.62 \nF-statistic:  385 on 9 and 2108 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "slides/Chapter07.html#variance-explained-review-from-chapter-6",
    "href": "slides/Chapter07.html#variance-explained-review-from-chapter-6",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Variance Explained (Review from Chapter 6)",
    "text": "Variance Explained (Review from Chapter 6)\n\\(R^2\\) is the proportion of variance in \\(y\\) explained by \\(x\\) .\n\\(R^2=\\frac{SS~regression}{SS~Total}\\)\n\n\\(R^2_{adj}\\) is adjusted to remove the variation that is explained by chance alone\n\\(R^2_{adj}=1-\\frac{MS~Error}{MS~Total}\\)\n\\(R^2_{adj}\\) can also be written as: \\(R^2_{adj}=1-\\frac{(1-R^2)*(n-1)}{(n-k-1)}\\)\nNow we have more sums of square to add in to our regression\n\n\nSS &lt;- anova(full_reg) |&gt; \n   tidy() |&gt; \n  select(term:sumsq) |&gt; \n  janitor::adorn_totals()\nSS\n\n      term   df    sumsq\n       PTS    1 141194.6\n       P2p    1   4001.7\n       P3p    1  14465.4\n       FTp    1    583.8\n      OREB    1   7082.2\n      DREB    1  78359.3\n       AST    1     10.1\n       STL    1  20082.0\n       BLK    1   1542.2\n Residuals 2108 162702.8\n     Total 2117 430024.0"
  },
  {
    "objectID": "slides/Chapter07.html#headline-higher-ice-cream-sales-increase-shark-attacks",
    "href": "slides/Chapter07.html#headline-higher-ice-cream-sales-increase-shark-attacks",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Headline: Higher ice cream sales increase shark attacks!",
    "text": "Headline: Higher ice cream sales increase shark attacks!\nWe’ll predict ice cream sales from the temperature and the number of shark attacks"
  },
  {
    "objectID": "slides/Chapter07.html#cross-validation-review-from-chapter-6",
    "href": "slides/Chapter07.html#cross-validation-review-from-chapter-6",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Cross validation (review from Chapter 6)",
    "text": "Cross validation (review from Chapter 6)\nIn sample error vs prediction error\n\nFor simpler models, increasing the number of parameters improves the fit to the sample.\nBut it seems to reduce the accuracy of the out-of-sample predictions.\nMost accurate models trade off flexibility (complexity) and overfitting\n\nGeneral idea: - Leave out some observations. - Train the model on the remaining samples; score on those left out. - Average over many left-out sets to get the out-of-sample (future) accuracy."
  },
  {
    "objectID": "slides/Chapter07.html#single-regression",
    "href": "slides/Chapter07.html#single-regression",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "Single regression",
    "text": "Single regression\n\n\n\nTemperature Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.00\n0.064\n0\n1\n\n\ntemp\n0.77\n0.064\n12\n0\n\n\n\n\n\n\n\n\nShark Only\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.000\n0.083\n0.00\n1\n\n\nshark\n0.557\n0.084\n6.64\n0"
  },
  {
    "objectID": "slides/Chapter07.html#ss-types-in-action-1",
    "href": "slides/Chapter07.html#ss-types-in-action-1",
    "title": "Chapter 7:Models with Multiple Predictors",
    "section": "SS types in action",
    "text": "SS types in action\n\nanova(full_reg)\n\nAnalysis of Variance Table\n\nResponse: Spread\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nPTS          1 141195  141195 1829.34 &lt; 2e-16 ***\nP2p          1   4002    4002   51.85 8.3e-13 ***\nP3p          1  14465   14465  187.42 &lt; 2e-16 ***\nFTp          1    584     584    7.56   0.006 ** \nOREB         1   7082    7082   91.76 &lt; 2e-16 ***\nDREB         1  78359   78359 1015.23 &lt; 2e-16 ***\nAST          1     10      10    0.13   0.718    \nSTL          1  20082   20082  260.18 &lt; 2e-16 ***\nBLK          1   1542    1542   19.98 8.2e-06 ***\nResiduals 2108 162703      77                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlibrary(car)\nAnova(full_reg,  type=2)\n\nAnova Table (Type II tests)\n\nResponse: Spread\n          Sum Sq   Df F value  Pr(&gt;F)    \nPTS          416    1    5.38    0.02 *  \nP2p        28767    1  372.71 &lt; 2e-16 ***\nP3p        37787    1  489.57 &lt; 2e-16 ***\nFTp         4497    1   58.26 3.4e-14 ***\nOREB       10276    1  133.13 &lt; 2e-16 ***\nDREB       83083    1 1076.43 &lt; 2e-16 ***\nAST           41    1    0.53    0.47    \nSTL        19232    1  249.17 &lt; 2e-16 ***\nBLK         1542    1   19.98 8.2e-06 ***\nResiduals 162703 2108                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAnova(full_reg,  type=3)\n\nAnova Table (Type III tests)\n\nResponse: Spread\n            Sum Sq   Df F value  Pr(&gt;F)    \n(Intercept) 189696    1 2457.72 &lt; 2e-16 ***\nPTS            416    1    5.38    0.02 *  \nP2p          28767    1  372.71 &lt; 2e-16 ***\nP3p          37787    1  489.57 &lt; 2e-16 ***\nFTp           4497    1   58.26 3.4e-14 ***\nOREB         10276    1  133.13 &lt; 2e-16 ***\nDREB         83083    1 1076.43 &lt; 2e-16 ***\nAST             41    1    0.53    0.47    \nSTL          19232    1  249.17 &lt; 2e-16 ***\nBLK           1542    1   19.98 8.2e-06 ***\nResiduals   162703 2108                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  }
]