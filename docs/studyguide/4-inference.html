<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>161250 Data Analysis - Chapter 4: Statistical Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../studyguide/5-tabulated.html" rel="next">
<link href="../studyguide/3-probability.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script src="../site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="../site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">161250 Data Analysis</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../slides.html" rel="" target="">
 <span class="menu-text">Slides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../studyguide/index.html" rel="" target="" aria-current="page">
 <span class="menu-text">Study Guide</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../workshops/ws01.html" rel="" target="">
 <span class="menu-text">Workshops</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../studyguide/index.html">Study Guide</a></li><li class="breadcrumb-item"><a href="../studyguide/4-inference.html">Chapter 4: Statistical Inference</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../studyguide/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Study Guide</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studyguide/1-data-collection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 1: Data Collection and Quality Issues</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studyguide/2-eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2: Exploratory Data Analysis (EDA)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studyguide/3-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 3: Probability Concepts and Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studyguide/4-inference.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Chapter 4: Statistical Inference</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studyguide/5-tabulated.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 5: Tabulated Counts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studyguide/6-single.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 6: Models with a Single Continuous Predictor</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studyguide/7-multiple.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 7: Models with Multiple Continuous Predictors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../studyguide/8-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 8: Analysis of Variance (ANOVA) and Covariance (ANCOVA)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-statistical-inference" id="toc-what-is-statistical-inference" class="nav-link active" data-scroll-target="#what-is-statistical-inference">What is statistical inference?</a>
  <ul class="collapse">
  <li><a href="#populations-and-parameters-samples-and-statistics" id="toc-populations-and-parameters-samples-and-statistics" class="nav-link" data-scroll-target="#populations-and-parameters-samples-and-statistics">Populations and parameters, samples and statistics</a></li>
  <li><a href="#sampling-error" id="toc-sampling-error" class="nav-link" data-scroll-target="#sampling-error">Sampling Error</a></li>
  </ul></li>
  <li><a href="#tests-for-normality" id="toc-tests-for-normality" class="nav-link" data-scroll-target="#tests-for-normality">Tests for normality</a>
  <ul class="collapse">
  <li><a href="#example-rangitikei" id="toc-example-rangitikei" class="nav-link" data-scroll-target="#example-rangitikei">Example: <code>rangitikei</code></a></li>
  <li><a href="#example-testmarks" id="toc-example-testmarks" class="nav-link" data-scroll-target="#example-testmarks">Example: <code>testmarks</code></a></li>
  </ul></li>
  <li><a href="#sampling-distributions" id="toc-sampling-distributions" class="nav-link" data-scroll-target="#sampling-distributions">Sampling distributions</a></li>
  <li><a href="#confidence-intervals-for-population-mean-t-tests" id="toc-confidence-intervals-for-population-mean-t-tests" class="nav-link" data-scroll-target="#confidence-intervals-for-population-mean-t-tests">Confidence Intervals for Population Mean &amp; <span class="math inline">\(t\)</span>-tests</a></li>
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing">Hypothesis Testing</a>
  <ul class="collapse">
  <li><a href="#hypothesis-testing-for-mean" id="toc-hypothesis-testing-for-mean" class="nav-link" data-scroll-target="#hypothesis-testing-for-mean">Hypothesis testing for mean</a></li>
  </ul></li>
  <li><a href="#inferences-for-two-groups" id="toc-inferences-for-two-groups" class="nav-link" data-scroll-target="#inferences-for-two-groups">Inferences for Two Groups</a>
  <ul class="collapse">
  <li><a href="#hypothesis-tests-for-two-groups" id="toc-hypothesis-tests-for-two-groups" class="nav-link" data-scroll-target="#hypothesis-tests-for-two-groups">Hypothesis tests for two groups</a></li>
  <li><a href="#confidence-intervals-for-the-difference-in-means" id="toc-confidence-intervals-for-the-difference-in-means" class="nav-link" data-scroll-target="#confidence-intervals-for-the-difference-in-means">Confidence Intervals for the Difference in Means</a></li>
  <li><a href="#paired-t-test" id="toc-paired-t-test" class="nav-link" data-scroll-target="#paired-t-test">Paired <span class="math inline">\(t\)</span> test</a></li>
  </ul></li>
  <li><a href="#transformations" id="toc-transformations" class="nav-link" data-scroll-target="#transformations">Transformations</a>
  <ul class="collapse">
  <li><a href="#transformation-and-shape" id="toc-transformation-and-shape" class="nav-link" data-scroll-target="#transformation-and-shape">Transformation and shape</a></li>
  <li><a href="#the-ladder-of-powers" id="toc-the-ladder-of-powers" class="nav-link" data-scroll-target="#the-ladder-of-powers">The Ladder of Powers</a></li>
  <li><a href="#checking-the-adequacy-of-a-transformation" id="toc-checking-the-adequacy-of-a-transformation" class="nav-link" data-scroll-target="#checking-the-adequacy-of-a-transformation">Checking the adequacy of a transformation</a></li>
  <li><a href="#some-words-of-caution-about-transformations" id="toc-some-words-of-caution-about-transformations" class="nav-link" data-scroll-target="#some-words-of-caution-about-transformations">Some Words of Caution About Transformations</a></li>
  <li><a href="#box-cox-normalising-transformations" id="toc-box-cox-normalising-transformations" class="nav-link" data-scroll-target="#box-cox-normalising-transformations">Box-Cox Normalising transformations</a></li>
  </ul></li>
  <li><a href="#transformations-for-inference" id="toc-transformations-for-inference" class="nav-link" data-scroll-target="#transformations-for-inference">Transformations for Inference</a></li>
  <li><a href="#transformations-to-constant-variance" id="toc-transformations-to-constant-variance" class="nav-link" data-scroll-target="#transformations-to-constant-variance">Transformations to Constant Variance</a></li>
  <li><a href="#nonparametric-methods" id="toc-nonparametric-methods" class="nav-link" data-scroll-target="#nonparametric-methods">Nonparametric Methods</a>
  <ul class="collapse">
  <li><a href="#ranking-and-rank-correlation" id="toc-ranking-and-rank-correlation" class="nav-link" data-scroll-target="#ranking-and-rank-correlation">Ranking and rank Correlation</a></li>
  <li><a href="#wilcoxon-signed-rank-test" id="toc-wilcoxon-signed-rank-test" class="nav-link" data-scroll-target="#wilcoxon-signed-rank-test">Wilcoxon signed rank test</a></li>
  <li><a href="#sign-test" id="toc-sign-test" class="nav-link" data-scroll-target="#sign-test">Sign test</a></li>
  <li><a href="#wilcoxon-rank-sum-or-mann-whitney-test" id="toc-wilcoxon-rank-sum-or-mann-whitney-test" class="nav-link" data-scroll-target="#wilcoxon-rank-sum-or-mann-whitney-test">Wilcoxon Rank-Sum or Mann-Whitney test</a></li>
  </ul></li>
  <li><a href="#permutation-and-bootstrap-tests" id="toc-permutation-and-bootstrap-tests" class="nav-link" data-scroll-target="#permutation-and-bootstrap-tests">Permutation and bootstrap tests</a>
  <ul class="collapse">
  <li><a href="#bootstrap-methods" id="toc-bootstrap-methods" class="nav-link" data-scroll-target="#bootstrap-methods">Bootstrap methods</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="4-inference.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Chapter 4: Statistical Inference</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<blockquote class="blockquote">
<p>“<em>All models are wrong, but some are useful.</em>”</p>
<p>– George Box</p>
</blockquote>
<blockquote class="blockquote">
<p>“<em>Absence of evidence is not evidence of absence.</em>”</p>
<p>– Carl Sagan</p>
</blockquote>
<blockquote class="blockquote">
<p>“<em>A statistical analysis, properly conducted, is a delicate dissection of uncertainties, a surgery of suppositions.</em>”</p>
<p>– M.J. Moroney</p>
</blockquote>
<p>This chapter provides an introduction to statistical inference. Many of the concepts in this chapter should be familiar to you because they are covered in all first-year statistics courses.</p>
<section id="what-is-statistical-inference" class="level1">
<h1>What is statistical inference?</h1>
<section id="populations-and-parameters-samples-and-statistics" class="level2">
<h2 class="anchored" data-anchor-id="populations-and-parameters-samples-and-statistics">Populations and parameters, samples and statistics</h2>
<p>Statistical inference is a fundamental concept in statistics. The vast majority of statistical analyses that you will do as an undergraduate involve statistical inference. Anything involving <em>p</em>-values, confidence intervals, or standard errors are a form of statistical inference.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Statistical inference is:
</div>
</div>
<div class="callout-body-container callout-body">
<p>the use of information from a sample to make statements about a population.</p>
</div>
</div>
<p>As discussed in previous chapters, most datasets contain information about a sample from a population, rather than the whole population of interest. For example (<a href="#fig-inf">Figure&nbsp;1</a>), say we owned a fish farm, and we wished to know the average length of the fish in our farm. Let’s say we had 2,000 fish in our farm. It would be too time-consuming to catch and measure every single fish. Instead, we take a random sample of, say, 10 fish, measure their lengths, and calculate the mean.</p>
<p>Remember, our goal here is to know something about the <em>whole population of 2,000 fish</em>. We don’t really care about the 10 fish in our sample. It is no use to say “<em>Well, I’ve no idea about the average length of my whole population fish, but you see those 10 fish there? They average 36.7 cm in length.</em>”. We only care about the 10 fish in our sample <em>in so far as they tell us something about the broader population.</em> We use the average length of the fish in our sample as <em>and estimate</em> of the average length of fish in the population. This is statistical inference: using information from a sample to make conclusions about a population.</p>
<div id="fig-inf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/inference.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Statistical inference from a sample to a population of fish</figcaption>
</figure>
</div>
<p>To clarify some terminology using the example in <a href="#fig-inf">Figure&nbsp;1</a>:</p>
<ul>
<li>The <strong>population</strong> is all 2,000 fish in our farm.</li>
<li>The <strong>sample</strong> is the 10 fish we happened to measure.</li>
<li>The <strong>parameter of interest</strong> (often denoted <span class="math inline">\(\mu\)</span>, if it is a mean, or <span class="math inline">\(\theta\)</span> more generally) is the average length of the fish in the population of 2,000. Population parameters are usually considered to be <em>fixed and unknown</em> values.</li>
<li>The <strong>statistic</strong> (often denoted <span class="math inline">\(\bar{y}\)</span> or <span class="math inline">\(\hat\mu\)</span>, if it is a mean, or <span class="math inline">\(\hat\theta\)</span> more generally) is the average of the 10 lengths of the fish in our sample. Unlike population parameters, which are <em>fixed and unknown</em>, sample statistics are <em>random variables</em>.</li>
<li><strong>Statistical inference</strong> in this case is the use of the sample mean <span class="math inline">\(\bar{y}\)</span> as an <em>estimate</em> of the population mean <span class="math inline">\(\mu\)</span>.</li>
</ul>
<p>The fact that we’ve only measured lengths from a sample rather than the whole population necessitates statistical inference. If we’d measured every fish in the farm, we wouldn’t need statistical inference, because we’d know precisely the population parameter (assuming negligible measurement error).</p>
</section>
<section id="sampling-error" class="level2">
<h2 class="anchored" data-anchor-id="sampling-error">Sampling Error</h2>
<p>This brings us to the next important concept of statistical inference: sampling error. A consequence of having collected data from a sample rather than the whole population is that there is <em>uncertainty</em> in our knowledge of the population parameter. Our sample mean is an <em>estimate</em> of the population mean; if we wanted to know population mean with zero uncertainty, we’d have to measure all the fish. This is the trade-off of sampling. It’s a lot cheaper to sample, but we sacrifice certainty.</p>
<p>The practical application of statistical inference involves (1) making estimates and (2) quantifying the uncertainty of those estimates. Uncertainty is often quantified using standard errors, confidence intervals, and <em>P</em>-values. All these quantities relate to <em>sampling error</em>. They’re all expressions of the uncertainty of an estimate of a population parameter.</p>
<p>In the fish farm example (<a href="#fig-inf">Figure&nbsp;1</a>), sampling error is the hypothetical variation in the means of the lengths of samples of fish, with a sample size of <span class="math inline">\(n\)</span> = 10. That is, if we were to (hypothetically) repeat the scientific process (i.e., randomly select 10 fish, measure their lengths, and calculate the sample mean), over and over again, how much would those sample means vary? That variation of sample statistics is sampling variation, or sampling error. <strong>And understanding sampling variation is the key to understanding most of undergraduate statistics</strong>.</p>
<p>So, when we do our study (i.e., randomly select 10 fish, measure their lengths, and calculate the sample mean), we are drawing one value of the sample mean, <span class="math inline">\(\bar y\)</span>, from a random variable, <span class="math inline">\(\bar Y\)</span>, which is the distribution of sample means that we <em>could</em> hypothetically draw.</p>
<p>Given this random sampling variation, here are some explanations for some commonly used measures of uncertainty:</p>
<ul>
<li><p>A <strong>standard error</strong> is simply the standard deviation of a statistic under repeated sampling–that is, how much it would vary (hypothetically) from sample to sample.</p></li>
<li><p>A <strong>confidence interval</strong> is a pair of numbers that contain the true value of the population parameter with 95% confidence. It is a simple function of the sample statistic and its standard error.</p></li>
<li><p>A <strong><em>p</em>-value</strong> is the probability of obtaining a sample statistic as or more extreme than the one observed, given a particular hypothesised value (usually zero) of the population parameter.</p></li>
</ul>
<p>Don’t worry if those definitions aren’t completely clear to you right now, but I encourage you to refer back to this section again and again as you learn about them in more detail during the rest of this course.</p>
<p><strong>Keep sampling error front of mind whenever you see a standard error, a confidence interval, or a <em>p</em>-value.</strong></p>
<p>Now, we’ll introduce some specific inference methods.</p>
</section>
</section>
<section id="tests-for-normality" class="level1">
<h1>Tests for normality</h1>
<p>Let’s say we have some data and we wish to test the idea that the data came from a population that is normally distributed. The null hypothesis is that the population conforms to a normal distribution. We calculate a test statistic that quantifies the degree of <em>departure</em> of the data from what we’d expect if the null hypothesis were true (i.e., the population were indeed normally distributed)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>If the data look substantially different to a normal distribution, then we will expect the associated test statistic to be large. How large does it have to be before we can reject the idea that the data came from a normal distribution? This question can be answered by calculating a <span class="math inline">\(p\)</span>-value for the test statistic. <strong>The <em>p</em>-value is the probability of obtaining a test statistic as or more extreme than the one we have calculated</strong> <strong><em>if the null hypothesis were true</em></strong>.</p>
<p>If the <span class="math inline">\(p\)</span>-value is smaller than some pre-decided threshold level, such as 5%, then we can reject the null hypothesis that the population is normally distributed, and conclude that the population is <strong>not</strong> normally distributed. If the <span class="math inline">\(p\)</span>-value is large, then we have <strong>no evidence</strong> that the population is normally distributed.</p>
<p>Two important points to remember about null hypotheses:</p>
<ul>
<li>Null (and alternative) hypotheses are <strong>always</strong> about population parameters, and <strong>never</strong> about sample statistics; inferences are always about the population, never about the sample.</li>
<li>With a large <span class="math inline">\(p\)</span>-value, we <strong>never</strong> say that we have “accepted” or “confirmed” the null hypothesis; we only ever reject or fail to reject a null hypothesis.</li>
</ul>
<p>There are several statistical tests for normality available in the literature. The <strong>Kolmogorov-Smirnov test</strong> for normality is based on the biggest difference between the empirical and theoretical cumulative distributions. On the other hand, <strong>Shapiro-Wilk test</strong> is based on variance of the difference. There are also several other normality test procedures and we will not be concerned with the details. It is also difficult to regard one particular test to be always superior or powerful than the other.</p>
<p>The Shapiro-Wilk test of normality for the number of people who made use of a recreational facility (<strong>rangitikei</strong>) gives a <span class="math inline">\(p\)</span>-value less than 0.001. The null hypothesis is that the data come from a normal distribution. The low <span class="math inline">\(p\)</span>-value indicates a significant departure from normality.</p>
<section id="example-rangitikei" class="level2">
<h2 class="anchored" data-anchor-id="example-rangitikei">Example: <code>rangitikei</code></h2>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">download.file</span>(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">url =</span> <span class="st">"http://www.massey.ac.nz/~anhsmith/data/rangitikei.RData"</span>,</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">destfile =</span> <span class="st">"rangitikei.RData"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"rangitikei.RData"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(rangitikei<span class="sc">$</span>people)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Shapiro-Wilk normality test

data:  rangitikei$people
W = 0.65346, p-value = 1.382e-07</code></pre>
</div>
</div>
<p>The low <span class="math inline">\(p\)</span>-value here indicates a significant departure from normality. We conclude that there is very strong evidence against the null hypothesis that we the population is normally distributed. (Remember, always express your conclusions by reference to the population, not the sample, or even “the data”.)</p>
<p>We can examine a Q-Q plot (<a href="#fig-rangidist">Figure&nbsp;2</a>), which plots the observed values (<span class="math inline">\(y\)</span>) against the theoretical values if the population were normally distributed. Departure from the diagonal line indicates departure from normality.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span> </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">sample =</span> people) <span class="sc">+</span> </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq</span>() <span class="sc">+</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>() </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span> </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>people, <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1, p2, <span class="at">ncol=</span><span class="dv">1</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rangidist" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-rangidist-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Distribution of people</figcaption>
</figure>
</div>
</div>
</div>
<p>The same conclusion is drawn with the Kolmogorov-Smirnov test. Note that this test does not allow ties and can be used to test the fitting of non-normal distributions.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ks.test</span>(rangitikei<span class="sc">$</span>people, <span class="st">"pnorm"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in ks.test.default(rangitikei$people, "pnorm"): ties should not be
present for the Kolmogorov-Smirnov test</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Asymptotic one-sample Kolmogorov-Smirnov test

data:  rangitikei$people
D = 0.99997, p-value &lt; 2.2e-16
alternative hypothesis: two-sided</code></pre>
</div>
</div>
<p>It is often informative to analyse data by fitting a <strong>statistical model</strong>. The idea is to look for real patterns, “signals” amongst the “noise” of individual variation, patterns that would reoccur in other, hypothetical samples we might have drawn from the population. We often try to approximate patterns by fitting a “statistical model”. A A statistical model usually comprises a mathematical formula describing the relationships among variables, along with a probabilistic description of the variation of the data around the formula. If the statistical model is a good approximation, it serves as a neat way of describing the system that generated the data, and we can use such a model to predict future values of the variables.</p>
</section>
<section id="example-testmarks" class="level2">
<h2 class="anchored" data-anchor-id="example-testmarks">Example: <code>testmarks</code></h2>
<p>The data set <strong>tv</strong> consists of the time that 46 school children spent watching television. Before fitting a model to the data, it is a good idea to see whether the data approximately follows a Normal distribution using a normal Q-Q Plot; see <a href="#fig-tvdist">Figure&nbsp;3</a>. The points plotted fall pretty much along the line, suggesting at least approximate Normality.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">download.file</span>(</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">url =</span> <span class="st">"http://www.massey.ac.nz/~anhsmith/data/tv.RData"</span>, </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">destfile =</span> <span class="st">"tv.RData"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"tv.RData"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>P.val <span class="ot">&lt;-</span> tv<span class="sc">$</span>TELETIME <span class="sc">|&gt;</span> </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">shapiro.test</span>() <span class="sc">|&gt;</span> </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pluck</span>(<span class="st">'p.value'</span>) <span class="sc">|&gt;</span> </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">3</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tv) <span class="sc">+</span> </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">sample =</span> TELETIME) <span class="sc">+</span> </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq</span>() <span class="sc">+</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>() <span class="sc">+</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">caption =</span> <span class="fu">paste</span>(<span class="st">"Shapiro Test P value"</span>, P.val))</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tv) <span class="sc">+</span> </span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>TELETIME, <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1, p2, <span class="at">ncol=</span><span class="dv">1</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tvdist" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-tvdist-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Distribution of TV viewing times</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-tvdist">Figure&nbsp;3</a> shows the boxplot of the data. The boxplot again suggests a very mild skew to the left but the middle 50% data show right skewness. However the whiskers are about the same length and there are no outliers. There is a difference of 31 between the mean and median, suggestive of a slight skew to the lower values. However this difference is small given the overall variability (standard deviation is 567.9, and the range is 2309) so we can probably ignore the observed skew. However we will look for any further evidence of skewness, since this could invalidate any inference we make based on the Normal distribution (at least it would if we had a smaller sample). The TV viewing time data also passes normality tests such as Shapiro-Wilk test. All told, we conclude that the normal model describes the distribution of these data fairly well.</p>
<p>As indicated earlier, a large number of naturally occurring measurements, such as height, appear to follow a Normal distribution so that a considerable amount of theory has been built on this distribution.</p>
<p>To recapitulate, Normal (or Gaussian) curves are determined by just two numbers, one indicating the <strong>location</strong> and the other the <strong>spread</strong>. Although there are an infinite number of Normal curves, their shapes are similar and, of course, the area under each curve is 1. Indeed, the location and spread parameters are the only differences between curves.</p>
<p>It is usual to take the measure of location as the mean, denoted by <span class="math inline">\(\mu\)</span>, and the measure of spread as the standard deviation, denoted by <span class="math inline">\(\sigma\)</span>. If a variable, Y, follows a Normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, we write Y <span class="math inline">\(\sim\)</span> N(<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span>) where <span class="math inline">\(\sim\)</span> means is <em>distributed as</em> (The squiggly symbol <span class="math inline">\(\sim\)</span> is known as <em>tilde</em>).</p>
<p>Note, however, that the Normal distribution, like all statistical distributions, is a theoretical concept and no naturally occurring measurement will <strong>exactly</strong> follow a probability distribution model. For one thing, any measurement is finite whereas the Normal curve is continuous in the interval <span class="math inline">\(\left(-\infty ,\infty \right)\)</span>. Also, the curve is asymptotic to the <span class="math inline">\(X\)</span>-axis so that any range of values of <span class="math inline">\(Y\)</span> however large or small will have a certain probability according to the Normal distribution, but in practice there will be limitations such as that a person’s blood pressure must be greater than zero.</p>
<p>Suppose we didn’t know that <span class="math inline">\(\mu\)</span> = 80 and <span class="math inline">\(\sigma\)</span> = 12. The obvious <strong>estimator</strong> of <span class="math inline">\(\mu\)</span>, based solely on the sample, is the sample mean <span class="math inline">\(\bar{y}\)</span>, and the obvious estimator of <span class="math inline">\(\sigma\)</span> is the sample standard deviation <span class="math inline">\(S\)</span>. Note that by estimator we don’t mean the observed value based on the <strong>particular</strong> sample. Rather the word estimator means the mathematical formula or procedure that we use to produce our estimates, namely <span class="math inline">\(\bar{y}={\frac{1}{n}} \sum y\)</span> and <span class="math inline">\(S=\sqrt{{\frac{1}{n-1}} \sum _{i=1}^{n}(y_{i} -\bar{y})^{2} }\)</span>.</p>
<p>The point is that there can be several alternative procedures for estimating the same parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, and in particular samples the actual computed estimates may be the same or different. For example, since the mean and median are the same for Normal data, we could estimate <span class="math inline">\(\mu\)</span> by the sample median, namely 81.313 for our example. This different procedure has given rise to a different number, and if we didn’t know the answer we would not know which estimate to use. The median estimate has a lot of attraction, since the median is robust, that is, is not affected by outliers.</p>
<p>Clearly all these estimates are close to the true parameter values, but not the same. A statistical question relates to how close estimates are to their true values <em>in general</em>. We usually can’t answer this question about the actual estimates (observed numbers) since we usually don’t know the correct answer. One approach, which is often used to test procedures in research, is to try a number of <strong>simulations</strong> and see which approach produces the closest results on average. We can also give error bounds that say, for example, that 95% of the time the estimator is within such-and-such a distance of the true parameter. This leads to the idea of using probability or so-called ‘confidence’. By making probability statements about the estimators we can say something useful about how trustworthy the particular estimates are also. We use <strong>standard errors</strong> to measure the trustworthiness of the estimators. The standard error is the standard deviation of the estimator, so the smaller the standard error the better.</p>
<p>Without going into details, it turns out that <span class="math inline">\(\bar{y}\)</span> has the smallest possible standard error for any unbiased estimator of <span class="math inline">\(\mu\)</span> for normal data. (An unbiased estimator is one that is not systematically too big or too small.) While the same is <em>not</em> true of <span class="math inline">\(S\)</span> in relation to <span class="math inline">\(\sigma\)</span>, the latter does have other useful mathematical properties. So these are some reasons for using these formulae so routinely.</p>
</section>
</section>
<section id="sampling-distributions" class="level1">
<h1>Sampling distributions</h1>
<p>A <em>sampling distribution</em> is a probabilistic model of <em>sampling variation</em>–it describes the behaviour of some sample statistic (such as a sample mean) if one were to repeat the sampling and calculation of the statistic many many times. The sampling distribution is not known, because we usually only have a single sample. However, we can make certain theoretical assumptions about how a statistic is distributed if one were to repeat the study over and over again.</p>
<p>For a normal population, when the population parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are <em>known</em>, we can easily derive the sampling distributions of the sample mean or sample variance. When the population parameters are <em>unknown</em>, we have to estimate them from data. When the sample size is small, we have large standard errors around estimates of the population parameters, and the distributions of the sample mean or sample variance are poorly known.</p>
<p>The Student’s <span class="math inline">\(t\)</span>, <span class="math inline">\(\chi^2\)</span> and <span class="math inline">\(F\)</span> distributions are the three useful sampling distributions for testing hypotheses for a normal population when the population parameters are unknown. These distributions also have a closed form expression for their probability density function. This means that we can calculate the areas under the distribution functions, and calculate <em>p</em>-values.</p>
<p>The <span class="math inline">\(t\)</span> distribution is the sampling distribution of the mean when <span class="math inline">\(\sigma\)</span> is unknown. The <span class="math inline">\(\chi^2\)</span> distribution is the sampling distribution of the sample variance <span class="math inline">\(S^2\)</span> when re-expressed as <span class="math inline">\((n-1)S^2/\sigma^2\)</span>. The <span class="math inline">\(F\)</span> distribution is ratio of two <span class="math inline">\(\chi^2\)</span> distributions, and hence it becomes the sampling distribution of the ratio of two sample variances <span class="math inline">\(S_1^2/S_2^2\)</span> from two normal populations (after appropriate scaling for the sample sizes).</p>
<p>While the <span class="math inline">\(t\)</span> distribution is symmetric, the <span class="math inline">\(\chi^2\)</span> and <span class="math inline">\(F\)</span> distributions are right skewed. When the sample size <span class="math inline">\(n\)</span> approaches infinity, both distributions become normal but you will start observing symmetry when the sample size(s) exceed 30.</p>
<p>For these sampling distributions, the sample size acts as the proxy parameter, called the degrees of freedom. For both <span class="math inline">\(t\)</span>, <span class="math inline">\(\chi^2\)</span>, the degrees of freedom, <span class="math inline">\(\nu\)</span> is <span class="math inline">\((n-1)\)</span>. What this means is that the sampling distribution or the probability density of these two distributions depend only on the degrees of freedom <span class="math inline">\(\nu\)</span>. The F distribution is based on two samples of size <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>. So, the F density has two parameters, <span class="math inline">\(v_1=(n_1-1)\)</span> and <span class="math inline">\(v_2=(n_2-1)\)</span> (often called the numerator and denominator degrees of freedom respectively).</p>
<p>The tail quantiles of <span class="math inline">\(t\)</span>, <span class="math inline">\(\chi^2\)</span> and <span class="math inline">\(F\)</span> distributions are used for hypothesis tests. You may like to visit https://shiny.massey.ac.nz/kgovinda/demos/demo.critical.values/ for exploring their probability densities and quantiles.</p>
<p>Sampling distributions of certain statistic such as the sample Range (=Maximum-Minimum) does not exist in a closed form but the quantiles of the distribution can be obtained numerically.</p>
<p>We cover the <span class="math inline">\(t\)</span> distribution in some detail below:</p>
<p><strong><span class="math inline">\(t\)</span> distribution</strong></p>
<p>Now consider a single observation <span class="math inline">\(Y \sim\)</span> N(<span class="math inline">\(\mu\)</span>,<span class="math inline">\(\sigma\)</span>). We have already seen that if <span class="math inline">\(Z = (Y-\mu)/\sigma\)</span>, then <span class="math inline">\(Z \sim\)</span> N(0,1). We can write this line slightly more generally as</p>
<p><span class="math inline">\(Z= \frac{{\text {Observed(Y)-Expected(Y)}}}{{\text {Standard Deviation(Y)}}}\)</span></p>
<p>implies <span class="math inline">\(Z \sim\)</span> N(0,1).</p>
<p>Next suppose we have a sample of <span class="math inline">\(n\)</span> data values <span class="math inline">\(\left(y_{1},y_{2},...,y_{n} \right)\)</span>. From these we compute the sample mean <span class="math inline">\(\bar{y}\)</span>. It can be shown that the expected value of <span class="math inline">\(\bar{y}\)</span> is <span class="math inline">\(\mu\)</span> and also that the standard deviation of <span class="math inline">\(\bar{y}\)</span> is <span class="math inline">\(\sigma /\sqrt{n}\)</span>. That is, if we take a large number of samples, then <em>on average</em> the various values of <span class="math inline">\(\bar{y}\)</span> will tend to cluster around <span class="math inline">\(\mu\)</span>, and if <span class="math inline">\(n\)</span> is large then they cluster around that value rather more closely than if <span class="math inline">\(n\)</span> is small.</p>
<p>It also turns out that if <span class="math inline">\(\left(y_{1},y_{2},...,y_{n} \right)\)</span> are each Normal then</p>
<p><span class="math inline">\(Z=\frac{\bar{y}-\mu }{\sigma /\sqrt{n} } =\frac{{\text {Observed(Y)-Expected(Y)}}}{{\text {Standard Deviation(Y)}}}\)</span></p>
<p>is also N(0,1).</p>
<p>In this case, the standard error of the sample mean is the standard deviation of the original population divided by the square root of <span class="math inline">\(n\)</span>.</p>
<p>In fact, a profound result called the Central Limit Theorem (CLT) says that, if the sample size <span class="math inline">\(n\)</span> is large enough, then <span class="math inline">\(Z = \frac{\bar{y}-\mu }{\sigma /\sqrt{n} }\)</span> will approximately have a Normal distribution, almost regardless of the distribution of the data. (There is an exception to the CLT that relates to distributions with too many outliers.) The importance of the CLT to Statistics can hardly be overstated. It means we can draw conclusions about the population mean <span class="math inline">\(\mu\)</span> based on the position of <span class="math inline">\(Z\)</span> on the Normal tables, even though the sample of data may not look exactly Normal (for example the original data may be discrete or skewed). How large <span class="math inline">\(n\)</span> has to be, before the Central Limit Theorem can be relied on, depends on the extent of skewness, discreteness, and so on. A commonly used guideline is <span class="math inline">\(n\)</span> <span class="math inline">\(\geq\)</span> 30. But this thumb rule requires random data to be drawn from a homogeneous population with no subgrouping. For discrete variables the probability mass should not be concentrated too much on a particular value.</p>
<p>Again suppose the data <span class="math inline">\(\left(y_{1},y_{2},...,y_{n} \right)\)</span> are Normal. If the population standard deviation <span class="math inline">\(\sigma\)</span> is not known then it must be estimated by the sample standard deviation <span class="math inline">\(S\)</span>. The sample mean is then standardised to a <strong><em>t statistic</em></strong> instead, where <span class="math display">\[t=\frac{\bar{y}-\mu }{S/\sqrt{n} } \sim t_{n-1}\]</span> The <strong>degrees of freedom</strong> associated with the <span class="math inline">\(t\)</span> statistic is the same as the degrees of freedom associated with <span class="math inline">\(S\)</span>, that is, the <span class="math inline">\(n-1\)</span> divisor in the formula for <span class="math inline">\(S\)</span>. The <span class="math inline">\(t\)</span> <em>distribution</em> <span class="citation" data-cites="student">(<a href="#ref-student" role="doc-biblioref">Gosset 1942</a>)</span> is more spread out than the Standard Normal curve, that is, it is said to have fatter tails; see <a href="#fig-tdensity">Figure&nbsp;4</a>. The extra spread reflects the fact that observed values of <strong><em>t</em></strong> <strong>tend to be more variable</strong> than observed values of <span class="math inline">\(z\)</span>. Essentially the <span class="math inline">\(t\)</span> distribution is predictive and takes into account the uncertainty in the population spread.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">dt</span>(x,<span class="dv">1</span>), <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.4</span>), <span class="at">ylab=</span><span class="st">"Density"</span> )</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">dt</span>(x,<span class="dv">2</span>), <span class="at">add=</span>T, <span class="at">lty=</span><span class="dv">2</span> )</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">dt</span>(x,<span class="dv">5</span>), <span class="at">add=</span>T, <span class="at">lty=</span><span class="dv">3</span> )</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">dnorm</span>(x), <span class="at">add=</span>T,<span class="at">lty=</span><span class="dv">4</span> )</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">c</span>(<span class="st">"t(1)"</span>, <span class="st">"t(2)"</span>, <span class="st">"t(5)"</span>, <span class="st">"Normal"</span>),</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tdensity" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-tdensity-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Student’s t Distribution</figcaption>
</figure>
</div>
</div>
</div>
<p>Note that, strictly speaking, the <span class="math inline">\(t\)</span> distribution only holds true if the data <span class="math inline">\(\left(y_{1},y_{2},...,y_{n} \right)\)</span> came from a Normal distribution. However some simulation studies have shown that the ratio <span class="math inline">\(t=\frac{\bar{y}-\mu }{S/\sqrt{n} }\)</span> closely follows a <span class="math inline">\(t\)</span>-curve even if the data are not Normal, provided that the data are not highly skewed. (Skewness tends to have a marked impact on <span class="math inline">\(S\)</span> in the denominator, as well as on the mean <span class="math inline">\(\bar{y}\)</span>.) In practice, whenever a model is fitted to data, it is important to check that the assumptions of the model seem to hold. In this case, we could form a Normal Plot of the values of <span class="math inline">\(\left(y_{1},y_{2},...,y_{n} \right)\)</span> to visually check whether they seem to follow a Normal distribution. The better the line, the more confidence we can have in our inference from the data.</p>
</section>
<section id="confidence-intervals-for-population-mean-t-tests" class="level1">
<h1>Confidence Intervals for Population Mean &amp; <span class="math inline">\(t\)</span>-tests</h1>
<p>If the batch of data consists of the whole population, the batch mean will be exactly the population mean <span class="math inline">\(\mu\)</span>. If the batch is a sample, the sample mean <span class="math inline">\(\bar{y}\)</span> will be an exact summary of the sample, but may not be the same as <span class="math inline">\(\mu\)</span>. In fact, if another sample is drawn, a sample mean different to the first will be obtained. The sample mean will not exactly equal the population mean but will vary about it from sample to sample. This variation of the sample mean about the population mean is discussed in every introductory textbook on Statistics and also in Chapter 1 of these notes.</p>
<p>If the batch of data is a sample, it is natural to use it to try to infer certain characteristics of the population. In particular, we estimate the population mean <span class="math inline">\(\mu\)</span> by the sample mean, <span class="math inline">\(\bar{y}\)</span>. It is also helpful to calculate an interval in which the population mean is likely to fall, giving the <strong>interval estimate</strong> with a certain <strong>margin of error:</strong></p>
<p>Interval estimate of <span class="math inline">\(\mu\)</span> = <span class="math inline">\(\bar{y}\)</span> <span class="math inline">\(\pm\)</span> margin of error .</p>
<p>To find the margin of error, we assume that the random variable, <span class="math inline">\(\bar{y}\)</span>, has a Normal distribution. Since we can never be 100% certain whether <span class="math inline">\(\mu\)</span> falls in a certain interval we often settle for a 95% level of confidence. This means that the margin of error should be about two standard deviations of <span class="math inline">\(\bar{y}\)</span>. To be more correct,</p>
<p>margin of error = <span class="math inline">\(t\)</span> <span class="math inline">\(\times\)</span> e.s.e. (<span class="math inline">\(\bar{y}\)</span>).</p>
<p>Here, e.s.e. stands for <strong>estimated standard error</strong>. It is usual practice to denote the spread of the observations in the batches by the term <strong>standard deviation</strong>. When referring to other statistics such as the sample mean, <span class="math inline">\(\bar{y}\)</span>, or the coefficients in an equation, the standard deviation of these statistics is usually termed <strong>standard error</strong>. For a sample of size <span class="math inline">\(n\)</span>, the standard error of <span class="math inline">\(\bar{y}\)</span> is given by:</p>
<p>(estimated) standard error (<span class="math inline">\(\bar{y}\)</span>) = standard deviation(<span class="math inline">\(Y\)</span>)/<span class="math inline">\(\sqrt n\)</span>.</p>
<p>If the batch is a sample, we rarely know the standard deviation of <span class="math inline">\(Y\)</span> in the whole population and it is for this reason that we need to estimate it from the sample. It seems a long story but we have finally arrived at e.s.e., the estimated standard error <span class="math inline">\(S/\sqrt n\)</span> To be specific, we define a <strong>95% Confidence Interval</strong> for <span class="math inline">\(\mu\)</span> as</p>
<p><span class="math inline">\(\bar{y}\)</span> <span class="math inline">\(\pm\)</span> <span class="math inline">\(t \times (S/\sqrt n)\)</span></p>
<p>The term <span class="math inline">\(t\)</span> is the appropriate percentile of the <span class="math inline">\(t\)</span>-statistic with <span class="math inline">\(n-1\)</span> degrees of freedom. The value of <span class="math inline">\(t\)</span> will generally be greater than 2. This reflects the fact that there is additional variability in that the standard error is not known but must be estimated.</p>
<p>Be aware that this derivation of the confidence interval means that it is a statistic (that is, a formula based on sample observations) that works for 95% of samples. What we mean is that if we use this formula repeatedly over our lifetime, then on average 95% of the intervals we obtain will be correct (that is, will contain the true mean <span class="math inline">\(\mu\)</span>) and 5% of the intervals we obtain will not be correct (that is, will not contain <span class="math inline">\(\mu\)</span>). The only way to cut down our error rate is to increase the confidence level, for example use 99% confidence intervals instead, which means using a different value of <span class="math inline">\(t\)</span>. The problem with this strategy is that our intervals will always be that much wider, and we do not always need such a high level of confidence. In conclusion, we usually employ 95% confidence intervals unless the context indicates we should use some other level.</p>
<p>For the TV viewing time data, the sample mean = 1729.28 with (estimated) standard deviation = 567.91. Hence the (estimated) standard error of the mean is <span class="math inline">\(S/ \sqrt n = 83.73\)</span>. For the sample of size <span class="math inline">\(n = 46\)</span>, we have <span class="math inline">\(n-1 = 46-1= 45\)</span> degrees of freedom for the <span class="math inline">\(t\)</span> statistic. We may use either software or use Student’s <span class="math inline">\(t\)</span> tables and obtain <span class="math inline">\(t\)</span> ordinate (i.e.&nbsp;quantile) value as 2.01 corresponding to a right tail probability (area) of 0.025; see <a href="#fig-tquant">Figure&nbsp;5</a>. Due to symmetry, the area below the <span class="math inline">\(t\)</span> ordinate of <span class="math inline">\(-2.01\)</span> will also be 0.025.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">dt</span>(x,<span class="dv">45</span>), <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.4</span>), <span class="at">ylab=</span><span class="st">"Density"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>lowert<span class="ot">=</span><span class="fu">qt</span>(.<span class="dv">025</span>, <span class="dv">45</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>lowert)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(lowert, .<span class="dv">4</span>, <span class="st">"lower t quantile"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="sc">-</span><span class="fl">2.9</span>, <span class="fl">0.1</span>, <span class="st">"2.5% left tail area"</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>uppert<span class="ot">=</span><span class="fu">qt</span>(.<span class="dv">975</span>, <span class="dv">45</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>uppert)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(uppert, .<span class="dv">4</span>, <span class="st">"upper t quantile"</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">2.9</span>, <span class="fl">0.1</span>, <span class="st">"2.5% right tail area"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tquant" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-tquant-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Student’s t quantiles for CI construction</figcaption>
</figure>
</div>
</div>
</div>
<p>Having obtained the theoretical <span class="math inline">\(t\)</span>-value, the 95% confidence interval can be found as</p>
<p><span class="math inline">\(\bar{y}\pm t\times S/ \sqrt n\)</span> = <span class="math inline">\(1729.28\pm 2.01\times83.73\)</span> or (1560.63, 1897.93).</p>
<p>With 95% confidence, we can state that the population mean falls in the interval (1560.63, 1897.93).</p>
<p>If we want to be <strong>more confident</strong> that we have included the true value of the population mean, we could use a 99% confidence interval instead. That is, we make the interval wider by using the 99<span class="math inline">\(^{\text th}\)</span> percentile of the <span class="math inline">\(t\)</span> statistic (namely 2.69). The 99% confidence interval is computed as (1504.07, 1954.49), clearly wider than the 95% confidence interval.</p>
<p>The confidence interval estimate of the unknown population parameter (mean in the above discussion) is affected by the following:</p>
<ol type="1">
<li>The amount of variation in the population.</li>
<li>The nature of variation in the population.</li>
<li>Sample size.</li>
<li>The degree of confidence (probability) needed.</li>
</ol>
<p>Sample size is simply a trade off to obtain a confidence interval of a desired width. If a population is more variable, then it has to be sampled heavily. The width of the confidence interval and the degree of confidence are inversely related. The corollary is that a 100% confidence interval is of infinite width.</p>
<p>Confidence intervals are NOT unique. For example, we can leave 1% probability for the left tail and 4% probability for the right tail. But such an interval will be wider for symmetric distributions but may be appropriate for skewed sampling distributions. Confidence intervals may not always possess good statistical properties such as accuracy (probability of covering any wrong value of the parameter than the other) and unbiasedness (An unbiased 95% confidence interval has probability no more than 5% of covering any value of the parameter). Stronger assumptions must be made to obtain sharper confidence intervals.</p>
<p>One sided (lower or upper only) confidence bounds can be obtained and the confidence need not be expressed as an interval.</p>
<p>Also note that there many types of statistical intervals available in the literature, and the confidence interval is one such interval which is expected to capture an unknown <code>parameter</code> of the population. In our example, we used the mean of the population (parameter). Naturally, we can construct a CI for the median, 90th percentile etc. A prediction interval (which will discuss later on) deals with covering a desired fraction of the population and is different from a confidence interval.</p>
</section>
<section id="hypothesis-testing" class="level1">
<h1>Hypothesis Testing</h1>
<blockquote class="blockquote">
<p>“<em>… the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation. Every experiment may be said to exist only to give the facts a chance of disproving the null hypothesis.</em>”</p>
<p>– Sir R.A. Fisher.</p>
</blockquote>
<p>Note that the confidence intervals give a range of likely values for the mean <span class="math inline">\(\mu\)</span>. Putting it another way, if someone postulated a value of <span class="math inline">\(\mu\)</span> falling outside this interval (for example, that <span class="math inline">\(\mu=1000\)</span>), then we could tell them that our data do not support their claim. On the other hand if they hypothesise a value of <span class="math inline">\(\mu\)</span> inside the interval, (for example that <span class="math inline">\(\mu\)</span> = 1600), then this value may not be exactly what our data suggest, but we could not reject their claim.</p>
<p>These thoughts lead to the more formal idea of hypothesis testing. In hypothesis testing we begin with a hypothesis about the value of some parameter, for example that the population mean <span class="math inline">\(\mu\)</span> equals some specific value <span class="math inline">\(\mu_0\)</span> say. In the television example we could use the specific value <span class="math inline">\(\mu_0=1500\)</span> hours, say, but for the purpose of general discussion we prefer to just assume <span class="math inline">\(\mu_0\)</span> is some fixed value.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Two important points to remember about null hypotheses:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Null (and alternative) hypotheses are <strong>always</strong> about population parameters, and <strong>never</strong> about sample statistics; inferences are always about the population, never about the sample.</li>
<li>With a large <span class="math inline">\(p\)</span>-value, we <strong>never</strong> say that we have “accepted” or “confirmed” the null hypothesis; we only ever reject or fail to reject a null hypothesis.</li>
</ul>
</div>
</div>
<p>The philosophy of classical statistical hypothesis testing was explained by <span class="citation" data-cites="fisher1935">Fisher (<a href="#ref-fisher1935" role="doc-biblioref">1935</a>)</span> using a context known as the “tea tasting lady” experiment. The original tea tasting experiment used a permutation test but we simplify this context in the following description.</p>
<p>A lady claimed that she can taste and tell whether milk or tea was poured first into the cup. Suppose that we tossed a fair coin to determine this in a series of trials. Coin tossing enables the binomial probability distribution as a model to determine the probability of various outcomes when tea tasting is done in a random order. Recall that the binomial mass function is given by-</p>
<p><span class="math display">\[P(X=x) = \binom{n}{x} p ^ x (1-p) ^ {n-x}\]</span></p>
<p>The <em>null hypothesis</em> is that the lady was guessing blindly. That is, there is equal probability for a correct guess and an incorrect one. The sample space here consists of all possible answers the lady might give in a series of random tasting of tea cups. Assume that 10 cups (with equal cases of milk and tea poured first) were randomly arranged and the lady guessed 9 cases correctly. We can calculate the probability of 9 correct guesses or more under the null hypothesis using the binomial distribution. We can find the probability of certain <span class="math inline">\(x\)</span> number of correct guesses under the null hypothesis using <code>R</code>. See <a href="#tbl-binom1">Table&nbsp;1</a>.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x, <span class="at">Probability=</span><span class="fu">round</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pbinom</span>(x, <span class="dv">10</span>, <span class="fl">0.5</span>), <span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-binom1" class="anchored">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<caption>Table&nbsp;1: Binomial probabilities</caption>
<thead>
<tr class="header">
<th style="text-align: right;" data-quarto-table-cell-role="th">x</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.999</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.989</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">0.945</td>
</tr>
<tr class="even">
<td style="text-align: right;">3</td>
<td style="text-align: right;">0.828</td>
</tr>
<tr class="odd">
<td style="text-align: right;">4</td>
<td style="text-align: right;">0.623</td>
</tr>
<tr class="even">
<td style="text-align: right;">5</td>
<td style="text-align: right;">0.377</td>
</tr>
<tr class="odd">
<td style="text-align: right;">6</td>
<td style="text-align: right;">0.172</td>
</tr>
<tr class="even">
<td style="text-align: right;">7</td>
<td style="text-align: right;">0.055</td>
</tr>
<tr class="odd">
<td style="text-align: right;">8</td>
<td style="text-align: right;">0.011</td>
</tr>
<tr class="even">
<td style="text-align: right;">9</td>
<td style="text-align: right;">0.001</td>
</tr>
<tr class="odd">
<td style="text-align: right;">10</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
<p><a href="#tbl-binom1">Table&nbsp;1</a> shows that the evidence in our data is well against the null hypothesis because the probabilities 9 or 10 correct answers are rather small.</p>
<p>So we cannot reject the lady’s claim. We often write our null hypothesis symbolically. For the tea lady example, the null hypothesis is written as <span class="math inline">\(H_0:p = 0.5\)</span>, where <span class="math inline">\(H_0\)</span> is pronounced ‘H nought’ and means ‘the null hypothesis we are testing’ and <span class="math inline">\(p\)</span> stands for the unknown proportion and <span class="math inline">\(p=0.5\)</span> stands for guessing the outcome by throwing a coin.</p>
<p>In our testing we give the <span class="math inline">\(H_0\)</span> the ‘benefit of the doubt’. That is, we won’t reject <span class="math inline">\(H_0\)</span> unless we are strongly convinced by the data that it cannot be right. There is some analogy here to a court case, where the defendant (the hypothesis) is assumed innocent until proven guilty beyond all reasonable doubt (hypothesis assumed true until the data prove it false beyond all reasonable doubt). Unlike a court case, however, we are usually able to use probability models to <em>quantify</em> the level of doubt/disbelief in <span class="math inline">\(H_0\)</span>. Another example is that a student may like to verify the weight labelling on butter sold in the supermarket is correct or not. Assume that the student collects a sample 20 blocks of butter with 500g nominal weight declared. In this case, the null hypothesis would be that the true mean weight is indeed 500g. In legal metrology, formulation of such a null hypothesis is rather natural. <span class="citation" data-cites="Hall1986">Hall and Selinger (<a href="#ref-Hall1986" role="doc-biblioref">1986</a>)</span> provided a good discussion on the nature of hypothesis testing in his paper entitled “Statistical significance: Balancing evidence against doubt” (not compulsory but worth a read).</p>
<p>We often use software to perform hypothesis tests. The <code>R</code> output performing a one-sample proportion test output on whether the lady was outperforming the mere guessing strategy is shown below:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="at">x=</span><span class="dv">9</span>, <span class="at">n=</span><span class="dv">10</span>, <span class="at">p=</span><span class="fl">0.5</span>, <span class="at">alternative=</span><span class="st">"greater"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Exact binomial test

data:  9 and 10
number of successes = 9, number of trials = 10, p-value = 0.01074
alternative hypothesis: true probability of success is greater than 0.5
95 percent confidence interval:
 0.6058367 1.0000000
sample estimates:
probability of success 
                   0.9 </code></pre>
</div>
</div>
<p>Do not worry the entries <code>P-value</code> and <code>alternative</code> appearing in the above output and these concepts are further explained in the later section.</p>
<p>As pointed out by Fisher, our hypothesis testing procedure does not prove or disprove the null hypothesis. We are simply assessing the evidence in the data against the null hypothesis. While describing the tea tasting experiment, <span class="citation" data-cites="fisher1935">Fisher (<a href="#ref-fisher1935" role="doc-biblioref">1935</a>)</span> (p.16) warned as follows:</p>
<blockquote class="blockquote">
<p>“<em>In relation to any experiment we may speak of this as the ”null hypothesis”, and it should be noted that the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation.</em>”</p>
</blockquote>
<p>More ideas on hypothesis testing follow in the next section.</p>
<section id="hypothesis-testing-for-mean" class="level2">
<h2 class="anchored" data-anchor-id="hypothesis-testing-for-mean">Hypothesis testing for mean</h2>
<p>For testing the mean of a population, the fact that we have specified a parameter value in <span class="math inline">\(H_0\)</span> enables us to specify a probability distribution, for example that the data are a random sample from N(<span class="math inline">\(\mu_0\)</span>, <span class="math inline">\(\sigma\)</span>). This immediately implies that</p>
<p><span class="math inline">\(t = \frac{\bar{y}-\mu _{0} }{S/\sqrt{n} } \sim t{}_{n-1}\)</span></p>
<p>Now if the hypothesis <span class="math inline">\(H_0\)</span> is true, we should have <span class="math inline">\(\mu\cong\mu_0\)</span> . The notation equal-with-squiggle, <span class="math inline">\(\cong\)</span>, stands for “approximately equal to” ), so that <span class="math inline">\(t\)</span> should generally be close to 0. If the hypothesis is false we should get either much less than <span class="math inline">\(\mu_0\)</span> or much greater than <span class="math inline">\(\mu_0\)</span>, in which case <span class="math inline">\(t\)</span> should be large, out in the tails of the <span class="math inline">\(t_{n-1}\)</span> distribution. Since we have a probability distribution, we can quantify just how unlikely our particular <span class="math inline">\(t\)</span> is by comparing our sample value with the distribution.</p>
<p>Let’s make things more specific by considering the television example. The degrees of freedom are <span class="math inline">\(n-1 = 45\)</span>. We have already indicated that <span class="math inline">\(t\cong0\)</span> implies <span class="math inline">\(\mu\cong\mu_0\)</span>, in other words that the data matches the hypothesis very closely. But suppose instead we observed <span class="math inline">\(t= 0.68\)</span>. Could we regard this as an unusually large value of <span class="math inline">\(t\)</span>, that is, as evidence against <span class="math inline">\(H_0\)</span>? The answer is no! The reason is that 0.68 is the upper quartile of the <span class="math inline">\(t_{45}\)</span> distribution; in other words half (50%) of the time we would see values of <span class="math inline">\(t\)</span> either greater than 0.68 or less than -0.68, even if the hypothesis <span class="math inline">\(H_0\)</span> is true. Now what if <span class="math inline">\(t\)</span> were below -1.6794? Would that be regarded as an unusual amount of discrepancy between <span class="math inline">\(t\)</span> and <span class="math inline">\(\mu\)</span>, that is as evidence against <span class="math inline">\(H_0\)</span>? The answer is maybe. The fact is that 10% of the time one sees <span class="math inline">\(t\)</span> <span class="math inline">\(&lt;-1.6794\)</span> or <span class="math inline">\(t&gt;1.6794\)</span>, even if <span class="math inline">\(H_0\)</span> is true. So if we use this rule, we have a 10% chance of wrongly rejecting <span class="math inline">\(H_0\)</span>. Few New Zealanders would feel happy with a legal system that allowed a 10% chance of wrongfully convicting an innocent person. Finally, what if we observe <span class="math inline">\(t= -2.6896\)</span>? Only 1% of the area under the <span class="math inline">\(t_{45}\)</span> curve lies outside the interval -2.6896 to +2.6896, so we would conclude that such a value of <span class="math inline">\(t\)</span> was quite unlikely. This then would be strong evidence against <span class="math inline">\(H_0\)</span>.</p>
<p>Suppose now we test the extremely unlikely hypothesis <span class="math inline">\(H_0:\mu = 0\)</span>. Since <span class="math inline">\(S/\sqrt n=83.7\)</span>, we obtain <span class="math inline">\(t = (1729.3-0)/83.7 = 20.65\)</span>. This is far larger than could be expected by chance, so the <span class="math inline">\(t\)</span> statistic provides clear evidence against <span class="math inline">\(H_0\)</span>.</p>
<p>A more realistic test may be whether <span class="math inline">\(H_0:\mu = 1500\)</span>. Perhaps a previous study found an average time of viewing of 1500 minutes per week, and we want to check this. The test statistic is: <span class="math inline">\(t=(1729.3-1500)/83.7=2.74\)</span>. This is just outside our 1% bounds established earlier, so we conclude the data and <span class="math inline">\(H_0\)</span> do not seem to agree. We reject <span class="math inline">\(H_0:\mu=1500\)</span>.</p>
<p>Finally suppose we test the hypothesis <span class="math inline">\(H_0:\mu=1600\)</span>. Again this hypothesis could have been prompted by a previous study. Then <span class="math inline">\(t=(1729.3-1600)/83.7=1.54\)</span>. This is within the interval -1.6794 to 1.6794, suggesting that 1.54 is not an unusually highly value since it is exceeded more than 10% of the time when <span class="math inline">\(H_0\)</span> is true. We would not reject the hypothesis <span class="math inline">\(H_0:\mu = 1600\)</span>.</p>
<p>When statistical test of hypothesis is done using <code>R</code> software, we rely on the <span class="math inline">\(p\)</span>-value displayed.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(tv<span class="sc">$</span>TELETIME, <span class="at">mu=</span><span class="dv">1500</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    One Sample t-test

data:  tv$TELETIME
t = 2.7382, df = 45, p-value = 0.008818
alternative hypothesis: true mean is not equal to 1500
95 percent confidence interval:
 1560.633 1897.932
sample estimates:
mean of x 
 1729.283 </code></pre>
</div>
</div>
<p>So what is a <span class="math inline">\(P\)</span>-value? Formally, <strong><em>A P-value is the probability of observing data as extreme or more extreme as the data you actually observed, if</em></strong> <span class="math inline">\(H_0\)</span> is true. This sounds like a very abstract and difficult concept to grasp, but it’s in fact exactly the rule we have been using. We saw that 50% of the time, <span class="math inline">\(t&lt;-0.68\)</span> or <span class="math inline">\(t&gt;0.68\)</span>. So the <span class="math inline">\(p\)</span>-value for a <span class="math inline">\(t=0.68\)</span> is 0.5. We saw that 10% of the time, <span class="math inline">\(t&lt;-1.6794\)</span> or <span class="math inline">\(t&gt;1.6794\)</span>. So the <span class="math inline">\(p\)</span>-value of <span class="math inline">\(t= -1.6794\)</span> is 0.1. And we saw that 1% of the time <span class="math inline">\(t&lt;-2.6896\)</span> or <span class="math inline">\(t&gt;2.6896\)</span>, so the <span class="math inline">\(p\)</span>-value of <span class="math inline">\(t=2.6896\)</span> is 0.01. What is the <span class="math inline">\(p\)</span>-value of <span class="math inline">\(t= 2.74\)</span>? The answer is 0.009 or just under 1%. What is the <span class="math inline">\(p\)</span>-value of <span class="math inline">\(t= 1.54\)</span>? Answer is 0.130.</p>
<p>Usually, we reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_{1}\)</span> if the <span class="math inline">\(p\)</span>-value of the data is <span class="math inline">\(&lt; 0.05\)</span>. In this case the test is said to be <strong>significant</strong>, and the 0.05 is called the <strong>significance level</strong> of the test. Otherwise (when we don’t reject <span class="math inline">\(H_0\)</span>) we call the test <strong>non-significant</strong>. Some refer to a <span class="math inline">\(p\)</span>-value <span class="math inline">\(&lt;0.01\)</span> as very significant or highly significant. In journal articles and published tables of results, the three case non-significant, significant, and highly significant are often abbreviated as <code>NS</code>, <code>*</code> and <code>**</code> respectively. The issues of using a standard cut-off of 5% for significance level, largely insisted in journals, has had its unintended consequences. The <em>false discovery</em> in science can be avoided if P values are not used as the sole criterion to draw conclusions. Read the advice on P values presented next.</p>
<p><strong>Advice on the use of P values</strong></p>
<p>Unfortunately the P-values are often misunderstood in practice. The advice issued by the American Statistical Association (<a href="https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf" class="uri">https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf</a>) is noteworthy:</p>
<blockquote class="blockquote">
<p>The statement’s six principles, many of which address misconceptions and misuse of the p-value, are the following:</p>
<ol type="1">
<li><p>P-values can indicate how incompatible the data are with a specified statistical model.</p></li>
<li><p>P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.</p></li>
<li><p>Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.</p></li>
<li><p>Proper inference requires full reporting and transparency.</p></li>
<li><p>A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.</p></li>
<li><p>By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.</p></li>
</ol>
</blockquote>
<p>In order to fully appreciate the reasons behind the above advice, we need learn more theory but you should be able to understand the following distinction between <em>statistical</em> significance and <em>practical</em> significance. A hypothesis test may suggest that the estimated size of the effect is not big enough compared to the effect that can occur due to errors under the assumed model. A small effect, particularly if it is known to be <em>caused</em> by a variable, can be of practical importance and can contribute to scientific knowledge. The opposite scenario is also possible. We may find a small difference to be statistically significant because of the large sample size but such a difference may not be of much practical significance.</p>
<p>A hypothesis test has a certain power (probability) to reject the null hypothesis when it is false. This power (probability) is a function of the sample size and the unknown parameters of the probability model adopted for testing.</p>
<p>Assume that we are testing the null hypothesis <span class="math inline">\(H_0:\mu = 0\)</span> using the null model <span class="math inline">\(N(0,1)\)</span>. The power of the one sample t test can be evaluated using the <code>R</code> function <code>power.t.test()</code> for a given <span class="math inline">\(\delta\)</span>, the difference in the true mean and what was hypothesised under <span class="math inline">\(H_0\)</span>. For example, the power of the <span class="math inline">\(t\)</span>-test for <span class="math inline">\(n=30\)</span> is lower than the power when <span class="math inline">\(n=50\)</span> (say) when other settings are the same. Try-</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">30</span>, <span class="at">delta =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.05</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">50</span>, <span class="at">delta =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The power to detect a small change in the mean is often low. Try-</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">30</span>, <span class="at">delta =</span> .<span class="dv">25</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.05</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">30</span>, <span class="at">delta =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>There is a trade-off between the significance level (Type I error or false positive) and Type II error or false negative (=1-power) probabilities. Try-</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">30</span>, <span class="at">delta =</span> <span class="fl">0.5</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.05</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">30</span>, <span class="at">delta =</span> <span class="fl">0.5</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>When we test many hypothesis in tandem, we are more concerned on the overall or family-wise error rates. The issues of false discovery in science is discussed in a later section.</p>
<p><em>P hacking</em> is a phrase used when a particular test or a meta procedure is deliberately chosen either to ensure a low p-value or just to achieve a value below 0.05.</p>
</section>
</section>
<section id="inferences-for-two-groups" class="level1">
<h1>Inferences for Two Groups</h1>
<p>In an earlier section, we considered confidence intervals and hypothesis tests for the television viewing times of pupils (<strong>tv</strong>). Now this sample can be divided into two groups, viewing times for boys and viewing times for girls. The boxplots in <a href="#fig-tvdistsex">Figure&nbsp;6</a> indicate that the median viewing time for boys (Sex=1) is less than for girls (Sex=2) although the spread of times is greater for the group of boys. The slopes in the normal quantile plot also confirms the differing spread.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tv) <span class="sc">+</span> </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">sample =</span> TELETIME, <span class="at">color=</span>SEX, <span class="at">shape=</span>SEX) <span class="sc">+</span> </span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq</span>() <span class="sc">+</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="fu">aes</span>(<span class="at">linetype=</span>SEX))</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tv) <span class="sc">+</span> </span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>TELETIME, <span class="at">x=</span>SEX) <span class="sc">+</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1,p2, <span class="at">ncol=</span><span class="dv">1</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tvdistsex" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-tvdistsex-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Distribution of TV viewing times for boys and girls</figcaption>
</figure>
</div>
</div>
</div>
<p>We see that average number of minutes per week of television watched is 1668 for boys and 1790 for girls. The difference in sample means for these two groups is <span class="math inline">\(1668-1790=-122\)</span>. The question arises as to whether this indicates a real difference in viewing times between the sexes in the population as a whole (all New Zealand primary age pupils?). We should keep in mind the distinction between samples and populations and, to do this, population values are usually written in Greek letters. The notation is:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Population means and SDs</th>
<th style="text-align: right;">Sample means and SDs</th>
<th style="text-align: left;">Sample size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\mu _1 ,\,\,\sigma _1\)</span></td>
<td style="text-align: right;"><span class="math inline">\(\bar {y}_1 \,\,,\,\,S_1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(n_1\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mu _2 ,\,\,\sigma _2\)</span></td>
<td style="text-align: right;"><span class="math inline">\(\bar {y}_2 \,\,,\,\,S_2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(n_2\)</span></td>
</tr>
</tbody>
</table>
<p>We are assuming that we have information about the samples and wish to make inferences about the populations. In particular, we wish to make inferences about the difference in population means, (<span class="math inline">\(\mu _{1}-\mu _{2}\)</span>), from the difference in sample means, <span class="math inline">\(\bar{y}_{1} -\bar{y}_{2}\)</span>, and the variances of the two samples, <span class="math inline">\(S_{1}^{2}\)</span> and <span class="math inline">\(S_{2}^{2}\)</span>.</p>
<section id="hypothesis-tests-for-two-groups" class="level2">
<h2 class="anchored" data-anchor-id="hypothesis-tests-for-two-groups">Hypothesis tests for two groups</h2>
<p>The null hypothesis is that population means of the two groups are equal (written as <span class="math inline">\(H_0:\mu_1 = \mu_2\)</span> or <span class="math inline">\(H_0:\mu_1-\mu_2=0\)</span>). The alternative hypothesis is that the population means of the two groups are different (i.e., <span class="math inline">\(H_0:\mu_1\neq \mu_2\)</span> or <span class="math inline">\(H_0:\mu_1-\mu_2\neq0\)</span>). Note that when the two population means are different either <span class="math inline">\(\mu_1&gt;\mu_2\)</span> or <span class="math inline">\(\mu_1&lt;\mu_2\)</span>.</p>
<p>The difference in sample means <span class="math inline">\(\bar{y}_{1} -\bar{y}_{2}\)</span> can be standardized to give a <span class="math inline">\(t\)</span> statistic:</p>
<p><span class="math inline">\(t\)</span> = ((<span class="math inline">\(\bar{y}_{1} -\bar{y}_{2}\)</span>)- expected)/e.s.e.</p>
<p>The test statistic is <span class="math inline">\((\bar{y}_{1} -\bar{y}_{2})\)</span>, which has the expected value of zero under the null hypothesis.</p>
<p>The estimated standard error (e.s.e) of (<span class="math inline">\(\bar{y}_{1} -\bar{y}_{2}\)</span>), is obtained in two ways depending on whether it is plausible or not to make the assumption that the variances within the populations are the same, (i.e.&nbsp;<span class="math inline">\(\sigma _{1}^{2} =\sigma _{2}^{2}\)</span>). Whether this assumption appears to be tenable or not can be explored using boxplots etc. For the television viewing time example, the variances of the TV viewing times do not appear to be the same for boys and girls. If the variances of the two populations are the same, then we will use a method of combining the individual variances of the groups to form a <strong>pooled variance</strong> estimate. To do this, we cannot simply average the two variances as the sample sizes may be quite different. A weighted sum is called for to give:</p>
<p>pooled estimate of variance, <span class="math inline">\(S_{p}^{2} =w_{1} S_{1}^{2} +w_{2} S_{2}^{2}\)</span></p>
<p>where the <strong>weights</strong> are, <span class="math inline">\(w_{1} =\frac{n_{1}-1}{n_{1} +n_{2}-2}\)</span> and <span class="math inline">\(w_{2} =\frac{n_{2}-1}{n_{1} +n_{2}-2}\)</span>. Hence the pooled estimate of the <em>standard deviation</em> is given by</p>
<p><span class="math inline">\(S_p = \sqrt{ w_1S_{1}^2 + w_2S_{2}^2 }\)</span> or</p>
<p><span class="math display">\[S_{p} =\sqrt{\frac{\left(n_{1}-1\right)S_{1}^{2} +\left(n_{2}-1\right)S_{2}^{2} }{n_{1} +n_{2} -2} }\]</span> Consequently,</p>
<p>Estimated standard error (<span class="math inline">\(\bar{y}_{i}\)</span>)=<span class="math inline">\(\frac{S_{p}}{\sqrt{n_{i}}},~~~i = 1, 2\)</span></p>
<p>so that</p>
<p>Estimated standard error (<span class="math inline">\(\bar{y}_{1}-\bar{y}_{2}\)</span>)=<span class="math inline">\(S_{p} \sqrt{1/n_{1}+1/n_{2}}\)</span></p>
<p>If the variances of the two populations are <strong>not</strong> the same, then we cannot pool the variances. Hence the estimated standard error for the difference in the two sample means is given by</p>
<p>Estimated standard error (<span class="math inline">\(\bar{y}_{1}-\bar{y}_{2}\)</span>)=<span class="math inline">\(\sqrt{\frac{S_{1}^{2}}{n_{1}} +\frac{S_{2}^{2}}{n_{2}}}\)</span></p>
<p>The degrees of freedom for our <span class="math inline">\(t\)</span>-test (called the <strong>two-sample</strong> <span class="math inline">\(t\)</span> test) depends on whether estimated standard error is based on the pooled variance or not. For the variance pooled case, the <span class="math inline">\(df\)</span> for the <span class="math inline">\(t\)</span>-test is <span class="math inline">\(n_{1}+n_{2}-2\)</span> but becomes smaller for the unpooled case to <span class="math display">\[df=\frac{\left(\frac{S_{1}^{2}}{n_{1}} +\frac{S_{2}^{2} }{n_{2}} \right)^{2} }{\frac{1}{n_{1} -1} \left(\frac{S_{1}^{2}}{n_{1}}\right)^{2} +\frac{1}{n_{2} -1} \left(\frac{S_{2}^{2}}{n_{2} } \right)^{2}}\]</span> That is, the <span class="math inline">\(df\)</span> is adjusted according to the ratio of the two variances. The <span class="math inline">\(t\)</span>-test is also approximate when the variances are not pooled and hence it would be advisable to perform a transformation as later outlined in this Chapter.</p>
<p>Hand calculations for the two sample <span class="math inline">\(t\)</span>-test are cumbersome. The test done on computer usually gives us an output which contains the <span class="math inline">\(t\)</span>-statistic value and the associated <span class="math inline">\(p\)</span>-value. Supplementary details such as the standard errors of the sample means, and their difference, associated <span class="math inline">\(df\)</span> etc will also be contained. The <code>R</code> output given below shows the two-sample test results for the TV viewing data set. Here we test whether the true mean TV viewing times are the same for boys and girls.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(TELETIME<span class="sc">~</span>SEX, <span class="at">data=</span>tv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Welch Two Sample t-test

data:  TELETIME by SEX
t = -0.7249, df = 40.653, p-value = 0.4727
alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0
95 percent confidence interval:
 -462.1384  218.0514
sample estimates:
mean in group 1 mean in group 2 
       1668.261        1790.304 </code></pre>
</div>
</div>
<p>Based on the EDA evidence seen in <a href="#fig-tvdistsex">Figure&nbsp;6</a>, we may take a conservative stand and prefer the unpooled two-sample <span class="math inline">\(t\)</span>-test (which is also known as Welch Two Sample t-test). The <span class="math inline">\(t\)</span>-value of -0.72 is not unusual as the probability of getting such an extreme value under the null hypothesis is <span class="math inline">\(p=0.47\)</span>. In other words, we cannot reject the null hypothesis; we accept it until we have more evidence to the contrary. Hence the conclusion of the <span class="math inline">\(t\)</span>-test is that the mean TV viewing times can be regarded as the same for the population of boys and girls. Alternatively there is no statistically significant gender effect on TV watching for boys and girls of Standards 2 to 4.</p>
</section>
<section id="confidence-intervals-for-the-difference-in-means" class="level2">
<h2 class="anchored" data-anchor-id="confidence-intervals-for-the-difference-in-means">Confidence Intervals for the Difference in Means</h2>
<p>The 95% Confidence Interval for the difference <span class="math inline">\(\left(\mu _{1} -\mu _{2} \right)\)</span> in population means is given by:</p>
<p>difference in sample means <span class="math inline">\(\pm t \times\)</span> e.s.e .</p>
<p>Or more specifically,</p>
<p>Interval estimate for <span class="math inline">\((\mu _1-\mu _2)\)</span> = <span class="math inline">\((\bar{y}_{1}-\bar{y}_{2})\pm t \times\)</span> e.s.e.<span class="math inline">\((\bar{y}_{1} -\bar{y}_{2})\)</span>.</p>
<p>Based on the <span class="math inline">\(t\)</span> quantile value of 2.021 for 40 <span class="math inline">\(df\)</span>, the CI in the unpooled case is</p>
<p><span class="math inline">\(-122 \pm2.021\times\sqrt{\frac{648^{2}}{23}+\frac{482^{2}}{23}}\)</span> or <span class="math inline">\((-462.3, 218.2)\)</span></p>
<p>The <span class="math inline">\(t\)</span>-test output for the null hypothesis <span class="math inline">\(H_{0}:\mu _1=\mu _2\)</span> (or <span class="math inline">\((\mu _1-\mu _2)= 0)\)</span> gives the confidence interval too. Notice that the CI actually includes zero as a possible value. This means that it is possible <span class="math inline">\((\mu_1-\mu_2)=0\)</span>; so we cannot reject the null hypothesis.</p>
</section>
<section id="paired-t-test" class="level2">
<h2 class="anchored" data-anchor-id="paired-t-test">Paired <span class="math inline">\(t\)</span> test</h2>
<p>Note that the two-sample data may be simply paired observations. For instance, a measurement may be made on the left and right eyes of the same person. If observations are paired in some way, a one-sample <span class="math inline">\(t\)</span>-test on the difference <span class="math inline">\(\left(X_{i} ,Y_{i} \right)\)</span> will suggest whether the true mean of the differences can be regarded as zero or not. Such a test will be more powerful than a two sample <span class="math inline">\(t\)</span>-test because of the correlation between the paired observations. If the correlation is weak, it is desirable to ignore the pairing variable and perform a two-sample <span class="math inline">\(t\)</span>-test.</p>
<p>Consider the maths and English test scores of students available in the data set <strong>testmarks</strong>. These test scores are paired being the scores of the same student. The correlation or linear relationship between the maths and English scores is high; see <a href="#fig-pairg">Figure&nbsp;7</a>.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">download.file</span>(</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">url =</span> <span class="st">"http://www.massey.ac.nz/~anhsmith/data/testmarks.RData"</span>,</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">destfile =</span> <span class="st">"testmarks.RData"</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"testmarks.RData"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(testmarks)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-pairg" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-pairg-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Relationship between Maths and English scores</figcaption>
</figure>
</div>
</div>
</div>
<p>The paired <span class="math inline">\(t\)</span>-test or the one-sample <span class="math inline">\(t\)</span>-test on the difference in scores gives a <span class="math inline">\(t\)</span>-statistic of 0.17 (<span class="math inline">\(p\)</span>-value of 0.868). This means that the true average difference in test scores can be regarded as zero or alternatively the true mean scores of maths and English can be regarded as equal.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(testmarks<span class="sc">$</span>Maths, testmarks<span class="sc">$</span>English, <span class="at">paired=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Paired t-test

data:  testmarks$Maths and testmarks$English
t = 0.16745, df = 39, p-value = 0.8679
alternative hypothesis: true mean difference is not equal to 0
95 percent confidence interval:
 -4.154646  4.904646
sample estimates:
mean difference 
          0.375 </code></pre>
</div>
</div>
</section>
</section>
<section id="transformations" class="level1">
<h1>Transformations</h1>
<p>A <strong>transformation</strong> is a function that is applied to each observation in a data set to make the distribution of the data symmetric (or even normal if possible). There are several reasons for this:</p>
<ol type="1">
<li><p>To make comparisons between two or more groups of data easier. Different samples may differ in a number of ways; different medians, spreads and even the shape of the distributions. The shape of a graph is actually quite hard to describe in words, let alone to compare numerically. But if we can transform the distributions to each have a similar shape (and especially to make them symmetric), then we can summarise all the remaining differences by numbers: medians, IQR, etc. This simplifies comparisons enormously.</p></li>
<li><p>It may enable us to describe the data by a simple model; for example the normal distribution. We can then use the model to: compute probabilities, such as the probability an observation will exceed a certain value; simulate new data, to help us predict what may happen in the future; compute confidence intervals for parameters; and for comparative statistical inference, for example computing <span class="math inline">\(p\)</span>-values for differences in group means.</p></li>
<li><p>To make the variances of groups of data nearly equal. This is needed in the fitting of certain statistical models.</p></li>
</ol>
<p>Of course for much statistical inference we don’t need the data to be normal itself, but can rely on the central limit theorem. But results based on the CLT will be more dependable if the data are at least symmetric, or can be made symmetric, as we shall see in an example. Thus in this chapter we shall be mainly concerned with transforming data to symmetry (with perhaps a secret longing that the data will be almost normally distributed).</p>
<section id="transformation-and-shape" class="level2">
<h2 class="anchored" data-anchor-id="transformation-and-shape">Transformation and shape</h2>
<p>When considering data, we may decide to use the measurements as they are, or we may rescale them. For example, we may change them to percentages of the total. As a simple example, consider a town with four stores in which the weekly turnovers are one, two, four and eight thousand dollars. These could be rescaled to percentages of the total (which is 15).</p>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 21%">
<col style="width: 23%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<tbody>
<tr class="odd">
<td>Raw data</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>8</td>
</tr>
<tr class="even">
<td>Data in %</td>
<td>1/15 <span class="math inline">\(\times\)</span> 100= 6.7%</td>
<td>2/15 <span class="math inline">\(\times\)</span> 100 = 13.3%</td>
<td>4/15 <span class="math inline">\(\times\)</span> 100= 26.7%</td>
<td>8/15 <span class="math inline">\(\times\)</span> 100= 53.3%</td>
</tr>
</tbody>
</table>
<p>If you were to compare a dotplot of the original data with a dotplot of the rescaled data you would find that the <strong>shape</strong> of the data had <strong>not changed</strong> by this rescaling. That is, the second percent is twice the first and the second weekly turnover is almost twice the first; the third percent is four times the first and twice the second and so on. This is an example of a <strong>linear</strong> transformation.</p>
<p>A linear transformation is one that can be described by the formula, <span class="math inline">\(y=a+bx\)</span> for certain constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, and where <span class="math inline">\(x\)</span> is the old data and <span class="math inline">\(y\)</span> is the transformed data. The key thing about linear transformations is that they do not change the shape of a dotplot, only the scale. Another linear example is converting temperature data from Fahrenheit (<span class="math inline">\(x\)</span>) to centigrade <span class="math inline">\(y = 5(x-32)/9\)</span>. Boxplots of the temperatures would <em>look</em> the same even though the scale was altered.</p>
<p>Another way of rescaling the store example would be to calculate the weekly turnover of a store divided by the number of employees in that store, to give weekly turnover per employee. Even though this looks like a linear transformation it is not, since the relative positions of the four stores on a scale would change depending on the number of employees. If we have two or more variables (e.g.&nbsp;turnover, employees) it is often useful to look at ratios like this to seek simple explanations of the data.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>rht <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">RHT=</span><span class="fu">rbeta</span>(<span class="fl">1e3</span>, <span class="dv">1</span>,<span class="dv">5</span>))</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rht) <span class="sc">+</span> </span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(RHT) <span class="sc">+</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"(a) Needs a Shrinking Transformation"</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rht) <span class="sc">+</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>RHT, <span class="at">x=</span><span class="st">"Right Skewed Data"</span>) <span class="sc">+</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>lht <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">LHT=</span><span class="fu">rbeta</span>(<span class="fl">1e3</span>, <span class="dv">5</span>,<span class="dv">1</span>))</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(lht) <span class="sc">+</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(LHT) <span class="sc">+</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"(a) Needs a Stretching Transformation"</span>)</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>p4 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(lht) <span class="sc">+</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>LHT, <span class="at">x=</span><span class="st">"Left Skewed Data"</span>) <span class="sc">+</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1, p3, p2, p4, <span class="at">ncol=</span><span class="dv">2</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-transf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-transf-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;8: Transformations</figcaption>
</figure>
</div>
</div>
</div>
<p>In this chapter, we focus on transformations involving just one variable (though perhaps more than one batch of data on that variable). <strong>Our goal is to change the shape of a distribution</strong> to make it more symmetric. Consider the two distributions in <a href="#fig-transf">Figure&nbsp;8</a> in which (a) is skewed to the left and (b) to the right. These could be made more symmetric by stretching the large values in (a) but shrinking them in (b).</p>
</section>
<section id="the-ladder-of-powers" class="level2">
<h2 class="anchored" data-anchor-id="the-ladder-of-powers">The Ladder of Powers</h2>
<p>From now on we assume that the data are very skewed, and we wish to transform it, (or, in Tukey’s terms, re-express it) to be as symmetrical as possible. A simple approach considers the data raised to different powers, that is, if the original data are <span class="math inline">\(x\)</span> and the new data are <span class="math inline">\(y\)</span>,</p>
<p><span class="math display">\[y =\left\{\begin{array}{l} {
\text {sign} (\lambda )x^\lambda~~~~~ \lambda \ne 0} \\
{\log(x)~~~~~~~~~~~\lambda = 0} \end{array} \right.\]</span></p>
<p>Note that the Greek letter <span class="math inline">\(\lambda\)</span> is pronounced as lambda. Here, (<span class="math inline">\(\text {sign}(\lambda)\)</span>) is +1 if <span class="math inline">\(\lambda&gt;0\)</span>, and <span class="math inline">\(\text {sign}(\lambda)=-1\)</span> if <span class="math inline">\(\lambda&lt;0\)</span>, for reasons discussed below. Some special cases of this <strong>power transformation</strong> are set out below:</p>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 32%">
<col style="width: 23%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">POWER</th>
<th style="text-align: left;">Formula</th>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">3</td>
<td style="text-align: left;"><span class="math inline">\(x^3\)</span></td>
<td style="text-align: left;">cube</td>
<td style="text-align: left;">stretches large values</td>
</tr>
<tr class="even">
<td style="text-align: left;">2</td>
<td style="text-align: left;"><span class="math inline">\(x^2\)</span></td>
<td style="text-align: left;">square</td>
<td style="text-align: left;">stretches large values</td>
</tr>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: left;"><span class="math inline">\(x\)</span></td>
<td style="text-align: left;">raw</td>
<td style="text-align: left;">No change</td>
</tr>
<tr class="even">
<td style="text-align: left;">1/2</td>
<td style="text-align: left;"><span class="math inline">\(\sqrt{x}\)</span></td>
<td style="text-align: left;">square root</td>
<td style="text-align: left;">squashes large values</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="math inline">\(\log{x}\)</span></td>
<td style="text-align: left;">logarithm</td>
<td style="text-align: left;">squashes large values</td>
</tr>
<tr class="even">
<td style="text-align: left;">-1/2</td>
<td style="text-align: left;"><span class="math inline">\(\frac{-1}{\sqrt{x}}\)</span></td>
<td style="text-align: left;">reciprocal root</td>
<td style="text-align: left;">squashes large values</td>
</tr>
<tr class="odd">
<td style="text-align: left;">-1</td>
<td style="text-align: left;"><span class="math inline">\(\frac{-1}{x}\)</span></td>
<td style="text-align: left;">reciprocal</td>
<td style="text-align: left;">squashes large values</td>
</tr>
</tbody>
</table>
<p>Raising the data to the power of 1 does not change it at all; as we proceed down or up from 1, the strength of the transformation increases. The special case <span class="math inline">\(\lambda\)</span>=0 has to be handled differently since <span class="math inline">\(x^0=1\)</span> for all non-zero <span class="math inline">\(x\)</span>. Instead we conventionally regard it as being equivalent to taking the natural logarithm because the transformation <span class="math inline">\(\frac{x^{\lambda } }{\lambda } -\frac{1}{\lambda }\)</span> is close to the logarithmic transformation if <span class="math inline">\(\lambda\)</span> is small. The ‘common’ logarithm to base 10 could be used but it just yields a constant multiple of the natural logarithm (<code>ln</code> = log to the base <span class="math inline">\(e\)</span>). Now regarding the <span class="math inline">\(\text {sign}(\lambda)\)</span>: Notice that with two numbers, say 2 and 5, the reciprocal transformation would yield 0.5 and 0.2 so that, whereas the original numbers are increasing in size the transformed values are decreasing. To keep the order the same we take the negative of the reciprocal values, -0.5 and -0.2. These are again increasing. The same principle holds for all transformations where <span class="math inline">\(\lambda\)</span> is negative. <span class="math inline">\(\text {sign}(\lambda)\)</span> is employed to keep the order the same as the raw data. (Alternatively we could divide by <span class="math inline">\(\lambda\)</span> which is consistent with the case of power zero that is the logarithm transformation).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>vehicle<span class="sc">^</span><span class="dv">2</span>, <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Square Transformation"</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>vehicle, <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span> </span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Raw Data"</span>)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>vehicle<span class="sc">^</span>.<span class="dv">5</span>, <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span> </span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Square-root Transformation"</span>)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>p4 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span> </span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span><span class="fu">log</span>(vehicle), <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"log Transformation"</span>)</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1,p3,p2, p4, <span class="at">ncol=</span><span class="dv">2</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rangitransf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-rangitransf-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;9: Effect of Transformations</figcaption>
</figure>
</div>
</div>
</div>
<p>As an example, the boxplots in <a href="#fig-rangitransf">Figure&nbsp;9</a> represent the number of vehicles at the two Rangitikei river locations (from <strong>rangitikei</strong>). The first boxplot shows the square of the number of vehicle, which is highly right-skewed. The second boxplot shows the raw data (vehicle) which are still skewed towards the larger values. So to squash these to make the distribution more symmetric, the ladder of powers suggests we could try a square root transformation, or a stronger one such as the logarithm (or reciprocal root). These are shown in the third and fourth boxplots.</p>
</section>
<section id="checking-the-adequacy-of-a-transformation" class="level2">
<h2 class="anchored" data-anchor-id="checking-the-adequacy-of-a-transformation">Checking the adequacy of a transformation</h2>
<p>Since the purpose of transforming the data is to make it more symmetric, we need to decide if we have achieved this goal. One approach which has been suggested is to consider the difference mean-median where the vertical lines stand for “the absolute value of” which means we ignore a negative sign if it occurs. If the batch is symmetric then the mean equals the median so that the difference mean - median= 0. We could design a measure, D, of symmetry by standardising it. That is, we divide this difference by a measure of spread - we could choose either the standard deviation or the F spread, whichever is at hand. These choices give the so-called <strong>D-Statistics</strong>:</p>
<p>D1 = (mean - median)/ std.dev. or</p>
<p>D2 = (mean - median)/F-spread.</p>
<p>Instead of calculating the mean, we could use the mid-F to give</p>
<p>D3 = (mid-F - median)/F-spread.</p>
<p>We first choose one of these criteria, and then apply it to all the transformations in turn, choosing as ‘best’ that transformation which gives the smallest absolute value of D (provided that other considerations do not disqualify it). Note that, in practice it is useful <em>not</em> to take absolute values when calculating the D-statistics, since a change of sign helps us to locate the zero of D, i.e.&nbsp;the direction of skewness of the data.</p>
<p>The following table gives the values of D1, D2, and D3 (with sign retained) for the four transformations.</p>
<table class="table">
<colgroup>
<col style="width: 19%">
<col style="width: 13%">
<col style="width: 19%">
<col style="width: 26%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">D-statistic</th>
<th style="text-align: left;">vehicle</th>
<th style="text-align: left;">vehicle<span class="math inline">\(^2\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\sqrt{vehicle}\)</span></th>
<th style="text-align: left;">loge(vehicle)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">D<span class="math inline">\(_{1}\)</span></td>
<td style="text-align: left;">0.2724</td>
<td style="text-align: left;">0.2831</td>
<td style="text-align: left;">0.1308</td>
<td style="text-align: left;">-0.0945</td>
</tr>
<tr class="even">
<td style="text-align: left;">D<span class="math inline">\(_{2}\)</span></td>
<td style="text-align: left;">0.2973</td>
<td style="text-align: left;">1.0615</td>
<td style="text-align: left;">0.0978</td>
<td style="text-align: left;">-0.0611</td>
</tr>
<tr class="odd">
<td style="text-align: left;">D<span class="math inline">\(_{3}\)</span></td>
<td style="text-align: left;">0.0714</td>
<td style="text-align: left;">0.2273</td>
<td style="text-align: left;">-0.0183</td>
<td style="text-align: left;">-0.1092</td>
</tr>
</tbody>
</table>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"http://www.massey.ac.nz/~anhsmith/161250/eda/lval.R"</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lval</span>(<span class="fu">log</span>(rangitikei<span class="sc">$</span>vehicle))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rangilval" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-rangilval-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;10: Letter values of Vechicle data</figcaption>
</figure>
</div>
</div>
</div>
<p>The D-statistics D1 and D2 presented above suggest that the natural log would be the best transformation. But D3 values suggest in favour of the square root transformation. Note that D3 is sensitive only to skewness in the middle 50% of the data, whereas D1 and D2 are sensitive to skewness in the whole data set. The boxplots of <a href="#fig-rangitransf">Figure&nbsp;9</a> would tend to confirm these observations on the D-statistics. Note also that <span class="math inline">\(\text {vehicle} ^2\)</span>, <span class="math inline">\(\text {vehicle}\)</span> and <span class="math inline">\(\sqrt{\text {vehicle}}\)</span> have produced positive values for the D-statistics indicating that these variables are skewed to the right (i.e.&nbsp;positive skewness) while <span class="math inline">\(\text {loge(vehicle)}\)</span> data are mildly negatively skewed.</p>
<p>Having chosen the transformation it is a good idea to check it graphically using a <strong>mids vs.&nbsp;spreads plot</strong>. The plot in <a href="#fig-rangilval">Figure&nbsp;10</a> shows a consistent upward trend in the untransformed (raw) data (right-skewed). Again a general upward trend is seen for squared data indicating that the skewness is not fully corrected. The square root data still shows the positive trend, which means that the transformation has failed to correct the skew in the non-middle 50% of the data. For logged data, the mids vs.&nbsp;spread plot shows a random scatter of points. This confirms the conclusion from the D statistic that the log transformation works well to correct the skew in the non-middle part of the data. Note that we may also compute the D-Statistics using the IQR in place of F-spread and the results will often be similar.</p>
</section>
<section id="some-words-of-caution-about-transformations" class="level2">
<h2 class="anchored" data-anchor-id="some-words-of-caution-about-transformations">Some Words of Caution About Transformations</h2>
<ol type="1">
<li><p>The D values can be compared for the same data set but we should be wary of comparing the D value from one data set with that of another (possibly transformed) data set.</p></li>
<li><p>Any one statistic can never capture all the peculiarities of a data set so that the D values should be used in conjunction with other measures and/or graphs.</p></li>
<li><p>If the data set is small, transformations should be approached with some scepticism, for if more data were available the shape of the distribution may change.</p></li>
<li><p>It should be kept in mind that there are different levels at which data can be considered such as (a) just a collection of numbers (any reasonable transformation would suffice); (b) referring to physical quantities (certain transformations may make physical sense and allow meaningful interpretations to be made) and (c) outcomes from a certain process (for example, frequencies or counts may often suggest a Poisson distribution for which a square root transformation is suitable). The choice of a transformation may depend on the additional information that is known about the batch of data.</p></li>
<li><p>Common sense should prevail in this area as a transformation which brings only marginal improvement to symmetry, for example, should be balanced against other drawbacks such as the difficulty of interpreting the results. For example, the logarithm function turns multiplications to additions and powers to multiplications. Hence it turns divisions to subtractions and roots to divisions. Thus the geometric mean becomes the arithmetic mean and the ratio of geometric means becomes the difference between arithmetic means. Therefore reversing the transformation implies that a confidence interval for the difference between the means of the transformed data becomes a confidence interval for the ratio of the geometric means of the two groups of raw data.</p></li>
<li><p>Although a transformation may lead to symmetry it may be better to consider other approaches. For example, the stem-and-leaf or dotplot display may show that there are at least two subgroups in the data. Transformations do not really solve the problem and we may have to subdivide the data into groups.</p></li>
<li><p>Note that there are several other transformation functions available. For example, transformations such as arcsine are useful for proportion data.</p></li>
</ol>
</section>
<section id="box-cox-normalising-transformations" class="level2">
<h2 class="anchored" data-anchor-id="box-cox-normalising-transformations">Box-Cox Normalising transformations</h2>
<p>A systematic approach to power transformations was developed by <span class="citation" data-cites="boxcox">Box and Cox (<a href="#ref-boxcox" role="doc-biblioref">1976</a>)</span> and <span class="citation" data-cites="boxcox2">Box and Cox (<a href="#ref-boxcox2" role="doc-biblioref">1982</a>)</span>. Their method produces a log-likelihood curve of possible values for the power <span class="math inline">\(\lambda\)</span>. Without going into details, the higher the curve is for a particular value of <span class="math inline">\(\lambda\)</span>, the more normal the transformed data will be. The plot of the log-likelihood curve of the Box-Cox method applied to the vehicle data is shown in <a href="#fig-rangibox">Figure&nbsp;11</a>.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS, <span class="at">exclude =</span> <span class="st">'select'</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="fu">boxcox</span>(rangitikei<span class="sc">$</span>vehicle <span class="sc">~</span> <span class="dv">1</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">"Log-likelihood curve of Box-Cox power parameter"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rangibox" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-rangibox-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;11: Box-Cox Transformation</figcaption>
</figure>
</div>
</div>
</div>
<p>The curve peaks near zero indicating that a log transformation would be appropriate. In addition to the curve the plot contains a 95% confidence interval for the transformation parameter <span class="math inline">\(\lambda\)</span>. The two vertical dotted lines are the endpoints of the confidence interval for <span class="math inline">\(\lambda\)</span>. In this case the width of the confidence interval is quite small. The wider the confidence interval, the less obvious the choice for <span class="math inline">\(\lambda\)</span>. If the confidence interval contains 1, then there is no need to perform a transformation. Note that the Box-Cox transformation is a normalising transformation. The EDA done in the previous sections relate to symmetry in the data.</p>
<p>The Box-Cox method, and other power transformations, should only be applied if the data are strictly positive. If all the data are negative one can of course take the absolute value of the data and then apply the transformation. If, however, only some of the data are not positive, we may add a constant to all of the data (to make the data positive) but the estimated power will vary depending on the constant.</p>
</section>
</section>
<section id="transformations-for-inference" class="level1">
<h1>Transformations for Inference</h1>
<p>If we have a sample of data of size 30 or more from a homogeneous population preferably symmetrically distributed, the central limit theorem allows us to calculate a confidence interval for the population mean, <span class="math inline">\(\mu\)</span>. If the population is skewed, then we will need a larger sample to find the “correct” confidence interval for <span class="math inline">\(\mu\)</span>; the greater the skewness, the larger the sample size needed. However, when the population is skewed, even if we have a large enough sample size to obtain the “correct” confidence interval for <span class="math inline">\(\mu\)</span>, we must ask ourselves just how useful the population mean is as a description of the centre (or location) of the population. As an example, consider a skewed theoretical distribution known as the <em>lognormal</em> distribution. This distribution is related to the normal distribution. If a variable follows the lognormal distribution, then the log of the variable follows the normal distribution. <a href="#fig-lnorml">Figure&nbsp;12</a> shows the density curve for the log normal distribution with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dlnorm</span>(x, mu, sigma, <span class="at">log =</span> <span class="cn">FALSE</span>),   <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>),</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab=</span><span class="st">"Quantile"</span>,</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">"log-normal density"</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>mean <span class="ot">=</span> <span class="fu">exp</span>(mu<span class="sc">+</span>(sigma<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mean, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>median <span class="ot">=</span> <span class="fu">qlnorm</span>(<span class="fl">0.5</span>, <span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>median, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>mode <span class="ot">=</span> <span class="fu">exp</span>(mu)<span class="sc">/</span><span class="fu">exp</span>(sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mode, <span class="at">lty =</span> <span class="dv">3</span>, <span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"mean"</span>, <span class="st">"median"</span>, <span class="st">"mode"</span>),</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="at">col=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-lnorml" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-lnorml-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;12: Log-normal Density Curve</figcaption>
</figure>
</div>
</div>
</div>
<p>Since the lognormal distribution is skewed, the mean, the median, and the mode, are all different, but the mean is the most different. The height of the density curve at the mean is also much lower than it is at the median (by definition the height of the density curve is a maximum at the mode). This means that observations from this distribution are less likely to be close to the mean than the median. The one attribute the mean has is that it represents the centre of mass (or the balancing point) of the distribution. Unfortunately, this is quite often of little practical significance. In practice, when summarising skewed distributions (e.g.&nbsp;house prices, incomes), the median is always preferred to the mean. This leads us to conclude that it would be better to find a confidence interval for the population median, <span class="math inline">\(m\)</span>, than the population mean <span class="math inline">\(\mu\)</span>. To do this we will use the following procedure:</p>
<ol type="1">
<li><p>Given a sample of data from the skewed population, transform it so that it is symmetrically distributed (such that the mean and median should be roughly equal).</p></li>
<li><p>Find a confidence interval for the mean of the transformed data, <span class="math inline">\(\mu\)</span>. This interval will also be a good interval for the median of the transformed data, <span class="math inline">\(m\)</span>.</p></li>
<li><p>“Reverse transform” the confidence interval; i.e.&nbsp;apply the back transformation used on the data.</p></li>
<li><p>This new interval will be a confidence interval for the median of the original skewed population.</p></li>
</ol>
<p>This procedure works because the median is a <em>monotonic</em> function with respect to power transform. For example, consider a data set <span class="math inline">\(X = \{x_i\}\)</span> that requires a logarithmic transformation. The median of the transformed data set, <span class="math inline">\(\{\log(x_i)\}\)</span>, is equal to the logarithm of the median of the untransformed data set, <span class="math inline">\(\{x_i\}\)</span>. This is not true of the mean. The mean of the transformed data set, <span class="math inline">\(\{\log(x_i)\}\)</span>, is not equal to the logarithm of the mean of the untransformed data set, <span class="math inline">\(\{x_i\}\)</span>. If we reverse transform the mean of the log transformed data set, then the reverse transformed value will be the geometric mean of the raw data.</p>
<p>The distribution of vehicles in the <strong>rangitikei</strong> dataset is not symmetrical but very much skewed to the right with a probable outlier. The sample size 33 is only just above 30. It is of interest to compute the confidence interval for the true mean number of vehicles using the raw data (even though the CLT cannot be fully applied here). Using software, we obtain the 95% confidence interval for the mean as 13.1154 <span class="math inline">\(\leq\)</span> <span class="math inline">\(\mu\)</span> <span class="math inline">\(\leq\)</span> 29.3694.</p>
</section>
<section id="transformations-to-constant-variance" class="level1">
<h1>Transformations to Constant Variance</h1>
<p>For one batch of data, it is relatively easy to choose an appropriate transformation for symmetry. With more than one batch of data, life is more complicated. On the one hand the symmetry of each batch could be examined, and various transformations tried quite separately. However it may then be very difficult to compare and interpret the batches. On the other hand, if each batch is skewed in the same direction, we may be able to apply a common transformation so that each batch becomes symmetric, or close to it.</p>
<p>If groups are skewed in different directions (i.e.&nbsp;some positively and some negatively), it would be difficult to choose a transformation. If the batches are small, the skewness may be due to random variation and could be ignored. If the batches are large, it may not be feasible to perform statistical inference if the degree of skewness is quite variable.</p>
<p>With 2 or more groups, we are often interested in comparing the location (or level) of the responses in these groups, the location being measured by the median, mean or similar statistic. In this situation we like to assume that the groups are similar in other aspects such as in the shape of the distribution and, in particular, the spread of the responses. These assumptions simplify and strengthen the hypothesis test. Quite often, the spread (measured (say) by the range) increases as the locations (measured (say) by the median) increases. It is advisable to use a transformation which removes the systematic relationship between spread and location. Confidence interval, and hypothesis tests, of two means assume that the samples come from populations which have the same variance, or spread. We later consider comparisons of more than two means, and standard methods of formally testing hypotheses regarding differences in means require the assumption that the spreads (variances) are constant over the groups. So in this section, we look at how to examine whether the variances are equal and, if not, how transformations of the data can sometimes serve to equalise the variances.</p>
<p>Consider the response variable people in the dataset <strong>rangitikei</strong> for time of the day (<code>time</code>) temperature groupings (<code>temp</code>). <a href="#fig-diabox">Figure&nbsp;13</a> shows the boxplots of price for the combinations of <code>time</code> and <code>temp</code> factors. Also compare the position of the medians in the boxplots.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>rangitikei <span class="sc">|&gt;</span> </span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">temp =</span> <span class="fu">paste</span>(<span class="st">"Temperature"</span>, temp)) <span class="sc">|&gt;</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y =</span> people, </span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> time,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">colour =</span> time</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>      ) <span class="sc">+</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>temp) <span class="sc">+</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"People by time and temperature"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-diabox" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-diabox-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;13: Boxplots of People vs.&nbsp;temp*time</figcaption>
</figure>
</div>
</div>
</div>
<p>The assumption of a Normal distribution with constant variance may become crucial for certain confirmatory analysis. Hence the constancy of variance for each of the 24 batches is an important aspect we must look into. The boxplots are not supportive of the constant variance in the number of admissions. The relationship between spread and location issue can be explored using the medians and ranges of these batches.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>rangitikei <span class="sc">|&gt;</span> </span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(time,temp) <span class="sc">|&gt;</span> </span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">medians =</span> <span class="fu">median</span>(people), </span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">ranges =</span> <span class="fu">max</span>(people) <span class="sc">-</span> <span class="fu">min</span>(people)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">|&gt;</span> </span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y =</span> ranges, <span class="at">x =</span> medians) <span class="sc">+</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-diatrans" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-diatrans-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;14: Ranges vs.&nbsp;Medians</figcaption>
</figure>
</div>
</div>
</div>
<p>In <a href="#fig-diatrans">Figure&nbsp;14</a> the ranges are is plotted against the medians. It is clear that there is a positive trend in the data (larger ranges being associated with larger medians). We might compare this Figure with the ideal situation in which the points would fall in a horizontal band. So we conclude that the variability is not constant across time <span class="math inline">\(\times\)</span> temp combinations.</p>
<p>In the absence of outliers, we can also plot standard deviations against means as shown in <a href="#fig-diatrans1">Figure&nbsp;15</a>.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>rangitikei <span class="sc">|&gt;</span> </span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(time,temp) <span class="sc">|&gt;</span> </span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">means =</span> <span class="fu">mean</span>(people), </span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">sds=</span><span class="fu">sd</span>(people)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">|&gt;</span> </span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>sds, <span class="at">x=</span>means) <span class="sc">+</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-diatrans1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-diatrans1-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;15: SDs vs.&nbsp;means</figcaption>
</figure>
</div>
</div>
</div>
<p>A common transformation can be found if ranges (SDs) and medians (means) are related somewhat strongly. For example, the log transformed people data shows a random scatter of ranges vs medians in <a href="#fig-diatrans2">Figure&nbsp;16</a>. This means that some improvement in the constancy of variance for subgroups is achieved by the chosen log transformation.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>rangitikei <span class="sc">|&gt;</span> </span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trans.ppl =</span> <span class="fu">log</span>(people)) <span class="sc">|&gt;</span> </span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(time, temp) <span class="sc">|&gt;</span> </span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">medians =</span> <span class="fu">median</span>(trans.ppl),</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">ranges =</span> <span class="fu">max</span>(trans.ppl) <span class="sc">-</span> <span class="fu">min</span>(trans.ppl)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">|&gt;</span> </span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>ranges, <span class="at">x=</span>medians) <span class="sc">+</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-diatrans2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-diatrans2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;16: Ranges vs.&nbsp;Medians with log transformed data</figcaption>
</figure>
</div>
</div>
</div>
<p>If groups are skewed in different directions, we may not be able to find a common variance stabilizing transformation.</p>
</section>
<section id="nonparametric-methods" class="level1">
<h1>Nonparametric Methods</h1>
<p>In this section we outline an alternative to using power transformations as a method of preparing the data for hypothesis testing. The alternative approach usually relies on replacing the actual observed data values by their <strong>ranks</strong>. That is, the smallest data value is replaced by ‘1’, the second smallest by ‘2’, and so on up to the largest data value replaced by ‘<span class="math inline">\(n\)</span>’. If there are several data with the same value (known as <em>ties</em>) then they are all assigned the average rank they would have gotten if they had received a tiny random increment before being placed in order: for example if the <span class="math inline">\(3^{\text{rd}}\)</span>, <span class="math inline">\(4^{\text{th}}\)</span>,<span class="math inline">\({5^\text{th}}\)</span> and <span class="math inline">\({6^\text{th}}\)</span> smallest data values are all the same, then the four corresponding points are all assigned rank 4.5, which is the average of 3, 4, 5 and 6. We then analyse the ranks as if they are the original data.</p>
<p>The rank approach in some ways seems like a bad idea, as we are throwing away information - the actual data - and only analysing a lesser amount of information which is the ranks. However the approach can be justified in two ways.</p>
<ul>
<li><p>One is the fact that mathematical and simulation studies have shown that hypothesis tests based on ranked data have very good power compared to tests based on the Normal distribution - even when the data are <em>truly</em> Normal. That is, we don’t lose much by using a <strong>nonparametric method</strong> even if the Normal assumptions are perfectly true.</p></li>
<li><p>On the other hand if the data are not normally distributed, then the nonparametric tests are still powerful but the normal-theory methods can go wrong. So we are safer using a nonparametric method. The second justification, on a bit more of a philosophical level, is that if we honestly do not know what the distribution is, then we are probably wise not to pretend that any transformation is going to make it Normal. After all some data are collected on very odd scales indeed (e.g.&nbsp;optometry refers to ‘20/20 vision’ etc.), so that the data are just ordinal rather than numerical. In such a context rank methods may be appropriate as they pick up on ordinal difference rather than exact numerical difference.</p></li>
</ul>
<section id="ranking-and-rank-correlation" class="level2">
<h2 class="anchored" data-anchor-id="ranking-and-rank-correlation">Ranking and rank Correlation</h2>
<p>A nonparametric approach used very frequently, especially in the social sciences, is the <strong>Spearman’s Rank Correlation</strong>. To calculate it, first rank the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> variable, and then obtain usual correlation (the so-called Pearson correlation) coefficient. If there are a great many ties in the ranks then various corrections or modifications to the Spearman method have been suggested, but these are beyond the scope of this course. In principle then, one could simply apply the usual data analysis techniques to the <span class="math inline">\(W= \text {rank}(Y)\)</span> data and quote the <span class="math inline">\(p\)</span>-values accordingly. We don’t usually do this in simple analyses, for reasons outlined below, but let’s explore this idea for a moment. <a href="#fig-spear">Figure&nbsp;17</a> shows the usual Pearson (lower diagonal) and Spearman rank correlations (upper diagonal) for the <em>trees</em> default <code>R</code> dataset.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>  trees, </span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">upper =</span> <span class="fu">list</span>(<span class="at">continuous =</span> <span class="fu">wrap</span>(<span class="st">'cor'</span>, <span class="at">method =</span> <span class="st">"spearman"</span>)), </span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower =</span> <span class="fu">list</span>(<span class="at">continuous =</span> <span class="st">'cor'</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-spear" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-spear-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;17: Comparison of Pearson and Spearman rank correlations</figcaption>
</figure>
</div>
</div>
</div>
<p>It can be noted that the size of the estimates differ depending on the skew and relationship between the variables. For large samples it would be quite a reasonable approach, as the distribution of <span class="math inline">\(W\)</span> values is symmetrical and therefore the usual data analysis methods - relying on <span class="math inline">\(W\)</span> being Normally distributed - will work pretty well. The main difference to standard hypothesis tests would be that they would need to be expressed in terms of medians, say, rather than means.</p>
<p>For example, a two-group hypothesis test would be based on computing <span class="math inline">\(W\)</span> for all the <span class="math inline">\(n\)</span> data values together, and then comparing the mean of the <span class="math inline">\(n_1\)</span> ranks in the first group of observations to the mean of the <span class="math inline">\(n_2\)</span> ranks for the second group of observations. If there is no difference in population medians for the two groups, then we should not be able to reject the hypothesis that the mean <span class="math inline">\(W\)</span> values are the same.</p>
<p>In practice there are a couple of complications. One is that for small samples the distribution of <span class="math inline">\(W=\text {rank}(Y)\)</span> is not Normal because it is discrete, taking only integers or averages of integers. But fortunately mathematical statisticians have long since worked out the exact distribution of <span class="math inline">\(W\)</span> for many simple situations including two-sample tests, one-way ANOVA, two-way ANOVA with balanced numbers, correlation coefficients (e.g.&nbsp;the correlation between <span class="math inline">\(\text {rank}(Y)\)</span> and <span class="math inline">\(\text {rank}(X)\)</span>) and some others. So these exact distributions can be used for hypothesis tests, and are available. The second complication is that these exact distributions for <span class="math inline">\(W= \text {rank}(Y)\)</span> usually depend on the assumption of no ties, i.e.&nbsp;no equal ranks. Since ties do often occur in practice (if only because data are not measured exactly enough) then we need to use methods that are modified or corrected to handle ties. Fortunately the use of a software handles such issues as a matter of course in many cases, so it doesn’t take any extra time or effort on our behalf.</p>
</section>
<section id="wilcoxon-signed-rank-test" class="level2">
<h2 class="anchored" data-anchor-id="wilcoxon-signed-rank-test">Wilcoxon signed rank test</h2>
<p>For the nonparametric equivalent of a one-sample <span class="math inline">\(t\)</span>-test for <span class="math inline">\(H_0:\mu= \mu_0\)</span>, we use the <strong>Wilcoxon signed rank test</strong> for <span class="math inline">\(H_0: \eta=\eta_0\)</span> where <span class="math inline">\(\eta\)</span> (Greek letter ‘eta’) is the population median. Effectively this test is based on rank <span class="math inline">\((|Y-\eta_0|)\)</span>, where the ranks for data with <span class="math inline">\(Y&lt;\eta_0\)</span> are compared to the ranks for data with <span class="math inline">\(Y&gt;\eta_0\)</span>. If the <span class="math inline">\(\eta_0\)</span> is in about the right place, then the distances to points above <span class="math inline">\(\eta_0\)</span> will tend to rank approximately the same as the distances to points below <span class="math inline">\(\eta_0\)</span>. But if median is assumed too low, say, then the distances above <span class="math inline">\(\eta_0\)</span> will tend to be bigger (ranked higher) than the distances to points below <span class="math inline">\(\eta_0\)</span>. A statistical test (Wilcoxon test) and the associated <span class="math inline">\(p\)</span>-value follow. In theory, this test assumes a continuous symmetric distribution, but a correction is available in the case of ties. The following output shows the two-sample <span class="math inline">\(t\)</span> test and Wilcoxon test results for testing the equality of median number of people for the time of day groups (morning &amp; afternoon).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(rangitikei<span class="sc">$</span>people <span class="sc">~</span> rangitikei<span class="sc">$</span>time, <span class="at">conf.int=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot
compute exact p-value with ties</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot
compute exact confidence intervals with ties</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Wilcoxon rank sum test with continuity correction

data:  rangitikei$people by rangitikei$time
W = 30, p-value = 0.007711
alternative hypothesis: true location shift is not equal to 0
95 percent confidence interval:
 -88.99996 -10.00005
sample estimates:
difference in location 
             -36.46835 </code></pre>
</div>
</div>
<p>The <code>t.test(rangitikei$people~rangitikei$time)</code> test also gives the same conclusion for the equality of means.</p>
</section>
<section id="sign-test" class="level2">
<h2 class="anchored" data-anchor-id="sign-test">Sign test</h2>
<p>There is an additional one-sample test available called the one-sample <strong>sign test</strong>, which is based on replacing <span class="math inline">\(Y\)</span> not by <span class="math inline">\(\text {rank}(Y)\)</span> but simply by the <em>sign</em> of <span class="math inline">\(Y-\eta_0\)</span>, i.e.&nbsp;whether it is positive or negative. This replacement represents an additional loss of detail in the data, but also requires no assumptions. The resulting test is based on a binomial distribution. For example, consider the television viewing time data. Suppose we wish to test the hypothesis that children watch 4 hours of television per day on average (1680 minutes per week). The one-sample sign test output follows:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(tv<span class="sc">$</span>TELETIME, <span class="at">mu=</span><span class="dv">1680</span>, <span class="at">conf.int=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Wilcoxon signed rank exact test

data:  tv$TELETIME
V = 588, p-value = 0.6108
alternative hypothesis: true location is not equal to 1680
95 percent confidence interval:
 1557.5 1906.5
sample estimates:
(pseudo)median 
          1728 </code></pre>
</div>
</div>
<p>Notice the Wilcoxon test has about the same <span class="math inline">\(p\)</span>-value as the Normal-based <span class="math inline">\(t\)</span>-test. However as it assumes symmetry the estimated median is the same as the sample mean. The <span class="math inline">\(p\)</span>-value for the sign test is similar, but not the same as the others, and the estimated median is the same. The Wilcoxon and Sign test procedure can also be used to generate approximate 95% confidence intervals for the median. Note that these are based on the sorted sample data, and so are discrete, so it is usually not possible to get exact 95% confidence intervals. Both intervals are wider than the confidence interval based on the mean. The loss of precision (longer interval) is reasonable as we are making much weaker assumptions.</p>
</section>
<section id="wilcoxon-rank-sum-or-mann-whitney-test" class="level2">
<h2 class="anchored" data-anchor-id="wilcoxon-rank-sum-or-mann-whitney-test">Wilcoxon Rank-Sum or Mann-Whitney test</h2>
<p>As an alternative to the two-sample <span class="math inline">\(t\)</span>-test is the <strong>Wilcoxon Rank-Sum test</strong> (also mathematically equivalent to a test known as the <strong>Mann-Whitney test</strong>). The assumptions of the test are that the data are continuous (or at least ordinal) from populations that have the same shape (e.g.&nbsp;same skewness and same variance) but just (possibly) different medians. For this test, the entire set of responses is ranked together and then the ranks for the first group are compared to the ranks for the second group. The null hypothesis is that the two group medians are the same: <span class="math inline">\(H_0: \eta_1=\eta_2\)</span>. The following <code>R</code> output shows the Wilcoxon Rank-Sum test results.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kruskal.test</span>(tv<span class="sc">$</span>TELETIME <span class="sc">~</span> <span class="fu">factor</span>(tv<span class="sc">$</span>SCHOOL))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Kruskal-Wallis rank sum test

data:  tv$TELETIME by factor(tv$SCHOOL)
Kruskal-Wallis chi-squared = 7.6389, df = 2, p-value = 0.02194</code></pre>
</div>
</div>
<p>Again since our data are normally distributed we expect to get a similar result for the Mann-Whitney test as for the two-sample <span class="math inline">\(t\)</span>-test of equal means <span class="math inline">\(\mu_1=\mu_2\)</span>. This is indeed the case.</p>
</section>
</section>
<section id="permutation-and-bootstrap-tests" class="level1">
<h1>Permutation and bootstrap tests</h1>
<p>The computationally intensive alternatives to non-parametric methods are also available. Such tests are popular in certain application areas. We will cover them at a basic level only but these are not too hard to perform using <code>R</code> packages.</p>
<p>A permutation (or randomisation) test is based on the idea of randomly permuting the observed data and then answering whether a hypothesis is negated or not. For example, consider the two-sample t-test example on testing whether the mean TV viewing times are the same for boys and girls. We can pool the all of the data and then randomly distribute the observed data into two groups and compute the difference in the means for the two groups (maintaining the group sizes of course). This process of randomly permuting data can be done for a large number of times and then the empirical (permutation) distribution of the differences can be obtained. We can then obtain a P value as a proportion of the permuted differences that are as extreme as the actual mean difference. Many <code>R</code> packages are available to do this test, often with a single command. There are asymptotic forms of the permutation distribution for the statistic and hence computations can be done fairly quickly.</p>
<p>Consider the one-sample t-test example for <span class="math inline">\(H_0:\mu = 1500\)</span> for the <em>tv</em> dataset.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(tv<span class="sc">$</span>TELETIME, <span class="at">mu =</span> <span class="dv">1500</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in wilcox.test.default(tv$TELETIME, mu = 1500): cannot compute exact
p-value with ties</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Wilcoxon signed rank test with continuity correction

data:  tv$TELETIME
V = 773.5, p-value = 0.01108
alternative hypothesis: true location is not equal to 1500</code></pre>
</div>
</div>
<p>For two-sample test example based on the <em>tv</em> dataset, we obtain the following output using the <code>coin</code> package. The P-values under this test is very similar to the P-value under the Welch t-test (0.471).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(coin)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: survival</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">independence_test</span>(TELETIME <span class="sc">~</span> SEX, <span class="at">distribution =</span> <span class="st">'exact'</span>, <span class="at">data =</span> tv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Exact General Independence Test

data:  TELETIME by SEX (1, 2)
Z = -0.72876, p-value = 0.471
alternative hypothesis: two.sided</code></pre>
</div>
</div>
<p>The symmetry in the distribution of the TV times can also be tested using the function <code>symmetry_test()</code>. The significance of the Pearson correlation coefficient can be tested using <code>spearman_test()</code>. Pairwise permutation test across groups can also be done; see the output shown below:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(<span class="at">x =</span> testmarks<span class="sc">$</span>Maths, </span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">y =</span> testmarks<span class="sc">$</span>English, </span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">paired=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in wilcox.test.default(x = testmarks$Maths, y = testmarks$English, :
cannot compute exact p-value with ties</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in wilcox.test.default(x = testmarks$Maths, y = testmarks$English, :
cannot compute exact p-value with zeroes</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Wilcoxon signed rank test with continuity correction

data:  testmarks$Maths and testmarks$English
V = 380, p-value = 0.8945
alternative hypothesis: true location shift is not equal to 0</code></pre>
</div>
</div>
<p>There is a close theoretical connection between randomisation type and nonparametric tests. So the results are often similar when the same size is large.</p>
<section id="bootstrap-methods" class="level2">
<h2 class="anchored" data-anchor-id="bootstrap-methods">Bootstrap methods</h2>
<p>When a random sample is taken from a population, the expectation is that it is representative. So why not sample from the sample (i.e.&nbsp;resample) so that the quality of how well the sample is representative can be examined. We cannot gain extra information hugely by ordinary resampling and it is more like moving forward pulling the bootstrap! The methodology bootstrapping or resampling was introduced by <span class="citation" data-cites="efron">Efron (<a href="#ref-efron" role="doc-biblioref">1979</a>)</span>. This computational intensive procedure can be implemented very mechanically and simpler. By computing the sampling distribution of a statistic of interest, issues such as its bias and the standard error can be addressed.</p>
<p><code>R</code> package <em>boot</em> has many features and several variations (types) of the bootstrap resampling method but harder to use. We will use the <em>resample</em> package instead because it is simpler and also includes simple permutation tests.</p>
<p>Under the simple bootstrap method, observations are resampled with replacement from the original sample to create a bootstrap sample. We can then compute a statistic such as the sample mean for this resample. This process can be repeated many times, say 10000, and we form the bootstrap distribution of the statistic. Consider the <em>tv</em> dataset. If we resample TELETIME, and compute the mean television viewing time for each sample, we construct the bootstrap distribution of mean. Using the <code>resample</code> package, we get-</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(resample)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>bootC <span class="ot">&lt;-</span> <span class="fu">bootstrap</span>(tv<span class="sc">$</span>TELETIME, mean)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>bootC</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
bootstrap(data = tv$TELETIME, statistic = mean)
Replications: 10000

Summary Statistics:
     Observed       SE    Mean      Bias
mean 1729.283 82.74484 1728.54 -0.742787</code></pre>
</div>
</div>
<p>This output shows the observed sample mean 1729.283 and the mean of all bootstrap means which is 1729.114. The bias is the difference, which is -0.168. The main advantage of the bootstrap method is that it can quantify the bias that can occur due to sampling. <a href="#fig-bootc">Figure&nbsp;18</a> and <a href="#fig-bootqq">Figure&nbsp;19</a>, respectively, show the histogram and the normal quantile plots for the bootstrap means. Obviously the bootstrap means follow normal (due to CLT).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(bootC)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-bootc" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-bootc-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;18: Histogram of the bootstrap means of TELETIME</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(bootC)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-bootqq" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="4-inference_files/figure-html/fig-bootqq-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;19: QQ plot of the bootstrap means of TELETIME</figcaption>
</figure>
</div>
</div>
</div>
<p>There are many versions of bootstrap confidence intervals depending on the way bootstrapping is done. Without going into details, the 95% confidence interval for the true mean viewing time is obtained as follows:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">CI.t</span>(bootC)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>         2.5%    97.5%
mean 1560.637 1897.928</code></pre>
</div>
</div>
<p>This interval compares well with the confidence interval using t-distribution found earlier namely (1560.633, 1897.932). The same approach can be taken to construct a confidence interval for the mean of the paired differences and thereby perform a test analogous to paired t-test. See the <em>testsmarks</em> data example given below:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>differ <span class="ot">&lt;-</span> testmarks<span class="sc">$</span>Maths<span class="sc">-</span>testmarks<span class="sc">$</span>English</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>bootC <span class="ot">&lt;-</span> <span class="fu">bootstrap</span>(differ, mean)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="fu">CI.t</span>(bootC)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>          2.5%    97.5%
mean -4.175863 4.925863</code></pre>
</div>
</div>
<p>The parametric (i.e.&nbsp;t-test based) and bootstrap results are very similar. The bootstrap method can obtain better confidence intervals for the mean when the population is skewed because resampling tends to adjust for the skew in the population when captured by the sample well. The bootstrap approach will work well only when reasonably large sample sizes are available because of the inherent uncertainty in the tails of the underlying distribution. Small samples are not sufficient to identify capture the tails of the distribution and hence certain types of inferences involving tail part of distribution will not work well. Parametric assumptions can be made to improve the bootstrap method and this approach is known as parametric bootstrapping. We will not study such methods in this course.</p>
<p>The <code>resample</code> package also has options to bootstrap from two vectors. Consider the television viewing times for the boys and girls groupings. We resample from the two groups to perform the two sample test.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>bootC <span class="ot">&lt;-</span> <span class="fu">bootstrap2</span>(tv<span class="sc">$</span>TELETIME, <span class="at">statistic=</span>mean, <span class="at">treatment=</span>tv<span class="sc">$</span>SEX)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="fu">CI.t</span>(bootC)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>               2.5%    97.5%
mean: 1-2 -468.7219 224.6349</code></pre>
</div>
</div>
<p>The bootstrap test conclusion again agrees with the Welch two-sample t-test conclusion.</p>
<p>You will not be examined on the use of permutation and bootstrap tests in the final exam. We may occasionally use this approach for assignments.</p>
</section>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>The Normal distribution is important not only in its own right as a model for data, but because it plays a basic part in confirmatory statistics. Indeed the <span class="math inline">\(t\)</span>, <span class="math inline">\(F\)</span> and <span class="math inline">\(\chi^2\)</span> statistics we will use in the rest of this course all assume that the populations are Normal. The sample under study will not follow exactly a Normal distribution but we will often require it to be at least approximately symmetric.</p>
<p>A standard Normal, <span class="math inline">\(Z\)</span>, can be obtained as follows:</p>
<p><span class="math inline">\(Z\)</span> = (statistic-expected)/standard error.</p>
<p>The statistic may be the sample mean, <span class="math inline">\(\bar{y}\)</span>, or it may be the coefficient of a linear model.</p>
<p>If the standard error is not known but must be estimated from the sample, the standard variable follows a <span class="math inline">\(t\)</span> rather than a <span class="math inline">\(Z\)</span> distribution. If the statistic is the mean, <span class="math inline">\(\bar{y}\)</span>, the standard error is <span class="math inline">\(S/\sqrt n\)</span>, here <span class="math inline">\(S=\sqrt{{\frac{1}{n-1}} \sum _{i=1}^{n}(y_{i} -\bar{y})^{2} }\)</span>.</p>
<p>For other statistics, the standard error may be more complicated and we shall rely on a computer program to calculate this.</p>
<p>Usually a confidence interval for the expected value has the form</p>
<p>statistic <span class="math inline">\(\pm\)</span> margin of error</p>
<p>where margin of error = <span class="math inline">\(t\)</span> <span class="math inline">\(\times\)</span> estimated standard error.</p>
<p>Notice that:</p>
<ol type="1">
<li><p>For a 95% confidence interval of population parameter, we would expect 95% of such intervals to include the true value of the parameter (such as the population mean).</p></li>
<li><p>To increase the confidence level from (say) 95% to 99%, the tabulated <span class="math inline">\(t\)</span>-value will increase, yielding a wider confidence interval.</p></li>
<li><p>If the sample size was increased, we would expect the estimated standard error to decrease so that the confidence interval would become shorter.</p></li>
<li><p>We have considered symmetric confidence intervals. If data is skewed (say to the right) a symmetric confidence interval may not be appropriate. In such a case we may use a transformation to make the data symmetric, find our confidence interval, and convert back to the original scale.</p></li>
</ol>
<p>Hypothesis testing and confidence intervals for the mean difference of two batches of data usually assume that the two populations are normally distributed and the variances of the two populations are the same. Of course the variances of the samples will differ but, hopefully, not by too much for this would suggest that a transformation may be desirable. If the means of the two populations are equal, we would expect the difference of the sample means, <span class="math inline">\((\bar{y}_{1} -\bar{y}_{2})\)</span>, to be close to zero and, conversely, we would expect the confidence interval for the difference in means to include zero.</p>
<p>If the population means are different, we would expect the difference in the sample means to be a value different from zero. So, if the value of zero does <em>not</em> fall in the confidence interval, the hypothesis that the means are equal will be <em>rejected</em>. We make our decision to accept or reject the hypothesis based on the P-value. We reject <span class="math inline">\(H_0: \mu_1 = \mu_2\)</span> only if the <span class="math inline">\(p\)</span>-value <span class="math inline">\(&lt;\)</span> the set significance level, usually 5%.</p>
<p>There are many ways of transforming data. Simple transformations can be used such as multiplying each value by 100 to remove decimals or to convert to percentages, or a ratio transformation, such as petrol consumption expressed as litres per 100 km. In this course, we concentrate on power transformations where each observation, <span class="math inline">\(y\)</span>, is raised to the power of <span class="math inline">\(\lambda\)</span>. For negative values of <span class="math inline">\(\lambda\)</span>, it is advisable to multiply transformed values by <span class="math inline">\(-1\)</span> as this preserves the original order in the data. In this chapter, we have considered two reasons for transformations.</p>
<ol type="1">
<li><p>The first reason was to obtain symmetry in a single batch of data. Such transformations are sometimes required when we decide to carry out hypothesis tests or compute confidence intervals for some variable.</p></li>
<li><p>With more than two batches of data, we have the additional reason of the need for common variance across batches. To compare means using Normal-based methods, it is advisable for the measurements in each batch to be both distributed symmetrically and with the same spread and variation as those in other batches. It may not be possible to find a single transformation, which will bring about these two ideal characteristics but a transformation may be found to bring the measurements closer to these ideals. Note that, to compare groups, or the means of these groups, the same transformation must be applied to each group. If groups are skewed in different directions (positive and negative), no transformation would be appropriate.</p></li>
</ol>
<p>Nonparametric and other computationally intensive methods such as the regular bootstrap methods can be useful when normal and other distributional assumptions are grossly violated. These methods are also useful for validating Normal-based results.</p>


<!-- -->


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-boxcox" class="csl-entry" role="listitem">
Box, G. E. P., and D. R. Cox. 1976. <span>“An Analysis of Transformations.”</span> <em>Journal of the Royal Statistical Society, Series B</em> 26: 211–43.
</div>
<div id="ref-boxcox2" class="csl-entry" role="listitem">
———. 1982. <span>“An Analysis of Transformations Revisited, Rebutted.”</span> <em>Journal of the American Statistical Association</em> 77 (377): 209–10.
</div>
<div id="ref-efron" class="csl-entry" role="listitem">
Efron, B. 1979. <span>“Computers and the Theory of Statistics: Thinking the Unthinkable.”</span> <em>SIAM Review</em> 21 (4): 460–80. <a href="http://www.jstor.org/stable/2030104">http://www.jstor.org/stable/2030104</a>.
</div>
<div id="ref-fisher1935" class="csl-entry" role="listitem">
Fisher, R. A. 1935. <em>The Design of Experiments</em>. Oliver &amp; Boyd Edinburgh, Scotland.
</div>
<div id="ref-student" class="csl-entry" role="listitem">
Gosset, W. S. 1942. <span>“"<span>S</span>tudent’s" Collected Papers.”</span> In, edited by E. S. Pearson and J. Wishart. London: Biometrika Office. <a href="https://archive.org/details/in.ernet.dli.2015.233812">https://archive.org/details/in.ernet.dli.2015.233812</a>.
</div>
<div id="ref-Hall1986" class="csl-entry" role="listitem">
Hall, Peter, and Ben Selinger. 1986. <span>“Statistical Significance: Balancing Evidence Against Doubt.”</span> <em>Australian Journal of Statistics</em> 28 (3): 354–70. <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-842X.1986.tb00708.x">https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-842X.1986.tb00708.x</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Bear in mind though that no real population of values (except for synthetic ones, simulated by a computer) is normally distributed <em>in reality</em>. A variable can only be normally distributed <em>in theory</em>. The real world just is what it is. Recall Box’s adage: “<em>All models are wrong, but some are useful</em>”. When we test for a departure of a variable from, say, a normal distribution, we are simply testing whether the normal distribution provides an adequate model for the data.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../studyguide/3-probability.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Chapter 3: Probability Concepts and Distributions</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../studyguide/5-tabulated.html" class="pagination-link">
        <span class="nav-page-text">Chapter 5: Tabulated Counts</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb68" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Chapter 4: Statistical Inference"</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "_All models are wrong, but some are useful._"</span></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; – George Box</span></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "_Absence of evidence is not evidence of absence._" </span></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; – Carl Sagan</span></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "_A statistical analysis, properly conducted, is a delicate dissection of uncertainties, a surgery of suppositions._"</span></span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; – M.J. Moroney</span></span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a>This chapter provides an introduction to statistical inference. Many of the concepts in this chapter should be familiar to you because they are covered in all first-year statistics courses.</span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a><span class="fu"># What is statistical inference?</span></span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a><span class="fu">## Populations and parameters, samples and statistics</span></span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a>Statistical inference is a fundamental concept in statistics. The vast majority of statistical analyses that you will do as an undergraduate involve statistical inference. Anything involving *p*-values, confidence intervals, or standard errors are a form of statistical inference.</span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a>:::{.callout-important}</span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a><span class="fu"># Statistical inference is: </span></span>
<span id="cb68-29"><a href="#cb68-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-30"><a href="#cb68-30" aria-hidden="true" tabindex="-1"></a>the use of information from a sample to make statements about a population.</span>
<span id="cb68-31"><a href="#cb68-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-32"><a href="#cb68-32" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb68-33"><a href="#cb68-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-34"><a href="#cb68-34" aria-hidden="true" tabindex="-1"></a>As discussed in previous chapters, most datasets contain information about a sample from a population, rather than the whole population of interest. For example (@fig-inf), say we owned a fish farm, and we wished to know the average length of the fish in our farm. Let's say we had 2,000 fish in our farm. It would be too time-consuming to catch and measure every single fish. Instead, we take a random sample of, say, 10 fish, measure their lengths, and calculate the mean.</span>
<span id="cb68-35"><a href="#cb68-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-36"><a href="#cb68-36" aria-hidden="true" tabindex="-1"></a>Remember, our goal here is to know something about the *whole population of 2,000 fish*. We don't really care about the 10 fish in our sample. It is no use to say "*Well, I've no idea about the average length of my whole population fish, but you see those 10 fish there? They average 36.7 cm in length.*". We only care about the 10 fish in our sample *in so far as they tell us something about the broader population.* We use the average length of the fish in our sample as *and estimate* of the average length of fish in the population. This is statistical inference: using information from a sample to make conclusions about a population. </span>
<span id="cb68-37"><a href="#cb68-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-38"><a href="#cb68-38" aria-hidden="true" tabindex="-1"></a><span class="al">![Statistical inference from a sample to a population of fish](images/inference.png)</span>{#fig-inf}</span>
<span id="cb68-39"><a href="#cb68-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-40"><a href="#cb68-40" aria-hidden="true" tabindex="-1"></a>To clarify some terminology using the example in @fig-inf:</span>
<span id="cb68-41"><a href="#cb68-41" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb68-42"><a href="#cb68-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The **population** is all 2,000 fish in our farm. </span>
<span id="cb68-43"><a href="#cb68-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The **sample** is the 10 fish we happened to measure. </span>
<span id="cb68-44"><a href="#cb68-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The **parameter of interest** (often denoted $\mu$, if it is a mean, or $\theta$ more generally) is the average length of the fish in the population of 2,000. Population parameters are usually considered to be *fixed and unknown* values.</span>
<span id="cb68-45"><a href="#cb68-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The **statistic** (often denoted $\bar{y}$ or $\hat\mu$, if it is a mean, or $\hat\theta$ more generally) is the average of the 10 lengths of the fish in our sample. Unlike population parameters, which are *fixed and unknown*, sample statistics are  *random variables*. </span>
<span id="cb68-46"><a href="#cb68-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Statistical inference** in this case is the use of the sample mean $\bar{y}$ as an *estimate* of the population mean $\mu$.</span>
<span id="cb68-47"><a href="#cb68-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-48"><a href="#cb68-48" aria-hidden="true" tabindex="-1"></a>The fact that we've only measured lengths from a sample rather than the whole population necessitates statistical inference. If we'd measured every fish in the farm, we wouldn't need statistical inference, because we'd know precisely the population parameter (assuming negligible measurement error). </span>
<span id="cb68-49"><a href="#cb68-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-50"><a href="#cb68-50" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sampling Error</span></span>
<span id="cb68-51"><a href="#cb68-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-52"><a href="#cb68-52" aria-hidden="true" tabindex="-1"></a>This brings us to the next important concept of statistical inference: sampling error. A consequence of having collected data from a sample rather than the whole population is that there is *uncertainty* in our knowledge of the population parameter. Our sample mean is an *estimate* of the population mean; if we wanted to know population mean with zero uncertainty, we'd have to measure all the fish. This is the trade-off of sampling. It's a lot cheaper to sample, but we sacrifice certainty.</span>
<span id="cb68-53"><a href="#cb68-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-54"><a href="#cb68-54" aria-hidden="true" tabindex="-1"></a>The practical application of statistical inference involves (1) making estimates and (2) quantifying the uncertainty of those estimates. Uncertainty is often quantified using standard errors, confidence intervals, and *P*-values. All these quantities relate to *sampling error*. They're all expressions of the uncertainty of an estimate of a population parameter.</span>
<span id="cb68-55"><a href="#cb68-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-56"><a href="#cb68-56" aria-hidden="true" tabindex="-1"></a>In the fish farm example (@fig-inf), sampling error is the hypothetical variation in the means of the lengths of samples of fish, with a sample size of $n$ = 10. That is, if we were to (hypothetically) repeat the scientific process (i.e., randomly select 10 fish, measure their lengths, and calculate the sample mean), over and over again, how much would those sample means vary? That variation of sample statistics is sampling variation, or sampling error. **And understanding sampling variation is the key to understanding most of undergraduate statistics**.</span>
<span id="cb68-57"><a href="#cb68-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-58"><a href="#cb68-58" aria-hidden="true" tabindex="-1"></a>So, when we do our study (i.e., randomly select 10 fish, measure their lengths, and calculate the sample mean), we are drawing one value of the sample mean, $\bar y$, from a random variable, $\bar Y$, which is the distribution of sample means that we *could* hypothetically draw.</span>
<span id="cb68-59"><a href="#cb68-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-60"><a href="#cb68-60" aria-hidden="true" tabindex="-1"></a>Given this random sampling variation, here are some explanations for some commonly used measures of uncertainty:</span>
<span id="cb68-61"><a href="#cb68-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-62"><a href="#cb68-62" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A **standard error** is simply the standard deviation of a statistic under repeated sampling–that is, how much it would vary (hypothetically) from sample to sample.</span>
<span id="cb68-63"><a href="#cb68-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-64"><a href="#cb68-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A **confidence interval** is a pair of numbers that contain the true value of the population parameter with 95% confidence. It is a simple function of the sample statistic and its standard error.</span>
<span id="cb68-65"><a href="#cb68-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-66"><a href="#cb68-66" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A **_p_-value** is the probability of obtaining a sample statistic as or more extreme than the one observed, given a particular hypothesised value (usually zero) of the population parameter. </span>
<span id="cb68-67"><a href="#cb68-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-68"><a href="#cb68-68" aria-hidden="true" tabindex="-1"></a>Don't worry if those definitions aren't completely clear to you right now, but I encourage you to refer back to this section again and again as you learn about them in more detail during the rest of this course. </span>
<span id="cb68-69"><a href="#cb68-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-70"><a href="#cb68-70" aria-hidden="true" tabindex="-1"></a>**Keep sampling error front of mind whenever you see a standard error, a confidence interval, or a _p_-value.**</span>
<span id="cb68-71"><a href="#cb68-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-72"><a href="#cb68-72" aria-hidden="true" tabindex="-1"></a>Now, we'll introduce some specific inference methods. </span>
<span id="cb68-73"><a href="#cb68-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-74"><a href="#cb68-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-75"><a href="#cb68-75" aria-hidden="true" tabindex="-1"></a><span class="fu"># Tests for normality</span></span>
<span id="cb68-76"><a href="#cb68-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-77"><a href="#cb68-77" aria-hidden="true" tabindex="-1"></a>Let's say we have some data and we wish to test the idea that the data came from a population that is normally distributed. The null hypothesis is that the population conforms to a normal distribution. We calculate a test statistic that quantifies the degree of *departure* of the data from what we'd expect if the null hypothesis were true (i.e., the population were indeed normally distributed)<span class="ot">[^1]</span>.</span>
<span id="cb68-78"><a href="#cb68-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-79"><a href="#cb68-79" aria-hidden="true" tabindex="-1"></a><span class="ot">[^1]: </span>Bear in mind though that no real population of values (except for synthetic ones, simulated by a computer) is normally distributed *in reality*. A variable can only be normally distributed *in theory*. The real world just is what it is. Recall Box's adage: "*All models are wrong, but some are useful*". When we test for a departure of a variable from, say, a normal distribution, we are simply testing whether the normal distribution provides an adequate model for the data. </span>
<span id="cb68-80"><a href="#cb68-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-81"><a href="#cb68-81" aria-hidden="true" tabindex="-1"></a>If the data look substantially different to a normal distribution, then we will expect the associated test statistic to be large. How large does it have to be before we can reject the idea that the data came from a normal distribution? This question can be answered by calculating a $p$-value for the test statistic. **The _p_-value is the probability of obtaining a test statistic as or more extreme than the one we have calculated** ***if the null hypothesis were true***.</span>
<span id="cb68-82"><a href="#cb68-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-83"><a href="#cb68-83" aria-hidden="true" tabindex="-1"></a>If the $p$-value is smaller than some pre-decided threshold level, such as 5%, then we can reject the null hypothesis that the population is normally distributed, and conclude that the population is **not** normally distributed. If the $p$-value is large, then we have **no evidence** that the population is normally distributed. </span>
<span id="cb68-84"><a href="#cb68-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-85"><a href="#cb68-85" aria-hidden="true" tabindex="-1"></a>Two important points to remember about null hypotheses:</span>
<span id="cb68-86"><a href="#cb68-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-87"><a href="#cb68-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Null (and alternative) hypotheses are **always** about population parameters, and **never** about sample statistics; inferences are always about the population, never about the sample.</span>
<span id="cb68-88"><a href="#cb68-88" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With a large $p$-value, we **never** say that we have "accepted" or "confirmed" the null hypothesis; we only ever reject or fail to reject a null hypothesis.</span>
<span id="cb68-89"><a href="#cb68-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-90"><a href="#cb68-90" aria-hidden="true" tabindex="-1"></a>There are several statistical tests for normality available in the literature. The **Kolmogorov-Smirnov test** for normality is based on the biggest difference between the empirical and theoretical cumulative distributions. On the other hand, **Shapiro-Wilk test** is based on variance of the difference. There are also several other normality test procedures and we will not be concerned with the details. It is also difficult to regard one particular test to be always superior or powerful than the other.</span>
<span id="cb68-91"><a href="#cb68-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-92"><a href="#cb68-92" aria-hidden="true" tabindex="-1"></a>The Shapiro-Wilk test of normality for the number of people who made use of a recreational facility (**rangitikei**) gives a $p$-value less than 0.001. The null hypothesis is that the data come from a normal distribution. The low $p$-value indicates a significant departure from normality.</span>
<span id="cb68-93"><a href="#cb68-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-94"><a href="#cb68-94" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: `rangitikei`</span></span>
<span id="cb68-95"><a href="#cb68-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-98"><a href="#cb68-98" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-99"><a href="#cb68-99" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb68-100"><a href="#cb68-100" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb68-101"><a href="#cb68-101" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span>
<span id="cb68-102"><a href="#cb68-102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-103"><a href="#cb68-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-106"><a href="#cb68-106" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-107"><a href="#cb68-107" aria-hidden="true" tabindex="-1"></a><span class="fu">download.file</span>(</span>
<span id="cb68-108"><a href="#cb68-108" aria-hidden="true" tabindex="-1"></a>  <span class="at">url =</span> <span class="st">"http://www.massey.ac.nz/~anhsmith/data/rangitikei.RData"</span>,</span>
<span id="cb68-109"><a href="#cb68-109" aria-hidden="true" tabindex="-1"></a>  <span class="at">destfile =</span> <span class="st">"rangitikei.RData"</span>)</span>
<span id="cb68-110"><a href="#cb68-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-111"><a href="#cb68-111" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"rangitikei.RData"</span>)</span>
<span id="cb68-112"><a href="#cb68-112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-113"><a href="#cb68-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-114"><a href="#cb68-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-117"><a href="#cb68-117" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-118"><a href="#cb68-118" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(rangitikei<span class="sc">$</span>people)</span>
<span id="cb68-119"><a href="#cb68-119" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-120"><a href="#cb68-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-121"><a href="#cb68-121" aria-hidden="true" tabindex="-1"></a>The low $p$-value here indicates a significant departure from normality. We conclude that there is very strong evidence against the null hypothesis that we the population is normally distributed. (Remember, always express your conclusions by reference to the population, not the sample, or even "the data".)</span>
<span id="cb68-122"><a href="#cb68-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-123"><a href="#cb68-123" aria-hidden="true" tabindex="-1"></a>We can examine a Q-Q plot (@fig-rangidist), which plots the observed values ($y$) against the theoretical values if the population were normally distributed. Departure from the diagonal line indicates departure from normality.</span>
<span id="cb68-124"><a href="#cb68-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-127"><a href="#cb68-127" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-128"><a href="#cb68-128" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-129"><a href="#cb68-129" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rangidist</span></span>
<span id="cb68-130"><a href="#cb68-130" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Distribution of people'</span></span>
<span id="cb68-131"><a href="#cb68-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-132"><a href="#cb68-132" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span> </span>
<span id="cb68-133"><a href="#cb68-133" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">sample =</span> people) <span class="sc">+</span> </span>
<span id="cb68-134"><a href="#cb68-134" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq</span>() <span class="sc">+</span></span>
<span id="cb68-135"><a href="#cb68-135" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>() </span>
<span id="cb68-136"><a href="#cb68-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-137"><a href="#cb68-137" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span> </span>
<span id="cb68-138"><a href="#cb68-138" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>people, <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb68-139"><a href="#cb68-139" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb68-140"><a href="#cb68-140" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb68-141"><a href="#cb68-141" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span>
<span id="cb68-142"><a href="#cb68-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-143"><a href="#cb68-143" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1, p2, <span class="at">ncol=</span><span class="dv">1</span>) </span>
<span id="cb68-144"><a href="#cb68-144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-145"><a href="#cb68-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-146"><a href="#cb68-146" aria-hidden="true" tabindex="-1"></a>The same conclusion is drawn with the Kolmogorov-Smirnov test.  Note that this test does not allow ties and can be used to test the fitting of non-normal distributions.</span>
<span id="cb68-147"><a href="#cb68-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-150"><a href="#cb68-150" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-151"><a href="#cb68-151" aria-hidden="true" tabindex="-1"></a><span class="fu">ks.test</span>(rangitikei<span class="sc">$</span>people, <span class="st">"pnorm"</span>)</span>
<span id="cb68-152"><a href="#cb68-152" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-153"><a href="#cb68-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-154"><a href="#cb68-154" aria-hidden="true" tabindex="-1"></a>It is often informative to analyse data by fitting a **statistical model**. The idea is to look for real patterns, "signals" amongst the "noise" of individual variation, patterns that would reoccur in other, hypothetical samples we might have drawn from the population. We often try to approximate patterns by fitting a "statistical model". A A statistical model usually comprises a mathematical formula describing the relationships among variables, along with a probabilistic description of the variation of the data around the formula. If the statistical model is a good approximation, it serves as a neat way of describing the system that generated the data, and we can use such a model  to predict future values of the variables.</span>
<span id="cb68-155"><a href="#cb68-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-156"><a href="#cb68-156" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: `testmarks`</span></span>
<span id="cb68-157"><a href="#cb68-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-158"><a href="#cb68-158" aria-hidden="true" tabindex="-1"></a>The data set **tv** consists of the time that 46 school children spent watching television. Before fitting a model to the data, it is a good idea to see whether the data approximately follows a Normal distribution using a normal Q-Q Plot; see @fig-tvdist. The points plotted fall pretty much along the line, suggesting at least approximate Normality.</span>
<span id="cb68-159"><a href="#cb68-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-160"><a href="#cb68-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-163"><a href="#cb68-163" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-164"><a href="#cb68-164" aria-hidden="true" tabindex="-1"></a><span class="fu">download.file</span>(</span>
<span id="cb68-165"><a href="#cb68-165" aria-hidden="true" tabindex="-1"></a>  <span class="at">url =</span> <span class="st">"http://www.massey.ac.nz/~anhsmith/data/tv.RData"</span>, </span>
<span id="cb68-166"><a href="#cb68-166" aria-hidden="true" tabindex="-1"></a>  <span class="at">destfile =</span> <span class="st">"tv.RData"</span>)</span>
<span id="cb68-167"><a href="#cb68-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-168"><a href="#cb68-168" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"tv.RData"</span>)</span>
<span id="cb68-169"><a href="#cb68-169" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-170"><a href="#cb68-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-173"><a href="#cb68-173" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-174"><a href="#cb68-174" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-175"><a href="#cb68-175" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tvdist</span></span>
<span id="cb68-176"><a href="#cb68-176" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Distribution of TV viewing times'</span></span>
<span id="cb68-177"><a href="#cb68-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-178"><a href="#cb68-178" aria-hidden="true" tabindex="-1"></a>P.val <span class="ot">&lt;-</span> tv<span class="sc">$</span>TELETIME <span class="sc">|&gt;</span> </span>
<span id="cb68-179"><a href="#cb68-179" aria-hidden="true" tabindex="-1"></a>  <span class="fu">shapiro.test</span>() <span class="sc">|&gt;</span> </span>
<span id="cb68-180"><a href="#cb68-180" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pluck</span>(<span class="st">'p.value'</span>) <span class="sc">|&gt;</span> </span>
<span id="cb68-181"><a href="#cb68-181" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">3</span>)</span>
<span id="cb68-182"><a href="#cb68-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-183"><a href="#cb68-183" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tv) <span class="sc">+</span> </span>
<span id="cb68-184"><a href="#cb68-184" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">sample =</span> TELETIME) <span class="sc">+</span> </span>
<span id="cb68-185"><a href="#cb68-185" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq</span>() <span class="sc">+</span></span>
<span id="cb68-186"><a href="#cb68-186" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>() <span class="sc">+</span></span>
<span id="cb68-187"><a href="#cb68-187" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">caption =</span> <span class="fu">paste</span>(<span class="st">"Shapiro Test P value"</span>, P.val))</span>
<span id="cb68-188"><a href="#cb68-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-189"><a href="#cb68-189" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tv) <span class="sc">+</span> </span>
<span id="cb68-190"><a href="#cb68-190" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>TELETIME, <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb68-191"><a href="#cb68-191" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb68-192"><a href="#cb68-192" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb68-193"><a href="#cb68-193" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span>
<span id="cb68-194"><a href="#cb68-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-195"><a href="#cb68-195" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1, p2, <span class="at">ncol=</span><span class="dv">1</span>) </span>
<span id="cb68-196"><a href="#cb68-196" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-197"><a href="#cb68-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-198"><a href="#cb68-198" aria-hidden="true" tabindex="-1"></a>@fig-tvdist shows the boxplot of the data. The boxplot again suggests a very mild skew to the left but the middle 50% data show right skewness. However the whiskers are about the same length and there are no outliers. There is a difference of 31 between the mean and median, suggestive of a slight skew to the lower values. However this difference is small given the overall variability (standard deviation is 567.9, and the range is 2309) so we can probably ignore the observed skew. However we will look for any further evidence of skewness, since this could invalidate any inference we make based on the Normal distribution (at least it would if we had a smaller sample). The TV viewing time data also passes normality tests such as Shapiro-Wilk test. All told, we conclude that the normal model describes the distribution of these data fairly well.</span>
<span id="cb68-199"><a href="#cb68-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-200"><a href="#cb68-200" aria-hidden="true" tabindex="-1"></a>As indicated earlier, a large number of naturally occurring measurements, such as height, appear to follow a Normal distribution so that a considerable amount of theory has been built on this distribution.</span>
<span id="cb68-201"><a href="#cb68-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-202"><a href="#cb68-202" aria-hidden="true" tabindex="-1"></a>To recapitulate, Normal (or Gaussian) curves are determined by just two numbers, one indicating the **location** and the other the **spread**. Although there are an infinite number of Normal curves, their shapes are similar and, of course, the area under each curve is 1. Indeed, the location and spread parameters are the only differences between curves.</span>
<span id="cb68-203"><a href="#cb68-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-204"><a href="#cb68-204" aria-hidden="true" tabindex="-1"></a>It is usual to take the measure of location as the mean, denoted by $\mu$, and the measure of spread as the standard deviation, denoted by $\sigma$. If a variable, Y, follows a Normal distribution with mean $\mu$ and standard deviation $\sigma$, we write Y $\sim$ N($\mu$, $\sigma$) where $\sim$ means is *distributed as* (The squiggly symbol $\sim$ is known as *tilde*).</span>
<span id="cb68-205"><a href="#cb68-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-206"><a href="#cb68-206" aria-hidden="true" tabindex="-1"></a>Note, however, that the Normal distribution, like all statistical distributions, is a theoretical concept and no naturally occurring measurement will **exactly** follow a probability distribution model. For one thing, any measurement is finite whereas the Normal curve is continuous in the interval $\left(-\infty ,\infty \right)$. Also, the curve is asymptotic to the $X$-axis so that any range of values of $Y$ however large or small will have a certain probability according to the Normal distribution, but in practice there will be limitations such as that a person's blood pressure must be greater than zero.</span>
<span id="cb68-207"><a href="#cb68-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-208"><a href="#cb68-208" aria-hidden="true" tabindex="-1"></a>Suppose we didn't know that $\mu$ = 80 and $\sigma$ = 12. The obvious **estimator** of $\mu$, based solely on the sample, is the sample mean $\bar{y}$, and the obvious estimator of $\sigma$ is the sample standard deviation $S$. Note that by estimator we don't mean the observed value based on the **particular** sample. Rather the word estimator means the mathematical formula or procedure that we use to produce our estimates, namely $\bar{y}={\frac{1}{n}} \sum y$ and $S=\sqrt{{\frac{1}{n-1}} \sum _{i=1}^{n}(y_{i} -\bar{y})^{2} }$.</span>
<span id="cb68-209"><a href="#cb68-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-210"><a href="#cb68-210" aria-hidden="true" tabindex="-1"></a>The point is that there can be several alternative procedures for estimating the same parameters $\mu$ and $\sigma$, and in particular samples the actual computed estimates may be the same or different. For example, since the mean and median are the same for Normal data, we could estimate $\mu$ by the sample median, namely 81.313 for our example. This different procedure has given rise to a different number, and if we didn't know the answer we would not know which estimate to use. The median estimate has a lot of attraction, since the median is robust, that is, is not affected by outliers.</span>
<span id="cb68-211"><a href="#cb68-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-212"><a href="#cb68-212" aria-hidden="true" tabindex="-1"></a>Clearly all these estimates are close to the true parameter values, but not the same. A statistical question relates to how close estimates are to their true values *in general*. We usually can't answer this question about the actual estimates (observed numbers) since we usually don't know the correct answer. One approach, which is often used to test procedures in research, is to try a number of **simulations** and see which approach produces the closest results on average. We can also give error bounds that say, for example, that 95% of the time the estimator is within such-and-such a distance of the true parameter. This leads to the idea of using probability or so-called 'confidence'. By making probability statements about the estimators we can say something useful about how trustworthy the particular estimates are also. We use **standard errors** to measure the trustworthiness of the estimators. The standard error is the standard deviation of the estimator, so the smaller the standard error the better.</span>
<span id="cb68-213"><a href="#cb68-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-214"><a href="#cb68-214" aria-hidden="true" tabindex="-1"></a>Without going into details, it turns out that $\bar{y}$ has the smallest possible standard error for any unbiased estimator of $\mu$ for normal data. (An unbiased estimator is one that is not systematically too big or too small.) While the same is *not* true of $S$ in relation to $\sigma$, the latter does have other useful mathematical properties. So these are some reasons for using these formulae so routinely.</span>
<span id="cb68-215"><a href="#cb68-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-216"><a href="#cb68-216" aria-hidden="true" tabindex="-1"></a><span class="fu"># Sampling distributions</span></span>
<span id="cb68-217"><a href="#cb68-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-218"><a href="#cb68-218" aria-hidden="true" tabindex="-1"></a>A *sampling distribution* is a probabilistic model of *sampling variation*–it describes the behaviour of some sample statistic (such as a sample mean) if one were to repeat the sampling and calculation of the statistic many many times. The sampling distribution is not known, because we usually only have a single sample. However, we can make certain theoretical assumptions about how a statistic is distributed if one were to repeat the study over and over again. </span>
<span id="cb68-219"><a href="#cb68-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-220"><a href="#cb68-220" aria-hidden="true" tabindex="-1"></a>For a normal population, when the population parameters $\mu$ and $\sigma$ are *known*, we can easily derive the sampling distributions of the sample mean or sample variance. When the population parameters are *unknown*, we have to estimate them from data. When the sample size is small, we have large standard errors around estimates of the population parameters, and the distributions of the sample mean or sample variance are poorly known.</span>
<span id="cb68-221"><a href="#cb68-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-222"><a href="#cb68-222" aria-hidden="true" tabindex="-1"></a>The Student's $t$, $\chi^2$ and $F$ distributions are the three useful sampling distributions for testing hypotheses for a normal population when the population parameters are unknown. These distributions also have a closed form expression for their probability density function. This means that we can calculate the areas under the distribution functions, and calculate *p*-values.</span>
<span id="cb68-223"><a href="#cb68-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-224"><a href="#cb68-224" aria-hidden="true" tabindex="-1"></a>The $t$ distribution is the sampling distribution of the mean when $\sigma$ is unknown.  The $\chi^2$ distribution is the sampling distribution of the sample variance $S^2$ when re-expressed as $(n-1)S^2/\sigma^2$. The $F$ distribution is ratio of two $\chi^2$ distributions, and hence it becomes the sampling distribution of the ratio of two sample variances $S_1^2/S_2^2$ from two normal populations (after appropriate scaling for the sample sizes). </span>
<span id="cb68-225"><a href="#cb68-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-226"><a href="#cb68-226" aria-hidden="true" tabindex="-1"></a>While the $t$ distribution is symmetric, the $\chi^2$ and $F$ distributions are right skewed. When the sample size $n$ approaches infinity, both distributions become normal but you will start observing symmetry when the sample size(s) exceed 30.</span>
<span id="cb68-227"><a href="#cb68-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-228"><a href="#cb68-228" aria-hidden="true" tabindex="-1"></a>For these sampling distributions, the sample size acts as the proxy parameter, called the degrees of freedom.  For both $t$, $\chi^2$, the degrees of freedom, $\nu$ is $(n-1)$.  What this means is that the sampling distribution or the probability density of these two distributions depend only on the degrees of freedom $\nu$.  The F distribution is based on two samples of size $n_1$ and $n_2$.  So, the F density has two parameters, $v_1=(n_1-1)$ and $v_2=(n_2-1)$ (often called the numerator and denominator degrees of freedom respectively).</span>
<span id="cb68-229"><a href="#cb68-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-230"><a href="#cb68-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-231"><a href="#cb68-231" aria-hidden="true" tabindex="-1"></a>The tail quantiles of $t$, $\chi^2$ and $F$ distributions are used for hypothesis tests.  You may like to visit</span>
<span id="cb68-232"><a href="#cb68-232" aria-hidden="true" tabindex="-1"></a>https://shiny.massey.ac.nz/kgovinda/demos/demo.critical.values/ for exploring their probability densities and quantiles.</span>
<span id="cb68-233"><a href="#cb68-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-234"><a href="#cb68-234" aria-hidden="true" tabindex="-1"></a>Sampling distributions of certain statistic such as the sample Range (=Maximum-Minimum) does not exist in a closed form but the quantiles of the distribution can be obtained numerically.</span>
<span id="cb68-235"><a href="#cb68-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-236"><a href="#cb68-236" aria-hidden="true" tabindex="-1"></a>We cover the $t$ distribution in some detail below:</span>
<span id="cb68-237"><a href="#cb68-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-238"><a href="#cb68-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-239"><a href="#cb68-239" aria-hidden="true" tabindex="-1"></a>**$t$ distribution**</span>
<span id="cb68-240"><a href="#cb68-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-241"><a href="#cb68-241" aria-hidden="true" tabindex="-1"></a>Now consider a single observation $Y \sim$ N($\mu$,$\sigma$). We have already seen that if $Z = (Y-\mu)/\sigma$, then $Z \sim$ N(0,1). We can write this line slightly more generally as</span>
<span id="cb68-242"><a href="#cb68-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-243"><a href="#cb68-243" aria-hidden="true" tabindex="-1"></a>$Z= \frac{{\text {Observed(Y)-Expected(Y)}}}{{\text {Standard Deviation(Y)}}}$</span>
<span id="cb68-244"><a href="#cb68-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-245"><a href="#cb68-245" aria-hidden="true" tabindex="-1"></a>implies $Z \sim$ N(0,1).</span>
<span id="cb68-246"><a href="#cb68-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-247"><a href="#cb68-247" aria-hidden="true" tabindex="-1"></a>Next suppose we have a sample of $n$ data values $\left(y_{1},y_{2},...,y_{n} \right)$. From these we compute the sample mean $\bar{y}$. It can be shown that the expected value of $\bar{y}$ is $\mu$ and also that the standard deviation of $\bar{y}$ is $\sigma /\sqrt{n}$. That is, if we take a large number of samples, then *on average* the various values of $\bar{y}$ will tend to cluster around $\mu$, and if $n$ is large then they cluster around that value rather more closely than if $n$ is small.</span>
<span id="cb68-248"><a href="#cb68-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-249"><a href="#cb68-249" aria-hidden="true" tabindex="-1"></a>It also turns out that if $\left(y_{1},y_{2},...,y_{n} \right)$ are each Normal then</span>
<span id="cb68-250"><a href="#cb68-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-251"><a href="#cb68-251" aria-hidden="true" tabindex="-1"></a>$Z=\frac{\bar{y}-\mu }{\sigma /\sqrt{n} } =\frac{{\text {Observed(Y)-Expected(Y)}}}{{\text {Standard Deviation(Y)}}}$</span>
<span id="cb68-252"><a href="#cb68-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-253"><a href="#cb68-253" aria-hidden="true" tabindex="-1"></a>is also N(0,1).</span>
<span id="cb68-254"><a href="#cb68-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-255"><a href="#cb68-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-256"><a href="#cb68-256" aria-hidden="true" tabindex="-1"></a>In this case, the standard error of the sample mean is the standard deviation of the original population divided by the square root of $n$.</span>
<span id="cb68-257"><a href="#cb68-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-258"><a href="#cb68-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-259"><a href="#cb68-259" aria-hidden="true" tabindex="-1"></a>In fact, a profound result called the Central Limit Theorem (CLT) says that, if the sample size $n$ is large enough, then $Z = \frac{\bar{y}-\mu }{\sigma /\sqrt{n} }$ will approximately have a Normal distribution, almost regardless of the distribution of the data. (There is an exception to the CLT that relates to distributions with too many outliers.) The importance of the CLT to Statistics can hardly be overstated. It means we can draw conclusions about the population mean $\mu$ based on the position of $Z$ on the Normal tables, even though the sample of data may not look exactly Normal (for example the original data may be discrete or skewed). How large $n$ has to be, before the Central Limit Theorem can be relied on, depends on the extent of skewness, discreteness, and so on. A commonly used guideline is $n$ $\geq$ 30. But this thumb rule requires random data to be drawn from a homogeneous population with no subgrouping. For discrete variables the probability mass should not be concentrated too much on a particular value.</span>
<span id="cb68-260"><a href="#cb68-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-261"><a href="#cb68-261" aria-hidden="true" tabindex="-1"></a>Again suppose the data $\left(y_{1},y_{2},...,y_{n} \right)$ are Normal. If the population standard deviation $\sigma$ is not known then it must be estimated by the sample standard deviation $S$. The sample mean is then standardised to a ***t statistic*** instead, where $$t=\frac{\bar{y}-\mu }{S/\sqrt{n} } \sim t_{n-1}$$ The **degrees of freedom** associated with the $t$ statistic is the same as the degrees of freedom associated with $S$, that is, the $n-1$ divisor in the formula for $S$. The $t$ *distribution* [@student] is more spread out than the Standard Normal curve, that is, it is said to have fatter tails; see @fig-tdensity. The extra spread reflects the fact that observed values of ***t*** **tend to be more variable** than observed values of $z$. Essentially the $t$ distribution is predictive and takes into account the uncertainty in the population spread.</span>
<span id="cb68-262"><a href="#cb68-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-263"><a href="#cb68-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-266"><a href="#cb68-266" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-267"><a href="#cb68-267" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-268"><a href="#cb68-268" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tdensity</span></span>
<span id="cb68-269"><a href="#cb68-269" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Student's t Distribution"</span></span>
<span id="cb68-270"><a href="#cb68-270" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4 </span></span>
<span id="cb68-271"><a href="#cb68-271" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 6</span></span>
<span id="cb68-272"><a href="#cb68-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-273"><a href="#cb68-273" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">dt</span>(x,<span class="dv">1</span>), <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.4</span>), <span class="at">ylab=</span><span class="st">"Density"</span> )</span>
<span id="cb68-274"><a href="#cb68-274" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">dt</span>(x,<span class="dv">2</span>), <span class="at">add=</span>T, <span class="at">lty=</span><span class="dv">2</span> )</span>
<span id="cb68-275"><a href="#cb68-275" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">dt</span>(x,<span class="dv">5</span>), <span class="at">add=</span>T, <span class="at">lty=</span><span class="dv">3</span> )</span>
<span id="cb68-276"><a href="#cb68-276" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">dnorm</span>(x), <span class="at">add=</span>T,<span class="at">lty=</span><span class="dv">4</span> )</span>
<span id="cb68-277"><a href="#cb68-277" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">c</span>(<span class="st">"t(1)"</span>, <span class="st">"t(2)"</span>, <span class="st">"t(5)"</span>, <span class="st">"Normal"</span>),</span>
<span id="cb68-278"><a href="#cb68-278" aria-hidden="true" tabindex="-1"></a><span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>))</span>
<span id="cb68-279"><a href="#cb68-279" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-280"><a href="#cb68-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-281"><a href="#cb68-281" aria-hidden="true" tabindex="-1"></a>Note that, strictly speaking, the $t$ distribution only holds true if the data $\left(y_{1},y_{2},...,y_{n} \right)$ came from a Normal distribution. However some simulation studies have shown that the ratio $t=\frac{\bar{y}-\mu }{S/\sqrt{n} }$ closely follows a $t$-curve even if the data are not Normal, provided that the data are not highly skewed. (Skewness tends to have a marked impact on $S$ in the denominator, as well as on the mean $\bar{y}$.) In practice, whenever a model is fitted to data, it is important to check that the assumptions of the model seem to hold. In this case, we could form a Normal Plot of the values of $\left(y_{1},y_{2},...,y_{n} \right)$ to visually check whether they seem to follow a Normal distribution. The better the line, the more confidence we can have in our inference from the data.</span>
<span id="cb68-282"><a href="#cb68-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-283"><a href="#cb68-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-284"><a href="#cb68-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-285"><a href="#cb68-285" aria-hidden="true" tabindex="-1"></a><span class="fu"># Confidence Intervals for Population Mean &amp; $t$-tests</span></span>
<span id="cb68-286"><a href="#cb68-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-287"><a href="#cb68-287" aria-hidden="true" tabindex="-1"></a>If the batch of data consists of the whole population, the batch mean will be exactly the population mean $\mu$. If the batch is a sample, the sample mean $\bar{y}$ will be an exact summary of the sample, but may not be the same as $\mu$. In fact, if another sample is drawn, a sample mean different to the first will be obtained. The sample mean will not exactly equal the population mean but will vary about it from sample to sample. This variation of the sample mean about the population mean is discussed in every introductory textbook on Statistics and also in Chapter 1 of these notes.</span>
<span id="cb68-288"><a href="#cb68-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-289"><a href="#cb68-289" aria-hidden="true" tabindex="-1"></a>If the batch of data is a sample, it is natural to use it to try to infer certain characteristics of the population. In particular, we estimate the population mean $\mu$ by the sample mean, $\bar{y}$. It is also helpful to calculate an interval in which the population mean is likely to fall, giving the **interval estimate** with a certain **margin of error:**</span>
<span id="cb68-290"><a href="#cb68-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-291"><a href="#cb68-291" aria-hidden="true" tabindex="-1"></a>Interval estimate of $\mu$ = $\bar{y}$ $\pm$ margin of error .</span>
<span id="cb68-292"><a href="#cb68-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-293"><a href="#cb68-293" aria-hidden="true" tabindex="-1"></a>To find the margin of error, we assume that the random variable, $\bar{y}$, has a Normal distribution. Since we can never be 100% certain whether $\mu$ falls in a certain interval we often settle for a 95% level of confidence. This means that the margin of error should be about two standard deviations of $\bar{y}$. To be more correct,</span>
<span id="cb68-294"><a href="#cb68-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-295"><a href="#cb68-295" aria-hidden="true" tabindex="-1"></a>margin of error = $t$ $\times$ e.s.e. ($\bar{y}$).</span>
<span id="cb68-296"><a href="#cb68-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-297"><a href="#cb68-297" aria-hidden="true" tabindex="-1"></a>Here, e.s.e. stands for **estimated standard error**. It is usual practice to denote the spread of the observations in the batches by the term **standard deviation**. When referring to other statistics such as the sample mean, $\bar{y}$, or the coefficients in an equation, the standard deviation of these statistics is usually termed **standard error**. For a sample of size $n$, the standard error of $\bar{y}$ is given by:</span>
<span id="cb68-298"><a href="#cb68-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-299"><a href="#cb68-299" aria-hidden="true" tabindex="-1"></a>(estimated) standard error ($\bar{y}$) = standard deviation($Y$)/$\sqrt n$.</span>
<span id="cb68-300"><a href="#cb68-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-301"><a href="#cb68-301" aria-hidden="true" tabindex="-1"></a>If the batch is a sample, we rarely know the standard deviation of $Y$ in the whole population and it is for this reason that we need to estimate it from the sample. It seems a long story but we have finally arrived at e.s.e., the estimated standard error $S/\sqrt n$ To be specific, we define a **95% Confidence Interval** for $\mu$ as</span>
<span id="cb68-302"><a href="#cb68-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-303"><a href="#cb68-303" aria-hidden="true" tabindex="-1"></a>$\bar{y}$ $\pm$ $t \times (S/\sqrt n)$</span>
<span id="cb68-304"><a href="#cb68-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-305"><a href="#cb68-305" aria-hidden="true" tabindex="-1"></a>The term $t$ is the appropriate percentile of the $t$-statistic with $n-1$ degrees of freedom. The value of $t$ will generally be greater than 2. This reflects the fact that there is additional variability in that the standard error is not known but must be estimated.</span>
<span id="cb68-306"><a href="#cb68-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-307"><a href="#cb68-307" aria-hidden="true" tabindex="-1"></a>Be aware that this derivation of the confidence interval means that it is a statistic (that is, a formula based on sample observations) that works for 95% of samples. What we mean is that if we use this formula repeatedly over our lifetime, then on average 95% of the intervals we obtain will be correct (that is, will contain the true mean $\mu$) and 5% of the intervals we obtain will not be correct (that is, will not contain $\mu$). The only way to cut down our error rate is to increase the confidence level, for example use 99% confidence intervals instead, which means using a different value of $t$. The problem with this strategy is that our intervals will always be that much wider, and we do not always need such a high level of confidence. In conclusion, we usually employ 95% confidence intervals unless the context indicates we should use some other level.</span>
<span id="cb68-308"><a href="#cb68-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-309"><a href="#cb68-309" aria-hidden="true" tabindex="-1"></a>For the TV viewing time data, the sample mean = 1729.28 with (estimated) standard deviation = 567.91. Hence the (estimated) standard error of the mean is $S/ \sqrt n = 83.73$. For the sample of size $n = 46$, we have $n-1 = 46-1= 45$ degrees of freedom for the $t$ statistic. We may use either software or use Student's $t$ tables and obtain $t$ ordinate (i.e. quantile) value as 2.01 corresponding to a right tail probability (area) of 0.025; see @fig-tquant. Due to symmetry, the area below the $t$ ordinate of $-2.01$ will also be 0.025.</span>
<span id="cb68-310"><a href="#cb68-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-313"><a href="#cb68-313" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-314"><a href="#cb68-314" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-315"><a href="#cb68-315" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tquant</span></span>
<span id="cb68-316"><a href="#cb68-316" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Student's t quantiles for CI construction"</span></span>
<span id="cb68-317"><a href="#cb68-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-318"><a href="#cb68-318" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>( <span class="fu">dt</span>(x,<span class="dv">45</span>), <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.4</span>), <span class="at">ylab=</span><span class="st">"Density"</span>)</span>
<span id="cb68-319"><a href="#cb68-319" aria-hidden="true" tabindex="-1"></a>lowert<span class="ot">=</span><span class="fu">qt</span>(.<span class="dv">025</span>, <span class="dv">45</span>)</span>
<span id="cb68-320"><a href="#cb68-320" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>lowert)</span>
<span id="cb68-321"><a href="#cb68-321" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(lowert, .<span class="dv">4</span>, <span class="st">"lower t quantile"</span>)</span>
<span id="cb68-322"><a href="#cb68-322" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="sc">-</span><span class="fl">2.9</span>, <span class="fl">0.1</span>, <span class="st">"2.5% left tail area"</span>)</span>
<span id="cb68-323"><a href="#cb68-323" aria-hidden="true" tabindex="-1"></a>uppert<span class="ot">=</span><span class="fu">qt</span>(.<span class="dv">975</span>, <span class="dv">45</span>)</span>
<span id="cb68-324"><a href="#cb68-324" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>uppert)</span>
<span id="cb68-325"><a href="#cb68-325" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(uppert, .<span class="dv">4</span>, <span class="st">"upper t quantile"</span>)</span>
<span id="cb68-326"><a href="#cb68-326" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">2.9</span>, <span class="fl">0.1</span>, <span class="st">"2.5% right tail area"</span>)</span>
<span id="cb68-327"><a href="#cb68-327" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-328"><a href="#cb68-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-329"><a href="#cb68-329" aria-hidden="true" tabindex="-1"></a>Having obtained the theoretical $t$-value, the 95% confidence interval can be found as</span>
<span id="cb68-330"><a href="#cb68-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-331"><a href="#cb68-331" aria-hidden="true" tabindex="-1"></a>$\bar{y}\pm t\times S/ \sqrt n$ = $1729.28\pm 2.01\times83.73$ or (1560.63, 1897.93).</span>
<span id="cb68-332"><a href="#cb68-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-333"><a href="#cb68-333" aria-hidden="true" tabindex="-1"></a>With 95% confidence, we can state that the population mean falls in the interval (1560.63, 1897.93).</span>
<span id="cb68-334"><a href="#cb68-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-335"><a href="#cb68-335" aria-hidden="true" tabindex="-1"></a>If we want to be **more confident** that we have included the true value of the population mean, we could use a 99% confidence interval instead. That is, we make the interval wider by using the 99$^{\text th}$ percentile of the $t$ statistic (namely 2.69). The 99% confidence interval is computed as (1504.07, 1954.49), clearly wider than the 95% confidence interval.</span>
<span id="cb68-336"><a href="#cb68-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-337"><a href="#cb68-337" aria-hidden="true" tabindex="-1"></a>The confidence interval estimate of the unknown population parameter (mean in the above discussion) is affected by the following:</span>
<span id="cb68-338"><a href="#cb68-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-339"><a href="#cb68-339" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The amount of variation in the population.</span>
<span id="cb68-340"><a href="#cb68-340" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The nature of variation in the population.</span>
<span id="cb68-341"><a href="#cb68-341" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Sample size.</span>
<span id="cb68-342"><a href="#cb68-342" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>The degree of confidence (probability) needed.</span>
<span id="cb68-343"><a href="#cb68-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-344"><a href="#cb68-344" aria-hidden="true" tabindex="-1"></a>Sample size is simply a trade off to obtain a confidence interval of a desired width. If a population is more variable, then it has to be sampled heavily. The width of the confidence interval and the degree of confidence are inversely related. The corollary is that a 100% confidence interval is of infinite width.</span>
<span id="cb68-345"><a href="#cb68-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-346"><a href="#cb68-346" aria-hidden="true" tabindex="-1"></a>Confidence intervals are NOT unique. For example, we can leave 1% probability for the left tail and 4% probability for the right tail. But such an interval will be wider for symmetric distributions but may be appropriate for skewed sampling distributions. Confidence intervals may not always possess good statistical properties such as accuracy (probability of covering any wrong value of the parameter than the other) and unbiasedness (An unbiased 95% confidence interval has probability no more than 5% of covering any value of the parameter). Stronger assumptions must be made to obtain sharper confidence intervals.</span>
<span id="cb68-347"><a href="#cb68-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-348"><a href="#cb68-348" aria-hidden="true" tabindex="-1"></a>One sided (lower or upper only) confidence bounds can be obtained and the confidence need not be expressed as an interval.</span>
<span id="cb68-349"><a href="#cb68-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-350"><a href="#cb68-350" aria-hidden="true" tabindex="-1"></a>Also note that there many types of statistical intervals available in the literature, and the confidence interval is one such interval which is expected to capture an unknown <span class="in">`parameter`</span> of the population. In our example, we used the mean of the population (parameter). Naturally, we can construct a CI for the median, 90th percentile etc. A prediction interval (which will discuss later on) deals with covering a desired fraction of the population and is different from a confidence interval.</span>
<span id="cb68-351"><a href="#cb68-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-352"><a href="#cb68-352" aria-hidden="true" tabindex="-1"></a><span class="fu"># Hypothesis Testing</span></span>
<span id="cb68-353"><a href="#cb68-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-354"><a href="#cb68-354" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "_… the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation. Every experiment may be said to exist only to give the facts a chance of disproving the null hypothesis._"</span></span>
<span id="cb68-355"><a href="#cb68-355" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb68-356"><a href="#cb68-356" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; – Sir R.A. Fisher.</span></span>
<span id="cb68-357"><a href="#cb68-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-358"><a href="#cb68-358" aria-hidden="true" tabindex="-1"></a>Note that the confidence intervals give a range of likely values for the mean $\mu$. Putting it another way, if someone postulated a value of $\mu$ falling outside this interval (for example, that $\mu=1000$), then we could tell them that our data do not support their claim. On the other hand if they hypothesise a value of $\mu$ inside the interval, (for example that $\mu$ = 1600), then this value may not be exactly what our data suggest, but we could not reject their claim.</span>
<span id="cb68-359"><a href="#cb68-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-360"><a href="#cb68-360" aria-hidden="true" tabindex="-1"></a>These thoughts lead to the more formal idea of hypothesis testing. In hypothesis testing we begin with a hypothesis about the value of some parameter, for example that the population mean $\mu$ equals some specific value $\mu_0$ say. In the television example we could use the specific value $\mu_0=1500$ hours, say, but for the purpose of general discussion we prefer to just assume $\mu_0$ is some fixed value.</span>
<span id="cb68-361"><a href="#cb68-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-362"><a href="#cb68-362" aria-hidden="true" tabindex="-1"></a>:::{.callout-important}</span>
<span id="cb68-363"><a href="#cb68-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-364"><a href="#cb68-364" aria-hidden="true" tabindex="-1"></a><span class="fu"># Two important points to remember about null hypotheses:</span></span>
<span id="cb68-365"><a href="#cb68-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-366"><a href="#cb68-366" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Null (and alternative) hypotheses are **always** about population parameters, and **never** about sample statistics; inferences are always about the population, never about the sample.</span>
<span id="cb68-367"><a href="#cb68-367" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With a large $p$-value, we **never** say that we have "accepted" or "confirmed" the null hypothesis; we only ever reject or fail to reject a null hypothesis.</span>
<span id="cb68-368"><a href="#cb68-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-369"><a href="#cb68-369" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb68-370"><a href="#cb68-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-371"><a href="#cb68-371" aria-hidden="true" tabindex="-1"></a>The philosophy of classical statistical hypothesis testing was explained by @fisher1935 using a context known as the "tea tasting lady" experiment. The original tea tasting experiment used a permutation test but we simplify this context in the following description.</span>
<span id="cb68-372"><a href="#cb68-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-373"><a href="#cb68-373" aria-hidden="true" tabindex="-1"></a>A lady claimed that she can taste and tell whether milk or tea was poured first into the cup. Suppose that we tossed a fair coin to determine this in a series of trials. Coin tossing enables the binomial probability distribution as a model to determine the probability of various outcomes when tea tasting is done in a random order. Recall that the binomial mass function is given by-</span>
<span id="cb68-374"><a href="#cb68-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-375"><a href="#cb68-375" aria-hidden="true" tabindex="-1"></a>$$P(X=x) = \binom{n}{x} p ^ x (1-p) ^ {n-x}$$</span>
<span id="cb68-376"><a href="#cb68-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-377"><a href="#cb68-377" aria-hidden="true" tabindex="-1"></a>The *null hypothesis* is that the lady was guessing blindly. That is, there is equal probability for a correct guess and an incorrect one. The sample space here consists of all possible answers the lady might give in a series of random tasting of tea cups. Assume that 10 cups (with equal cases of milk and tea poured first) were randomly arranged and the lady guessed 9 cases correctly. We can calculate the probability of 9 correct guesses or more under the null hypothesis using the binomial distribution. We can find the probability of certain $x$ number of correct guesses under the null hypothesis using <span class="in">`R`</span>. See @tbl-binom1.</span>
<span id="cb68-378"><a href="#cb68-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-381"><a href="#cb68-381" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-382"><a href="#cb68-382" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb68-383"><a href="#cb68-383" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x, <span class="at">Probability=</span><span class="fu">round</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pbinom</span>(x, <span class="dv">10</span>, <span class="fl">0.5</span>), <span class="dv">3</span>))</span>
<span id="cb68-384"><a href="#cb68-384" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-385"><a href="#cb68-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-386"><a href="#cb68-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-389"><a href="#cb68-389" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-390"><a href="#cb68-390" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb68-391"><a href="#cb68-391" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb68-392"><a href="#cb68-392" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-binom1</span></span>
<span id="cb68-393"><a href="#cb68-393" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Binomial probabilities"</span></span>
<span id="cb68-394"><a href="#cb68-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-395"><a href="#cb68-395" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb68-396"><a href="#cb68-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-397"><a href="#cb68-397" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(out) <span class="sc">|&gt;</span> </span>
<span id="cb68-398"><a href="#cb68-398" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> F)</span>
<span id="cb68-399"><a href="#cb68-399" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-400"><a href="#cb68-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-401"><a href="#cb68-401" aria-hidden="true" tabindex="-1"></a>@tbl-binom1 shows that the evidence in our data is well against the null hypothesis because the probabilities 9 or 10 correct answers are rather small.</span>
<span id="cb68-402"><a href="#cb68-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-403"><a href="#cb68-403" aria-hidden="true" tabindex="-1"></a>So we cannot reject the lady's claim. We often write our null hypothesis symbolically. For the tea lady example, the null hypothesis is written as $H_0:p = 0.5$, where $H_0$ is pronounced 'H nought' and means 'the null hypothesis we are testing' and $p$ stands for the unknown proportion and $p=0.5$ stands for guessing the outcome by throwing a coin.</span>
<span id="cb68-404"><a href="#cb68-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-405"><a href="#cb68-405" aria-hidden="true" tabindex="-1"></a>In our testing we give the $H_0$ the 'benefit of the doubt'. That is, we won't reject $H_0$ unless we are strongly convinced by the data that it cannot be right. There is some analogy here to a court case, where the defendant (the hypothesis) is assumed innocent until proven guilty beyond all reasonable doubt (hypothesis assumed true until the data prove it false beyond all reasonable doubt). Unlike a court case, however, we are usually able to use probability models to *quantify* the level of doubt/disbelief in $H_0$. Another example is that a student may like to verify the weight labelling on butter sold in the supermarket is correct or not. Assume that the student collects a sample 20 blocks of butter with 500g nominal weight declared. In this case, the null hypothesis would be that the true mean weight is indeed 500g. In legal metrology, formulation of such a null hypothesis is rather natural. @Hall1986 provided a good discussion on the nature of hypothesis testing in his paper entitled "Statistical significance: Balancing evidence against doubt" (not compulsory but worth a read).</span>
<span id="cb68-406"><a href="#cb68-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-407"><a href="#cb68-407" aria-hidden="true" tabindex="-1"></a>We often use software to perform hypothesis tests. The <span class="in">`R`</span> output performing a one-sample proportion test output on whether the lady was outperforming the mere guessing strategy is shown below:</span>
<span id="cb68-408"><a href="#cb68-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-411"><a href="#cb68-411" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-412"><a href="#cb68-412" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="at">x=</span><span class="dv">9</span>, <span class="at">n=</span><span class="dv">10</span>, <span class="at">p=</span><span class="fl">0.5</span>, <span class="at">alternative=</span><span class="st">"greater"</span>)</span>
<span id="cb68-413"><a href="#cb68-413" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-414"><a href="#cb68-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-415"><a href="#cb68-415" aria-hidden="true" tabindex="-1"></a>Do not worry the entries <span class="in">`P-value`</span> and <span class="in">`alternative`</span> appearing in the above output and these concepts are further explained in the later section.</span>
<span id="cb68-416"><a href="#cb68-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-417"><a href="#cb68-417" aria-hidden="true" tabindex="-1"></a>As pointed out by Fisher, our hypothesis testing procedure does not prove or disprove the null hypothesis. We are simply assessing the evidence in the data against the null hypothesis. While describing the tea tasting experiment, @fisher1935 (p.16) warned as follows:</span>
<span id="cb68-418"><a href="#cb68-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-419"><a href="#cb68-419" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "_In relation to any experiment we may speak of this as the "null hypothesis", and it should be noted that the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation._"</span></span>
<span id="cb68-420"><a href="#cb68-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-421"><a href="#cb68-421" aria-hidden="true" tabindex="-1"></a>More ideas on hypothesis testing follow in the next section.</span>
<span id="cb68-422"><a href="#cb68-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-423"><a href="#cb68-423" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hypothesis testing for mean</span></span>
<span id="cb68-424"><a href="#cb68-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-425"><a href="#cb68-425" aria-hidden="true" tabindex="-1"></a>For testing the mean of a population, the fact that we have specified a parameter value in $H_0$ enables us to specify a probability distribution, for example that the data are a random sample from N($\mu_0$, $\sigma$). This immediately implies that</span>
<span id="cb68-426"><a href="#cb68-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-427"><a href="#cb68-427" aria-hidden="true" tabindex="-1"></a>$t = \frac{\bar{y}-\mu _{0} }{S/\sqrt{n} } \sim t{}_{n-1}$</span>
<span id="cb68-428"><a href="#cb68-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-429"><a href="#cb68-429" aria-hidden="true" tabindex="-1"></a>Now if the hypothesis $H_0$ is true, we should have $\mu\cong\mu_0$ . The notation equal-with-squiggle, $\cong$, stands for "approximately equal to" ), so that $t$ should generally be close to 0. If the hypothesis is false we should get either much less than $\mu_0$ or much greater than $\mu_0$, in which case $t$ should be large, out in the tails of the $t_{n-1}$ distribution. Since we have a probability distribution, we can quantify just how unlikely our particular $t$ is by comparing our sample value with the distribution.</span>
<span id="cb68-430"><a href="#cb68-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-431"><a href="#cb68-431" aria-hidden="true" tabindex="-1"></a>Let's make things more specific by considering the television example. The degrees of freedom are $n-1 = 45$. We have already indicated that $t\cong0$ implies $\mu\cong\mu_0$, in other words that the data matches the hypothesis very closely. But suppose instead we observed $t= 0.68$. Could we regard this as an unusually large value of $t$, that is, as evidence against $H_0$? The answer is no! The reason is that 0.68 is the upper quartile of the $t_{45}$ distribution; in other words half (50%) of the time we would see values of $t$ either greater than 0.68 or less than -0.68, even if the hypothesis $H_0$ is true. Now what if $t$ were below -1.6794? Would that be regarded as an unusual amount of discrepancy between $t$ and $\mu$, that is as evidence against $H_0$? The answer is maybe. The fact is that 10% of the time one sees $t$ $&lt;-1.6794$ or $t&gt;1.6794$, even if $H_0$ is true. So if we use this rule, we have a 10% chance of wrongly rejecting $H_0$. Few New Zealanders would feel happy with a legal system that allowed a 10% chance of wrongfully convicting an innocent person. Finally, what if we observe $t= -2.6896$? Only 1% of the area under the $t_{45}$ curve lies outside the interval -2.6896 to +2.6896, so we would conclude that such a value of $t$ was quite unlikely. This then would be strong evidence against $H_0$.</span>
<span id="cb68-432"><a href="#cb68-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-433"><a href="#cb68-433" aria-hidden="true" tabindex="-1"></a>Suppose now we test the extremely unlikely hypothesis $H_0:\mu = 0$. Since $S/\sqrt n=83.7$, we obtain $t = (1729.3-0)/83.7 = 20.65$. This is far larger than could be expected by chance, so the $t$ statistic provides clear evidence against $H_0$.</span>
<span id="cb68-434"><a href="#cb68-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-435"><a href="#cb68-435" aria-hidden="true" tabindex="-1"></a>A more realistic test may be whether $H_0:\mu = 1500$. Perhaps a previous study found an average time of viewing of 1500 minutes per week, and we want to check this. The test statistic is: $t=(1729.3-1500)/83.7=2.74$. This is just outside our 1% bounds established earlier, so we conclude the data and $H_0$ do not seem to agree. We reject $H_0:\mu=1500$.</span>
<span id="cb68-436"><a href="#cb68-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-437"><a href="#cb68-437" aria-hidden="true" tabindex="-1"></a>Finally suppose we test the hypothesis $H_0:\mu=1600$. Again this hypothesis could have been prompted by a previous study. Then $t=(1729.3-1600)/83.7=1.54$. This is within the interval -1.6794 to 1.6794, suggesting that 1.54 is not an unusually highly value since it is exceeded more than 10% of the time when $H_0$ is true. We would not reject the hypothesis $H_0:\mu = 1600$.</span>
<span id="cb68-438"><a href="#cb68-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-439"><a href="#cb68-439" aria-hidden="true" tabindex="-1"></a>When statistical test of hypothesis is done using <span class="in">`R`</span> software, we rely on the $p$-value displayed.</span>
<span id="cb68-440"><a href="#cb68-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-441"><a href="#cb68-441" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=TRUE}</span></span>
<span id="cb68-442"><a href="#cb68-442" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(tv<span class="sc">$</span>TELETIME, <span class="at">mu=</span><span class="dv">1500</span>)</span>
<span id="cb68-443"><a href="#cb68-443" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-444"><a href="#cb68-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-445"><a href="#cb68-445" aria-hidden="true" tabindex="-1"></a>So what is a $P$-value? Formally, ***A P-value is the probability of observing data as extreme or more extreme as the data you actually observed, if*** $H_0$ is true. This sounds like a very abstract and difficult concept to grasp, but it's in fact exactly the rule we have been using. We saw that 50% of the time, $t&lt;-0.68$ or $t&gt;0.68$. So the $p$-value for a $t=0.68$ is 0.5. We saw that 10% of the time, $t&lt;-1.6794$ or $t&gt;1.6794$. So the $p$-value of $t= -1.6794$ is 0.1. And we saw that 1% of the time $t&lt;-2.6896$ or $t&gt;2.6896$, so the $p$-value of $t=2.6896$ is 0.01. What is the $p$-value of $t= 2.74$? The answer is 0.009 or just under 1%. What is the $p$-value of $t= 1.54$? Answer is 0.130.</span>
<span id="cb68-446"><a href="#cb68-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-447"><a href="#cb68-447" aria-hidden="true" tabindex="-1"></a>Usually, we reject $H_0$ in favour of $H_{1}$ if the $p$-value of the data is $&lt; 0.05$. In this case the test is said to be **significant**, and the 0.05 is called the **significance level** of the test. Otherwise (when we don't reject $H_0$) we call the test **non-significant**. Some refer to a $p$-value $&lt;0.01$ as very significant or highly significant. In journal articles and published tables of results, the three case non-significant, significant, and highly significant are often abbreviated as `NS`, `*` and `**` respectively. The issues of using a standard cut-off of 5% for significance level, largely insisted in journals, has had its unintended consequences. The *false discovery* in science can be avoided if P values are not used as the sole criterion to draw conclusions. Read the advice on P values presented next.</span>
<span id="cb68-448"><a href="#cb68-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-449"><a href="#cb68-449" aria-hidden="true" tabindex="-1"></a>**Advice on the use of P values**</span>
<span id="cb68-450"><a href="#cb68-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-451"><a href="#cb68-451" aria-hidden="true" tabindex="-1"></a>Unfortunately the P-values are often misunderstood in practice. The advice issued by the American Statistical Association (<span class="ot">&lt;https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf&gt;</span>) is noteworthy:</span>
<span id="cb68-452"><a href="#cb68-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-453"><a href="#cb68-453" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The statement's six principles, many of which address misconceptions and misuse of the p-value, are the following:</span></span>
<span id="cb68-454"><a href="#cb68-454" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb68-455"><a href="#cb68-455" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 1.  P-values can indicate how incompatible the data are with a specified statistical model.</span></span>
<span id="cb68-456"><a href="#cb68-456" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb68-457"><a href="#cb68-457" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 2.  P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.</span></span>
<span id="cb68-458"><a href="#cb68-458" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb68-459"><a href="#cb68-459" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 3.  Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.</span></span>
<span id="cb68-460"><a href="#cb68-460" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb68-461"><a href="#cb68-461" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 4.  Proper inference requires full reporting and transparency.</span></span>
<span id="cb68-462"><a href="#cb68-462" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb68-463"><a href="#cb68-463" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 5.  A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.</span></span>
<span id="cb68-464"><a href="#cb68-464" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb68-465"><a href="#cb68-465" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 6.  By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.</span></span>
<span id="cb68-466"><a href="#cb68-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-467"><a href="#cb68-467" aria-hidden="true" tabindex="-1"></a>In order to fully appreciate the reasons behind the above advice, we need learn more theory but you should be able to understand the following distinction between *statistical* significance and *practical* significance. A hypothesis test may suggest that the estimated size of the effect is not big enough compared to the effect that can occur due to errors under the assumed model. A small effect, particularly if it is known to be *caused* by a variable, can be of practical importance and can contribute to scientific knowledge. The opposite scenario is also possible. We may find a small difference to be statistically significant because of the large sample size but such a difference may not be of much practical significance.</span>
<span id="cb68-468"><a href="#cb68-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-469"><a href="#cb68-469" aria-hidden="true" tabindex="-1"></a>A hypothesis test has a certain power (probability) to reject the null hypothesis when it is false. This power (probability) is a function of the sample size and the unknown parameters of the probability model adopted for testing.</span>
<span id="cb68-470"><a href="#cb68-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-471"><a href="#cb68-471" aria-hidden="true" tabindex="-1"></a>Assume that we are testing the null hypothesis $H_0:\mu = 0$ using the null model $N(0,1)$. The power of the one sample t test can be evaluated using the <span class="in">`R`</span> function <span class="in">`power.t.test()`</span> for a given $\delta$, the difference in the true mean and what was hypothesised under $H_0$. For example, the power of the $t$-test for $n=30$ is lower than the power when $n=50$ (say) when other settings are the same. Try-</span>
<span id="cb68-472"><a href="#cb68-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-473"><a href="#cb68-473" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, results=FALSE}</span></span>
<span id="cb68-474"><a href="#cb68-474" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">30</span>, <span class="at">delta =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.05</span>)</span>
<span id="cb68-475"><a href="#cb68-475" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">50</span>, <span class="at">delta =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.05</span>)</span>
<span id="cb68-476"><a href="#cb68-476" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-477"><a href="#cb68-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-478"><a href="#cb68-478" aria-hidden="true" tabindex="-1"></a>The power to detect a small change in the mean is often low. Try-</span>
<span id="cb68-479"><a href="#cb68-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-480"><a href="#cb68-480" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, results=FALSE}</span></span>
<span id="cb68-481"><a href="#cb68-481" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">30</span>, <span class="at">delta =</span> .<span class="dv">25</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.05</span>)</span>
<span id="cb68-482"><a href="#cb68-482" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">30</span>, <span class="at">delta =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.05</span>)</span>
<span id="cb68-483"><a href="#cb68-483" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-484"><a href="#cb68-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-485"><a href="#cb68-485" aria-hidden="true" tabindex="-1"></a>There is a trade-off between the significance level (Type I error or false positive) and Type II error or false negative (=1-power) probabilities. Try-</span>
<span id="cb68-486"><a href="#cb68-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-487"><a href="#cb68-487" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, results=FALSE}</span></span>
<span id="cb68-488"><a href="#cb68-488" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">30</span>, <span class="at">delta =</span> <span class="fl">0.5</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.05</span>)</span>
<span id="cb68-489"><a href="#cb68-489" aria-hidden="true" tabindex="-1"></a><span class="fu">power.t.test</span>(<span class="at">n =</span> <span class="dv">30</span>, <span class="at">delta =</span> <span class="fl">0.5</span>, <span class="at">sd =</span> <span class="dv">1</span>, <span class="at">sig.level =</span> <span class="fl">0.01</span>)</span>
<span id="cb68-490"><a href="#cb68-490" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-491"><a href="#cb68-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-492"><a href="#cb68-492" aria-hidden="true" tabindex="-1"></a>When we test many hypothesis in tandem, we are more concerned on the overall or family-wise error rates. The issues of false discovery in science is discussed in a later section.</span>
<span id="cb68-493"><a href="#cb68-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-494"><a href="#cb68-494" aria-hidden="true" tabindex="-1"></a>*P hacking* is a phrase used when a particular test or a meta procedure is deliberately chosen either to ensure a low p-value or just to achieve a value below 0.05. </span>
<span id="cb68-495"><a href="#cb68-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-496"><a href="#cb68-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-497"><a href="#cb68-497" aria-hidden="true" tabindex="-1"></a><span class="fu"># Inferences for Two Groups</span></span>
<span id="cb68-498"><a href="#cb68-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-499"><a href="#cb68-499" aria-hidden="true" tabindex="-1"></a>In an earlier section, we considered confidence intervals and hypothesis tests for the television viewing times of pupils (**tv**). Now this sample can be divided into two groups, viewing times for boys and viewing times for girls. The boxplots in @fig-tvdistsex indicate that the median viewing time for boys (Sex=1) is less than for girls (Sex=2) although the spread of times is greater for the group of boys. The slopes in the normal quantile plot also confirms the differing spread.</span>
<span id="cb68-500"><a href="#cb68-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-503"><a href="#cb68-503" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-504"><a href="#cb68-504" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-505"><a href="#cb68-505" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tvdistsex</span></span>
<span id="cb68-506"><a href="#cb68-506" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Distribution of TV viewing times for boys and girls'</span></span>
<span id="cb68-507"><a href="#cb68-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-508"><a href="#cb68-508" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tv) <span class="sc">+</span> </span>
<span id="cb68-509"><a href="#cb68-509" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">sample =</span> TELETIME, <span class="at">color=</span>SEX, <span class="at">shape=</span>SEX) <span class="sc">+</span> </span>
<span id="cb68-510"><a href="#cb68-510" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq</span>() <span class="sc">+</span></span>
<span id="cb68-511"><a href="#cb68-511" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="fu">aes</span>(<span class="at">linetype=</span>SEX))</span>
<span id="cb68-512"><a href="#cb68-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-513"><a href="#cb68-513" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tv) <span class="sc">+</span> </span>
<span id="cb68-514"><a href="#cb68-514" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>TELETIME, <span class="at">x=</span>SEX) <span class="sc">+</span></span>
<span id="cb68-515"><a href="#cb68-515" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb68-516"><a href="#cb68-516" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb68-517"><a href="#cb68-517" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span>
<span id="cb68-518"><a href="#cb68-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-519"><a href="#cb68-519" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1,p2, <span class="at">ncol=</span><span class="dv">1</span>) </span>
<span id="cb68-520"><a href="#cb68-520" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-521"><a href="#cb68-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-522"><a href="#cb68-522" aria-hidden="true" tabindex="-1"></a>We see that average number of minutes per week of television watched is 1668 for boys and 1790 for girls. The difference in sample means for these two groups is $1668-1790=-122$. The question arises as to whether this indicates a real difference in viewing times between the sexes in the population as a whole (all New Zealand primary age pupils?). We should keep in mind the distinction between samples and populations and, to do this, population values are usually written in Greek letters. The notation is:</span>
<span id="cb68-523"><a href="#cb68-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-524"><a href="#cb68-524" aria-hidden="true" tabindex="-1"></a>| Population means and SDs |   Sample means and SDs    | Sample size |</span>
<span id="cb68-525"><a href="#cb68-525" aria-hidden="true" tabindex="-1"></a>|:-------------------------|--------------------------:|:----------|</span>
<span id="cb68-526"><a href="#cb68-526" aria-hidden="true" tabindex="-1"></a>| $\mu _1 ,\,\,\sigma _1$  | $\bar {y}_1 \,\,,\,\,S_1$ | $n_1$     |</span>
<span id="cb68-527"><a href="#cb68-527" aria-hidden="true" tabindex="-1"></a>| $\mu _2 ,\,\,\sigma _2$  | $\bar {y}_2 \,\,,\,\,S_2$ | $n_2$     |</span>
<span id="cb68-528"><a href="#cb68-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-529"><a href="#cb68-529" aria-hidden="true" tabindex="-1"></a>We are assuming that we have information about the samples and wish to make inferences about the populations. In particular, we wish to make inferences about the difference in population means, ($\mu _{1}-\mu _{2}$), from the difference in sample means, $\bar{y}_{1} -\bar{y}_{2}$, and the variances of the two samples, $S_{1}^{2}$ and $S_{2}^{2}$.</span>
<span id="cb68-530"><a href="#cb68-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-531"><a href="#cb68-531" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hypothesis tests for two groups</span></span>
<span id="cb68-532"><a href="#cb68-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-533"><a href="#cb68-533" aria-hidden="true" tabindex="-1"></a>The null hypothesis is that population means of the two groups are equal (written as $H_0:\mu_1 = \mu_2$ or $H_0:\mu_1-\mu_2=0$). The alternative hypothesis is that the population means of the two groups are different (i.e., $H_0:\mu_1\neq \mu_2$ or $H_0:\mu_1-\mu_2\neq0$). Note that when the two population means are different either $\mu_1&gt;\mu_2$ or $\mu_1&lt;\mu_2$.</span>
<span id="cb68-534"><a href="#cb68-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-535"><a href="#cb68-535" aria-hidden="true" tabindex="-1"></a>The difference in sample means $\bar{y}_{1} -\bar{y}_{2}$ can be standardized to give a $t$ statistic:</span>
<span id="cb68-536"><a href="#cb68-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-537"><a href="#cb68-537" aria-hidden="true" tabindex="-1"></a>$t$ = (($\bar{y}_{1} -\bar{y}_{2}$)- expected)/e.s.e.</span>
<span id="cb68-538"><a href="#cb68-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-539"><a href="#cb68-539" aria-hidden="true" tabindex="-1"></a>The test statistic is $(\bar{y}_{1} -\bar{y}_{2})$, which has the expected value of zero under the null hypothesis.</span>
<span id="cb68-540"><a href="#cb68-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-541"><a href="#cb68-541" aria-hidden="true" tabindex="-1"></a>The estimated standard error (e.s.e) of ($\bar{y}_{1} -\bar{y}_{2}$), is obtained in two ways depending on whether it is plausible or not to make the assumption that the variances within the populations are the same, (i.e. $\sigma _{1}^{2} =\sigma _{2}^{2}$). Whether this assumption appears to be tenable or not can be explored using boxplots etc. For the television viewing time example, the variances of the TV viewing times do not appear to be the same for boys and girls. If the variances of the two populations are the same, then we will use a method of combining the individual variances of the groups to form a **pooled variance** estimate. To do this, we cannot simply average the two variances as the sample sizes may be quite different. A weighted sum is called for to give:</span>
<span id="cb68-542"><a href="#cb68-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-543"><a href="#cb68-543" aria-hidden="true" tabindex="-1"></a>pooled estimate of variance, $S_{p}^{2} =w_{1} S_{1}^{2} +w_{2} S_{2}^{2}$</span>
<span id="cb68-544"><a href="#cb68-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-545"><a href="#cb68-545" aria-hidden="true" tabindex="-1"></a>where the **weights** are, $w_{1} =\frac{n_{1}-1}{n_{1} +n_{2}-2}$ and $w_{2} =\frac{n_{2}-1}{n_{1} +n_{2}-2}$. Hence the pooled estimate of the *standard deviation* is given by</span>
<span id="cb68-546"><a href="#cb68-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-547"><a href="#cb68-547" aria-hidden="true" tabindex="-1"></a>$S_p = \sqrt{ w_1S_{1}^2 + w_2S_{2}^2 }$ or</span>
<span id="cb68-548"><a href="#cb68-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-549"><a href="#cb68-549" aria-hidden="true" tabindex="-1"></a>$$S_{p} =\sqrt{\frac{\left(n_{1}-1\right)S_{1}^{2} +\left(n_{2}-1\right)S_{2}^{2} }{n_{1} +n_{2} -2} }$$ Consequently,</span>
<span id="cb68-550"><a href="#cb68-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-551"><a href="#cb68-551" aria-hidden="true" tabindex="-1"></a>Estimated standard error ($\bar{y}_{i}$)=$\frac{S_{p}}{\sqrt{n_{i}}},~~~i = 1, 2$</span>
<span id="cb68-552"><a href="#cb68-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-553"><a href="#cb68-553" aria-hidden="true" tabindex="-1"></a>so that</span>
<span id="cb68-554"><a href="#cb68-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-555"><a href="#cb68-555" aria-hidden="true" tabindex="-1"></a>Estimated standard error ($\bar{y}_{1}-\bar{y}_{2}$)=$S_{p} \sqrt{1/n_{1}+1/n_{2}}$</span>
<span id="cb68-556"><a href="#cb68-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-557"><a href="#cb68-557" aria-hidden="true" tabindex="-1"></a>If the variances of the two populations are **not** the same, then we cannot pool the variances. Hence the estimated standard error for the difference in the two sample means is given by</span>
<span id="cb68-558"><a href="#cb68-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-559"><a href="#cb68-559" aria-hidden="true" tabindex="-1"></a>Estimated standard error ($\bar{y}_{1}-\bar{y}_{2}$)=$\sqrt{\frac{S_{1}^{2}}{n_{1}} +\frac{S_{2}^{2}}{n_{2}}}$</span>
<span id="cb68-560"><a href="#cb68-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-561"><a href="#cb68-561" aria-hidden="true" tabindex="-1"></a>The degrees of freedom for our $t$-test (called the **two-sample** $t$ test) depends on whether estimated standard error is based on the pooled variance or not. For the variance pooled case, the $df$ for the $t$-test is $n_{1}+n_{2}-2$ but becomes smaller for the unpooled case to $$df=\frac{\left(\frac{S_{1}^{2}}{n_{1}} +\frac{S_{2}^{2} }{n_{2}} \right)^{2} }{\frac{1}{n_{1} -1} \left(\frac{S_{1}^{2}}{n_{1}}\right)^{2} +\frac{1}{n_{2} -1} \left(\frac{S_{2}^{2}}{n_{2} } \right)^{2}}$$ That is, the $df$ is adjusted according to the ratio of the two variances. The $t$-test is also approximate when the variances are not pooled and hence it would be advisable to perform a transformation as later outlined in this Chapter.</span>
<span id="cb68-562"><a href="#cb68-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-563"><a href="#cb68-563" aria-hidden="true" tabindex="-1"></a>Hand calculations for the two sample $t$-test are cumbersome. The test done on computer usually gives us an output which contains the $t$-statistic value and the associated $p$-value. Supplementary details such as the standard errors of the sample means, and their difference, associated $df$ etc will also be contained. The <span class="in">`R`</span> output given below shows the two-sample test results for the TV viewing data set. Here we test whether the true mean TV viewing times are the same for boys and girls.</span>
<span id="cb68-564"><a href="#cb68-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-567"><a href="#cb68-567" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-568"><a href="#cb68-568" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(TELETIME<span class="sc">~</span>SEX, <span class="at">data=</span>tv)</span>
<span id="cb68-569"><a href="#cb68-569" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-570"><a href="#cb68-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-571"><a href="#cb68-571" aria-hidden="true" tabindex="-1"></a>Based on the EDA evidence seen in @fig-tvdistsex, we may take a conservative stand and prefer the unpooled two-sample $t$-test (which is also known as Welch Two Sample t-test). The $t$-value of -0.72 is not unusual as the probability of getting such an extreme value under the null hypothesis is $p=0.47$. In other words, we cannot reject the null hypothesis; we accept it until we have more evidence to the contrary. Hence the conclusion of the $t$-test is that the mean TV viewing times can be regarded as the same for the population of boys and girls. Alternatively there is no statistically significant gender effect on TV watching for boys and girls of Standards 2 to 4.</span>
<span id="cb68-572"><a href="#cb68-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-573"><a href="#cb68-573" aria-hidden="true" tabindex="-1"></a><span class="fu">## Confidence Intervals for the Difference in Means</span></span>
<span id="cb68-574"><a href="#cb68-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-575"><a href="#cb68-575" aria-hidden="true" tabindex="-1"></a>The 95% Confidence Interval for the difference $\left(\mu _{1} -\mu _{2} \right)$ in population means is given by:</span>
<span id="cb68-576"><a href="#cb68-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-577"><a href="#cb68-577" aria-hidden="true" tabindex="-1"></a>difference in sample means $\pm t \times$ e.s.e .</span>
<span id="cb68-578"><a href="#cb68-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-579"><a href="#cb68-579" aria-hidden="true" tabindex="-1"></a>Or more specifically,</span>
<span id="cb68-580"><a href="#cb68-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-581"><a href="#cb68-581" aria-hidden="true" tabindex="-1"></a>Interval estimate for $(\mu _1-\mu _2)$ = $(\bar{y}_{1}-\bar{y}_{2})\pm t \times$ e.s.e.$(\bar{y}_{1} -\bar{y}_{2})$.</span>
<span id="cb68-582"><a href="#cb68-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-583"><a href="#cb68-583" aria-hidden="true" tabindex="-1"></a>Based on the $t$ quantile value of 2.021 for 40 $df$, the CI in the unpooled case is</span>
<span id="cb68-584"><a href="#cb68-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-585"><a href="#cb68-585" aria-hidden="true" tabindex="-1"></a>$-122 \pm2.021\times\sqrt{\frac{648^{2}}{23}+\frac{482^{2}}{23}}$ or $(-462.3, 218.2)$</span>
<span id="cb68-586"><a href="#cb68-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-587"><a href="#cb68-587" aria-hidden="true" tabindex="-1"></a>The $t$-test output for the null hypothesis $H_{0}:\mu _1=\mu _2$ (or $(\mu _1-\mu _2)= 0)$ gives the confidence interval too. Notice that the CI actually includes zero as a possible value. This means that it is possible $(\mu_1-\mu_2)=0$; so we cannot reject the null hypothesis.</span>
<span id="cb68-588"><a href="#cb68-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-589"><a href="#cb68-589" aria-hidden="true" tabindex="-1"></a><span class="fu">## Paired $t$ test</span></span>
<span id="cb68-590"><a href="#cb68-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-591"><a href="#cb68-591" aria-hidden="true" tabindex="-1"></a>Note that the two-sample data may be simply paired observations. For instance, a measurement may be made on the left and right eyes of the same person. If observations are paired in some way, a one-sample $t$-test on the difference $\left(X_{i} ,Y_{i} \right)$ will suggest whether the true mean of the differences can be regarded as zero or not. Such a test will be more powerful than a two sample $t$-test because of the correlation between the paired observations. If the correlation is weak, it is desirable to ignore the pairing variable and perform a two-sample $t$-test.</span>
<span id="cb68-592"><a href="#cb68-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-593"><a href="#cb68-593" aria-hidden="true" tabindex="-1"></a>Consider the maths and English test scores of students available in the data set **testmarks**. These test scores are paired being the scores of the same student. The correlation or linear relationship between the maths and English scores is high; see @fig-pairg.</span>
<span id="cb68-594"><a href="#cb68-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-597"><a href="#cb68-597" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-598"><a href="#cb68-598" aria-hidden="true" tabindex="-1"></a><span class="fu">download.file</span>(</span>
<span id="cb68-599"><a href="#cb68-599" aria-hidden="true" tabindex="-1"></a>  <span class="at">url =</span> <span class="st">"http://www.massey.ac.nz/~anhsmith/data/testmarks.RData"</span>,</span>
<span id="cb68-600"><a href="#cb68-600" aria-hidden="true" tabindex="-1"></a>  <span class="at">destfile =</span> <span class="st">"testmarks.RData"</span>)</span>
<span id="cb68-601"><a href="#cb68-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-602"><a href="#cb68-602" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"testmarks.RData"</span>)</span>
<span id="cb68-603"><a href="#cb68-603" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-606"><a href="#cb68-606" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-607"><a href="#cb68-607" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-608"><a href="#cb68-608" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb68-609"><a href="#cb68-609" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pairg</span></span>
<span id="cb68-610"><a href="#cb68-610" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Relationship between Maths and English scores'</span></span>
<span id="cb68-611"><a href="#cb68-611" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb68-612"><a href="#cb68-612" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(testmarks)</span>
<span id="cb68-613"><a href="#cb68-613" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-614"><a href="#cb68-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-615"><a href="#cb68-615" aria-hidden="true" tabindex="-1"></a>The paired $t$-test or the one-sample $t$-test on the difference in scores gives a $t$-statistic of 0.17 ($p$-value of 0.868). This means that the true average difference in test scores can be regarded as zero or alternatively the true mean scores of maths and English can be regarded as equal.</span>
<span id="cb68-616"><a href="#cb68-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-619"><a href="#cb68-619" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-620"><a href="#cb68-620" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(testmarks<span class="sc">$</span>Maths, testmarks<span class="sc">$</span>English, <span class="at">paired=</span>T)</span>
<span id="cb68-621"><a href="#cb68-621" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-622"><a href="#cb68-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-623"><a href="#cb68-623" aria-hidden="true" tabindex="-1"></a><span class="fu"># Transformations</span></span>
<span id="cb68-624"><a href="#cb68-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-625"><a href="#cb68-625" aria-hidden="true" tabindex="-1"></a>A **transformation** is a function that is applied to each observation in a data set to make the distribution of the data symmetric (or even normal if possible). There are several reasons for this:</span>
<span id="cb68-626"><a href="#cb68-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-627"><a href="#cb68-627" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>To make comparisons between two or more groups of data easier. Different samples may differ in a number of ways; different medians, spreads and even the shape of the distributions. The shape of a graph is actually quite hard to describe in words, let alone to compare numerically. But if we can transform the distributions to each have a similar shape (and especially to make them symmetric), then we can summarise all the remaining differences by numbers: medians, IQR, etc. This simplifies comparisons enormously.</span>
<span id="cb68-628"><a href="#cb68-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-629"><a href="#cb68-629" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>It may enable us to describe the data by a simple model; for example the normal distribution. We can then use the model to: compute probabilities, such as the probability an observation will exceed a certain value; simulate new data, to help us predict what may happen in the future; compute confidence intervals for parameters; and for comparative statistical inference, for example computing $p$-values for differences in group means.</span>
<span id="cb68-630"><a href="#cb68-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-631"><a href="#cb68-631" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>To make the variances of groups of data nearly equal. This is needed in the fitting of certain statistical models.</span>
<span id="cb68-632"><a href="#cb68-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-633"><a href="#cb68-633" aria-hidden="true" tabindex="-1"></a>Of course for much statistical inference we don't need the data to be normal itself, but can rely on the central limit theorem. But results based on the CLT will be more dependable if the data are at least symmetric, or can be made symmetric, as we shall see in an example. Thus in this chapter we shall be mainly concerned with transforming data to symmetry (with perhaps a secret longing that the data will be almost normally distributed).</span>
<span id="cb68-634"><a href="#cb68-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-635"><a href="#cb68-635" aria-hidden="true" tabindex="-1"></a><span class="fu">## Transformation and shape</span></span>
<span id="cb68-636"><a href="#cb68-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-637"><a href="#cb68-637" aria-hidden="true" tabindex="-1"></a>When considering data, we may decide to use the measurements as they are, or we may rescale them. For example, we may change them to percentages of the total. As a simple example, consider a town with four stores in which the weekly turnovers are one, two, four and eight thousand dollars. These could be rescaled to percentages of the total (which is 15).</span>
<span id="cb68-638"><a href="#cb68-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-639"><a href="#cb68-639" aria-hidden="true" tabindex="-1"></a>|           |                         |                           |                          |                          |</span>
<span id="cb68-640"><a href="#cb68-640" aria-hidden="true" tabindex="-1"></a>|-----------|-------------------------|---------------------------|--------------------------|--------------------------|</span>
<span id="cb68-641"><a href="#cb68-641" aria-hidden="true" tabindex="-1"></a>| Raw data  | 1                       | 2                         | 4                        | 8                        |</span>
<span id="cb68-642"><a href="#cb68-642" aria-hidden="true" tabindex="-1"></a>| Data in % | 1/15 $\times$ 100= 6.7% | 2/15 $\times$ 100 = 13.3% | 4/15 $\times$ 100= 26.7% |  8/15 $\times$ 100= 53.3%|</span>
<span id="cb68-643"><a href="#cb68-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-644"><a href="#cb68-644" aria-hidden="true" tabindex="-1"></a>If you were to compare a dotplot of the original data with a dotplot of the rescaled data you would find that the **shape** of the data had **not changed** by this rescaling. That is, the second percent is twice the first and the second weekly turnover is almost twice the first; the third percent is four times the first and twice the second and so on. This is an example of a **linear** transformation.</span>
<span id="cb68-645"><a href="#cb68-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-646"><a href="#cb68-646" aria-hidden="true" tabindex="-1"></a>A linear transformation is one that can be described by the formula, $y=a+bx$ for certain constants $a$ and $b$, and where $x$ is the old data and $y$ is the transformed data. The key thing about linear transformations is that they do not change the shape of a dotplot, only the scale. Another linear example is converting temperature data from Fahrenheit ($x$) to centigrade $y = 5(x-32)/9$. Boxplots of the temperatures would *look* the same even though the scale was altered.</span>
<span id="cb68-647"><a href="#cb68-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-648"><a href="#cb68-648" aria-hidden="true" tabindex="-1"></a>Another way of rescaling the store example would be to calculate the weekly turnover of a store divided by the number of employees in that store, to give weekly turnover per employee. Even though this looks like a linear transformation it is not, since the relative positions of the four stores on a scale would change depending on the number of employees. If we have two or more variables (e.g. turnover, employees) it is often useful to look at ratios like this to seek simple explanations of the data.</span>
<span id="cb68-649"><a href="#cb68-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-652"><a href="#cb68-652" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-653"><a href="#cb68-653" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-654"><a href="#cb68-654" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-transf</span></span>
<span id="cb68-655"><a href="#cb68-655" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Transformations'</span></span>
<span id="cb68-656"><a href="#cb68-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-657"><a href="#cb68-657" aria-hidden="true" tabindex="-1"></a>rht <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">RHT=</span><span class="fu">rbeta</span>(<span class="fl">1e3</span>, <span class="dv">1</span>,<span class="dv">5</span>))</span>
<span id="cb68-658"><a href="#cb68-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-659"><a href="#cb68-659" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rht) <span class="sc">+</span> </span>
<span id="cb68-660"><a href="#cb68-660" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(RHT) <span class="sc">+</span></span>
<span id="cb68-661"><a href="#cb68-661" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb68-662"><a href="#cb68-662" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb68-663"><a href="#cb68-663" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"(a) Needs a Shrinking Transformation"</span>)</span>
<span id="cb68-664"><a href="#cb68-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-665"><a href="#cb68-665" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rht) <span class="sc">+</span></span>
<span id="cb68-666"><a href="#cb68-666" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>RHT, <span class="at">x=</span><span class="st">"Right Skewed Data"</span>) <span class="sc">+</span></span>
<span id="cb68-667"><a href="#cb68-667" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb68-668"><a href="#cb68-668" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb68-669"><a href="#cb68-669" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span>
<span id="cb68-670"><a href="#cb68-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-671"><a href="#cb68-671" aria-hidden="true" tabindex="-1"></a>lht <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">LHT=</span><span class="fu">rbeta</span>(<span class="fl">1e3</span>, <span class="dv">5</span>,<span class="dv">1</span>))</span>
<span id="cb68-672"><a href="#cb68-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-673"><a href="#cb68-673" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(lht) <span class="sc">+</span></span>
<span id="cb68-674"><a href="#cb68-674" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(LHT) <span class="sc">+</span></span>
<span id="cb68-675"><a href="#cb68-675" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span></span>
<span id="cb68-676"><a href="#cb68-676" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb68-677"><a href="#cb68-677" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"(a) Needs a Stretching Transformation"</span>)</span>
<span id="cb68-678"><a href="#cb68-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-679"><a href="#cb68-679" aria-hidden="true" tabindex="-1"></a>p4 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(lht) <span class="sc">+</span></span>
<span id="cb68-680"><a href="#cb68-680" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>LHT, <span class="at">x=</span><span class="st">"Left Skewed Data"</span>) <span class="sc">+</span></span>
<span id="cb68-681"><a href="#cb68-681" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb68-682"><a href="#cb68-682" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb68-683"><a href="#cb68-683" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span>
<span id="cb68-684"><a href="#cb68-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-685"><a href="#cb68-685" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1, p3, p2, p4, <span class="at">ncol=</span><span class="dv">2</span>) </span>
<span id="cb68-686"><a href="#cb68-686" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-687"><a href="#cb68-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-688"><a href="#cb68-688" aria-hidden="true" tabindex="-1"></a>In this chapter, we focus on transformations involving just one variable (though perhaps more than one batch of data on that variable). **Our goal is to change the shape of a distribution** to make it more symmetric. Consider the two distributions in @fig-transf in which (a) is skewed to the left and (b) to the right. These could be made more symmetric by stretching the large values in (a) but shrinking them in (b).</span>
<span id="cb68-689"><a href="#cb68-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-690"><a href="#cb68-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-691"><a href="#cb68-691" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Ladder of Powers</span></span>
<span id="cb68-692"><a href="#cb68-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-693"><a href="#cb68-693" aria-hidden="true" tabindex="-1"></a>From now on we assume that the data are very skewed, and we wish to transform it, (or, in Tukey's terms, re-express it) to be as symmetrical as possible. A simple approach considers the data raised to different powers, that is, if the original data are $x$ and the new data are $y$,</span>
<span id="cb68-694"><a href="#cb68-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-695"><a href="#cb68-695" aria-hidden="true" tabindex="-1"></a>$$y =\left<span class="sc">\{</span>\begin{array}{l} {</span>
<span id="cb68-696"><a href="#cb68-696" aria-hidden="true" tabindex="-1"></a>\text {sign} (\lambda )x^\lambda~~~~~ \lambda \ne 0} <span class="sc">\\</span></span>
<span id="cb68-697"><a href="#cb68-697" aria-hidden="true" tabindex="-1"></a>{\log(x)~~~~~~~~~~~\lambda = 0} \end{array} \right.$$</span>
<span id="cb68-698"><a href="#cb68-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-699"><a href="#cb68-699" aria-hidden="true" tabindex="-1"></a>Note that the Greek letter $\lambda$ is pronounced as lambda. Here, ($\text {sign}(\lambda)$) is +1 if $\lambda&gt;0$, and $\text {sign}(\lambda)=-1$ if $\lambda&lt;0$, for reasons discussed below. Some special cases of this **power transformation** are set out below:</span>
<span id="cb68-700"><a href="#cb68-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-701"><a href="#cb68-701" aria-hidden="true" tabindex="-1"></a>| POWER | Formula               | Name            | Result                 |</span>
<span id="cb68-702"><a href="#cb68-702" aria-hidden="true" tabindex="-1"></a>|:------|:----------------------|:----------------|:-----------------------|</span>
<span id="cb68-703"><a href="#cb68-703" aria-hidden="true" tabindex="-1"></a>| 3     | $x^3$                 | cube            | stretches large values |</span>
<span id="cb68-704"><a href="#cb68-704" aria-hidden="true" tabindex="-1"></a>| 2     | $x^2$                 | square          | stretches large values |</span>
<span id="cb68-705"><a href="#cb68-705" aria-hidden="true" tabindex="-1"></a>| 1     | $x$                   | raw             | No change              |</span>
<span id="cb68-706"><a href="#cb68-706" aria-hidden="true" tabindex="-1"></a>| 1/2   | $\sqrt{x}$            | square root     | squashes large values  |</span>
<span id="cb68-707"><a href="#cb68-707" aria-hidden="true" tabindex="-1"></a>| 0     | $\log{x}$             | logarithm       | squashes large values  |</span>
<span id="cb68-708"><a href="#cb68-708" aria-hidden="true" tabindex="-1"></a>| -1/2  | $\frac{-1}{\sqrt{x}}$ | reciprocal root | squashes large values  |</span>
<span id="cb68-709"><a href="#cb68-709" aria-hidden="true" tabindex="-1"></a>| -1    | $\frac{-1}{x}$        | reciprocal      | squashes large values  |</span>
<span id="cb68-710"><a href="#cb68-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-711"><a href="#cb68-711" aria-hidden="true" tabindex="-1"></a>Raising the data to the power of 1 does not change it at all; as we proceed down or up from 1, the strength of the transformation increases. The special case $\lambda$=0 has to be handled differently since $x^0=1$ for all non-zero $x$. Instead we conventionally regard it as being equivalent to taking the natural logarithm because the transformation $\frac{x^{\lambda } }{\lambda } -\frac{1}{\lambda }$ is close to the logarithmic transformation if $\lambda$ is small. The 'common' logarithm to base 10 could be used but it just yields a constant multiple of the natural logarithm (<span class="in">`ln`</span> = log to the base $e$). Now regarding the $\text {sign}(\lambda)$: Notice that with two numbers, say 2 and 5, the reciprocal transformation would yield 0.5 and 0.2 so that, whereas the original numbers are increasing in size the transformed values are decreasing. To keep the order the same we take the negative of the reciprocal values, -0.5 and -0.2. These are again increasing. The same principle holds for all transformations where $\lambda$ is negative. $\text {sign}(\lambda)$ is employed to keep the order the same as the raw data. (Alternatively we could divide by $\lambda$ which is consistent with the case of power zero that is the logarithm transformation).</span>
<span id="cb68-712"><a href="#cb68-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-715"><a href="#cb68-715" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-716"><a href="#cb68-716" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-717"><a href="#cb68-717" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rangitransf</span></span>
<span id="cb68-718"><a href="#cb68-718" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Effect of Transformations'</span></span>
<span id="cb68-719"><a href="#cb68-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-720"><a href="#cb68-720" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span></span>
<span id="cb68-721"><a href="#cb68-721" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>vehicle<span class="sc">^</span><span class="dv">2</span>, <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb68-722"><a href="#cb68-722" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb68-723"><a href="#cb68-723" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb68-724"><a href="#cb68-724" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb68-725"><a href="#cb68-725" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Square Transformation"</span>)</span>
<span id="cb68-726"><a href="#cb68-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-727"><a href="#cb68-727" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span></span>
<span id="cb68-728"><a href="#cb68-728" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>vehicle, <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb68-729"><a href="#cb68-729" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb68-730"><a href="#cb68-730" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb68-731"><a href="#cb68-731" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span> </span>
<span id="cb68-732"><a href="#cb68-732" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Raw Data"</span>)</span>
<span id="cb68-733"><a href="#cb68-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-734"><a href="#cb68-734" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span></span>
<span id="cb68-735"><a href="#cb68-735" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>vehicle<span class="sc">^</span>.<span class="dv">5</span>, <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb68-736"><a href="#cb68-736" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb68-737"><a href="#cb68-737" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb68-738"><a href="#cb68-738" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span> </span>
<span id="cb68-739"><a href="#cb68-739" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Square-root Transformation"</span>)</span>
<span id="cb68-740"><a href="#cb68-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-741"><a href="#cb68-741" aria-hidden="true" tabindex="-1"></a>p4 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(rangitikei) <span class="sc">+</span> </span>
<span id="cb68-742"><a href="#cb68-742" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span><span class="fu">log</span>(vehicle), <span class="at">x=</span><span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb68-743"><a href="#cb68-743" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb68-744"><a href="#cb68-744" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> </span>
<span id="cb68-745"><a href="#cb68-745" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb68-746"><a href="#cb68-746" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"log Transformation"</span>)</span>
<span id="cb68-747"><a href="#cb68-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-748"><a href="#cb68-748" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1,p3,p2, p4, <span class="at">ncol=</span><span class="dv">2</span>) </span>
<span id="cb68-749"><a href="#cb68-749" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-750"><a href="#cb68-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-751"><a href="#cb68-751" aria-hidden="true" tabindex="-1"></a>As an example, the boxplots in @fig-rangitransf represent the number of vehicles at the two Rangitikei river locations (from **rangitikei**). The first boxplot shows the square of the number of vehicle, which is highly right-skewed. The second boxplot shows the raw data (vehicle) which are still skewed towards the larger values. So to squash these to make the distribution more symmetric, the ladder of powers suggests we could try a square root transformation, or a stronger one such as the logarithm (or reciprocal root). These are shown in the third and fourth boxplots.</span>
<span id="cb68-752"><a href="#cb68-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-753"><a href="#cb68-753" aria-hidden="true" tabindex="-1"></a><span class="fu">## Checking the adequacy of a transformation</span></span>
<span id="cb68-754"><a href="#cb68-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-755"><a href="#cb68-755" aria-hidden="true" tabindex="-1"></a>Since the purpose of transforming the data is to make it more symmetric, we need to decide if we have achieved this goal. One approach which has been suggested is to consider the difference mean-median where the vertical lines stand for "the absolute value of" which means we ignore a negative sign if it occurs. If the batch is symmetric then the mean equals the median so that the difference mean - median= 0. We could design a measure, D, of symmetry by standardising it. That is, we divide this difference by a measure of spread - we could choose either the standard deviation or the F spread, whichever is at hand. These choices give the so-called **D-Statistics**:</span>
<span id="cb68-756"><a href="#cb68-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-757"><a href="#cb68-757" aria-hidden="true" tabindex="-1"></a>D1 = (mean - median)/ std.dev. or</span>
<span id="cb68-758"><a href="#cb68-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-759"><a href="#cb68-759" aria-hidden="true" tabindex="-1"></a>D2 = (mean - median)/F-spread.</span>
<span id="cb68-760"><a href="#cb68-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-761"><a href="#cb68-761" aria-hidden="true" tabindex="-1"></a>Instead of calculating the mean, we could use the mid-F to give</span>
<span id="cb68-762"><a href="#cb68-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-763"><a href="#cb68-763" aria-hidden="true" tabindex="-1"></a>D3 = (mid-F - median)/F-spread.</span>
<span id="cb68-764"><a href="#cb68-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-765"><a href="#cb68-765" aria-hidden="true" tabindex="-1"></a>We first choose one of these criteria, and then apply it to all the transformations in turn, choosing as 'best' that transformation which gives the smallest absolute value of D (provided that other considerations do not disqualify it). Note that, in practice it is useful *not* to take absolute values when calculating the D-statistics, since a change of sign helps us to locate the zero of D, i.e. the direction of skewness of the data.</span>
<span id="cb68-766"><a href="#cb68-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-767"><a href="#cb68-767" aria-hidden="true" tabindex="-1"></a>The following table gives the values of D1, D2, and D3 (with sign retained) for the four transformations.</span>
<span id="cb68-768"><a href="#cb68-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-769"><a href="#cb68-769" aria-hidden="true" tabindex="-1"></a>| D-statistic | vehicle | vehicle$^2$ | $\sqrt{vehicle}$ | loge(vehicle) |</span>
<span id="cb68-770"><a href="#cb68-770" aria-hidden="true" tabindex="-1"></a>|:------------|:--------|:------------|:-----------------|:--------------|</span>
<span id="cb68-771"><a href="#cb68-771" aria-hidden="true" tabindex="-1"></a>| D$_{1}$     | 0.2724  | 0.2831      | 0.1308           | -0.0945       |</span>
<span id="cb68-772"><a href="#cb68-772" aria-hidden="true" tabindex="-1"></a>| D$_{2}$     | 0.2973  | 1.0615      | 0.0978           | -0.0611       |</span>
<span id="cb68-773"><a href="#cb68-773" aria-hidden="true" tabindex="-1"></a>| D$_{3}$     | 0.0714  | 0.2273      | -0.0183          | -0.1092       |</span>
<span id="cb68-774"><a href="#cb68-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-777"><a href="#cb68-777" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-778"><a href="#cb68-778" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-779"><a href="#cb68-779" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "fig-rangilval"</span></span>
<span id="cb68-780"><a href="#cb68-780" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Letter values of Vechicle data"</span></span>
<span id="cb68-781"><a href="#cb68-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-782"><a href="#cb68-782" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"http://www.massey.ac.nz/~anhsmith/161250/eda/lval.R"</span>)</span>
<span id="cb68-783"><a href="#cb68-783" aria-hidden="true" tabindex="-1"></a><span class="fu">lval</span>(<span class="fu">log</span>(rangitikei<span class="sc">$</span>vehicle))</span>
<span id="cb68-784"><a href="#cb68-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-785"><a href="#cb68-785" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-786"><a href="#cb68-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-787"><a href="#cb68-787" aria-hidden="true" tabindex="-1"></a>The D-statistics D1 and D2 presented above suggest that the natural log would be the best transformation. But D3 values suggest in favour of the square root transformation. Note that D3 is sensitive only to skewness in the middle 50% of the data, whereas D1 and D2 are sensitive to skewness in the whole data set. The boxplots of @fig-rangitransf would tend to confirm these observations on the D-statistics. Note also that $\text {vehicle} ^2$, $\text {vehicle}$ and $\sqrt{\text {vehicle}}$ have produced positive values for the D-statistics indicating that these variables are skewed to the right (i.e. positive skewness) while $\text {loge(vehicle)}$ data are mildly negatively skewed.</span>
<span id="cb68-788"><a href="#cb68-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-789"><a href="#cb68-789" aria-hidden="true" tabindex="-1"></a>Having chosen the transformation it is a good idea to check it graphically using a **mids vs. spreads plot**. The plot in @fig-rangilval shows a consistent upward trend in the untransformed (raw) data (right-skewed). Again a general upward trend is seen for squared data indicating that the skewness is not fully corrected. The square root data still shows the positive trend, which means that the transformation has failed to correct the skew in the non-middle 50% of the data. For logged data, the mids vs. spread plot shows a random scatter of points. This confirms the conclusion from the D statistic that the log transformation works well to correct the skew in the non-middle part of the data. Note that we may also compute the D-Statistics using the IQR in place of F-spread and the results will often be similar.</span>
<span id="cb68-790"><a href="#cb68-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-791"><a href="#cb68-791" aria-hidden="true" tabindex="-1"></a><span class="fu">## Some Words of Caution About Transformations</span></span>
<span id="cb68-792"><a href="#cb68-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-793"><a href="#cb68-793" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>The D values can be compared for the same data set but we should be wary of comparing the D value from one data set with that of another (possibly transformed) data set.</span>
<span id="cb68-794"><a href="#cb68-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-795"><a href="#cb68-795" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Any one statistic can never capture all the peculiarities of a data set so that the D values should be used in conjunction with other measures and/or graphs.</span>
<span id="cb68-796"><a href="#cb68-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-797"><a href="#cb68-797" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>If the data set is small, transformations should be approached with some scepticism, for if more data were available the shape of the distribution may change.</span>
<span id="cb68-798"><a href="#cb68-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-799"><a href="#cb68-799" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>It should be kept in mind that there are different levels at which data can be considered such as (a) just a collection of numbers (any reasonable transformation would suffice); (b) referring to physical quantities (certain transformations may make physical sense and allow meaningful interpretations to be made) and (c) outcomes from a certain process (for example, frequencies or counts may often suggest a Poisson distribution for which a square root transformation is suitable). The choice of a transformation may depend on the additional information that is known about the batch of data.</span>
<span id="cb68-800"><a href="#cb68-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-801"><a href="#cb68-801" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Common sense should prevail in this area as a transformation which brings only marginal improvement to symmetry, for example, should be balanced against other drawbacks such as the difficulty of interpreting the results. For example, the logarithm function turns multiplications to additions and powers to multiplications. Hence it turns divisions to subtractions and roots to divisions. Thus the geometric mean becomes the arithmetic mean and the ratio of geometric means becomes the difference between arithmetic means. Therefore reversing the transformation implies that a confidence interval for the difference between the means of the transformed data becomes a confidence interval for the ratio of the geometric means of the two groups of raw data.</span>
<span id="cb68-802"><a href="#cb68-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-803"><a href="#cb68-803" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>Although a transformation may lead to symmetry it may be better to consider other approaches. For example, the stem-and-leaf or dotplot display may show that there are at least two subgroups in the data. Transformations do not really solve the problem and we may have to subdivide the data into groups.</span>
<span id="cb68-804"><a href="#cb68-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-805"><a href="#cb68-805" aria-hidden="true" tabindex="-1"></a><span class="ss">7.  </span>Note that there are several other transformation functions available. For example, transformations such as arcsine are useful for proportion data.</span>
<span id="cb68-806"><a href="#cb68-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-807"><a href="#cb68-807" aria-hidden="true" tabindex="-1"></a><span class="fu">## Box-Cox Normalising transformations</span></span>
<span id="cb68-808"><a href="#cb68-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-809"><a href="#cb68-809" aria-hidden="true" tabindex="-1"></a>A systematic approach to power transformations was developed by @boxcox and @boxcox2. Their method produces a log-likelihood curve of possible values for the power $\lambda$. Without going into details, the higher the curve is for a particular value of $\lambda$, the more normal the transformed data will be. The plot of the log-likelihood curve of the Box-Cox method applied to the vehicle data is shown in @fig-rangibox.</span>
<span id="cb68-810"><a href="#cb68-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-813"><a href="#cb68-813" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-814"><a href="#cb68-814" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-815"><a href="#cb68-815" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rangibox</span></span>
<span id="cb68-816"><a href="#cb68-816" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Box-Cox Transformation'</span></span>
<span id="cb68-817"><a href="#cb68-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-818"><a href="#cb68-818" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS, <span class="at">exclude =</span> <span class="st">'select'</span>)</span>
<span id="cb68-819"><a href="#cb68-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-820"><a href="#cb68-820" aria-hidden="true" tabindex="-1"></a><span class="fu">boxcox</span>(rangitikei<span class="sc">$</span>vehicle <span class="sc">~</span> <span class="dv">1</span>)</span>
<span id="cb68-821"><a href="#cb68-821" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">"Log-likelihood curve of Box-Cox power parameter"</span>)</span>
<span id="cb68-822"><a href="#cb68-822" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-823"><a href="#cb68-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-824"><a href="#cb68-824" aria-hidden="true" tabindex="-1"></a>The curve peaks near zero indicating that a log transformation would be appropriate. In addition to the curve the plot contains a 95% confidence interval for the transformation parameter $\lambda$. The two vertical dotted lines are the endpoints of the confidence interval for $\lambda$. In this case the width of the confidence interval is quite small. The wider the confidence interval, the less obvious the choice for $\lambda$. If the confidence interval contains 1, then there is no need to perform a transformation. Note that the Box-Cox transformation is a normalising transformation. The EDA done in the previous sections relate to symmetry in the data.</span>
<span id="cb68-825"><a href="#cb68-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-826"><a href="#cb68-826" aria-hidden="true" tabindex="-1"></a>The Box-Cox method, and other power transformations, should only be applied if the data are strictly positive. If all the data are negative one can of course take the absolute value of the data and then apply the transformation. If, however, only some of the data are not positive, we may add a constant to all of the data (to make the data positive) but the estimated power will vary depending on the constant.</span>
<span id="cb68-827"><a href="#cb68-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-828"><a href="#cb68-828" aria-hidden="true" tabindex="-1"></a><span class="fu"># Transformations for Inference</span></span>
<span id="cb68-829"><a href="#cb68-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-830"><a href="#cb68-830" aria-hidden="true" tabindex="-1"></a>If we have a sample of data of size 30 or more from a homogeneous population preferably symmetrically distributed, the central limit theorem allows us to calculate a confidence interval for the population mean, $\mu$. If the population is skewed, then we will need a larger sample to find the "correct" confidence interval for $\mu$; the greater the skewness, the larger the sample size needed. However, when the population is skewed, even if we have a large enough sample size to obtain the "correct" confidence interval for $\mu$, we must ask ourselves just how useful the population mean is as a description of the centre (or location) of the population. As an example, consider a skewed theoretical distribution known as the *lognormal* distribution. This distribution is related to the normal distribution. If a variable follows the lognormal distribution, then the log of the variable follows the normal distribution. @fig-lnorml shows the density curve for the log normal distribution with $\mu=0$ and $\sigma=1$.</span>
<span id="cb68-831"><a href="#cb68-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-832"><a href="#cb68-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-835"><a href="#cb68-835" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-836"><a href="#cb68-836" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-837"><a href="#cb68-837" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lnorml</span></span>
<span id="cb68-838"><a href="#cb68-838" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Log-normal Density Curve'</span></span>
<span id="cb68-839"><a href="#cb68-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-840"><a href="#cb68-840" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb68-841"><a href="#cb68-841" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb68-842"><a href="#cb68-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-843"><a href="#cb68-843" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dlnorm</span>(x, mu, sigma, <span class="at">log =</span> <span class="cn">FALSE</span>),   <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>),</span>
<span id="cb68-844"><a href="#cb68-844" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab=</span><span class="st">"Quantile"</span>,</span>
<span id="cb68-845"><a href="#cb68-845" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">"log-normal density"</span>)</span>
<span id="cb68-846"><a href="#cb68-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-847"><a href="#cb68-847" aria-hidden="true" tabindex="-1"></a>mean <span class="ot">=</span> <span class="fu">exp</span>(mu<span class="sc">+</span>(sigma<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb68-848"><a href="#cb68-848" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mean, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb68-849"><a href="#cb68-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-850"><a href="#cb68-850" aria-hidden="true" tabindex="-1"></a>median <span class="ot">=</span> <span class="fu">qlnorm</span>(<span class="fl">0.5</span>, <span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb68-851"><a href="#cb68-851" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>median, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb68-852"><a href="#cb68-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-853"><a href="#cb68-853" aria-hidden="true" tabindex="-1"></a>mode <span class="ot">=</span> <span class="fu">exp</span>(mu)<span class="sc">/</span><span class="fu">exp</span>(sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb68-854"><a href="#cb68-854" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mode, <span class="at">lty =</span> <span class="dv">3</span>, <span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb68-855"><a href="#cb68-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-856"><a href="#cb68-856" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"mean"</span>, <span class="st">"median"</span>, <span class="st">"mode"</span>),</span>
<span id="cb68-857"><a href="#cb68-857" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="at">col=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>))</span>
<span id="cb68-858"><a href="#cb68-858" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-859"><a href="#cb68-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-860"><a href="#cb68-860" aria-hidden="true" tabindex="-1"></a>Since the lognormal distribution is skewed, the mean, the median, and the mode, are all different, but the mean is the most different. The height of the density curve at the mean is also much lower than it is at the median (by definition the height of the density curve is a maximum at the mode). This means that observations from this distribution are less likely to be close to the mean than the median. The one attribute the mean has is that it represents the centre of mass (or the balancing point) of the distribution. Unfortunately, this is quite often of little practical significance. In practice, when summarising skewed distributions (e.g. house prices, incomes), the median is always preferred to the mean. This leads us to conclude that it would be better to find a confidence interval for the population median, $m$, than the population mean $\mu$. To do this we will use the following procedure:</span>
<span id="cb68-861"><a href="#cb68-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-862"><a href="#cb68-862" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Given a sample of data from the skewed population, transform it so that it is symmetrically distributed (such that the mean and median should be roughly equal).</span>
<span id="cb68-863"><a href="#cb68-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-864"><a href="#cb68-864" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Find a confidence interval for the mean of the transformed data, $\mu$. This interval will also be a good interval for the median of the transformed data, $m$.</span>
<span id="cb68-865"><a href="#cb68-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-866"><a href="#cb68-866" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>"Reverse transform" the confidence interval; i.e. apply the back transformation used on the data.</span>
<span id="cb68-867"><a href="#cb68-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-868"><a href="#cb68-868" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>This new interval will be a confidence interval for the median of the original skewed population.</span>
<span id="cb68-869"><a href="#cb68-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-870"><a href="#cb68-870" aria-hidden="true" tabindex="-1"></a>This procedure works because the median is a *monotonic* function with respect to power transform. For example, consider a data set $X = <span class="sc">\{</span>x_i<span class="sc">\}</span>$ that requires a logarithmic transformation. The median of the transformed data set, $<span class="sc">\{</span>\log(x_i)<span class="sc">\}</span>$, is equal to the logarithm of the median of the untransformed data set, $<span class="sc">\{</span>x_i<span class="sc">\}</span>$. This is not true of the mean. The mean of the transformed data set, $<span class="sc">\{</span>\log(x_i)<span class="sc">\}</span>$, is not equal to the logarithm of the mean of the untransformed data set, $<span class="sc">\{</span>x_i<span class="sc">\}</span>$. If we reverse transform the mean of the log transformed data set, then the reverse transformed value will be the geometric mean of the raw data.</span>
<span id="cb68-871"><a href="#cb68-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-872"><a href="#cb68-872" aria-hidden="true" tabindex="-1"></a>The distribution of vehicles in the **rangitikei** dataset is not symmetrical but very much skewed to the right with a probable outlier. The sample size 33 is only just above 30. It is of interest to compute the confidence interval for the true mean number of vehicles using the raw data (even though the CLT cannot be fully applied here). Using software, we obtain the 95% confidence interval for the mean as 13.1154 $\leq$ $\mu$ $\leq$ 29.3694.</span>
<span id="cb68-873"><a href="#cb68-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-874"><a href="#cb68-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-875"><a href="#cb68-875" aria-hidden="true" tabindex="-1"></a><span class="fu"># Transformations to Constant Variance</span></span>
<span id="cb68-876"><a href="#cb68-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-877"><a href="#cb68-877" aria-hidden="true" tabindex="-1"></a>For one batch of data, it is relatively easy to choose an appropriate transformation for symmetry. With more than one batch of data, life is more complicated. On the one hand the symmetry of each batch could be examined, and various transformations tried quite separately. However it may then be very difficult to compare and interpret the batches. On the other hand, if each batch is skewed in the same direction, we may be able to apply a common transformation so that each batch becomes symmetric, or close to it.</span>
<span id="cb68-878"><a href="#cb68-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-879"><a href="#cb68-879" aria-hidden="true" tabindex="-1"></a>If groups are skewed in different directions (i.e. some positively and some negatively), it would be difficult to choose a transformation. If the batches are small, the skewness may be due to random variation and could be ignored. If the batches are large, it may not be feasible to perform statistical inference if the degree of skewness is quite variable.</span>
<span id="cb68-880"><a href="#cb68-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-881"><a href="#cb68-881" aria-hidden="true" tabindex="-1"></a>With 2 or more groups, we are often interested in comparing the location (or level) of the responses in these groups, the location being measured by the median, mean or similar statistic. In this situation we like to assume that the groups are similar in other aspects such as in the shape of the distribution and, in particular, the spread of the responses. These assumptions simplify and strengthen the hypothesis test. Quite often, the spread (measured (say) by the range) increases as the locations (measured (say) by the median) increases. It is advisable to use a transformation which removes the systematic relationship between spread and location. Confidence interval, and hypothesis tests, of two means assume that the samples come from populations which have the same variance, or spread. We later consider comparisons of more than two means, and standard methods of formally testing hypotheses regarding differences in means require the assumption that the spreads (variances) are constant over the groups. So in this section, we look at how to examine whether the variances are equal and, if not, how transformations of the data can sometimes serve to equalise the variances.</span>
<span id="cb68-882"><a href="#cb68-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-883"><a href="#cb68-883" aria-hidden="true" tabindex="-1"></a>Consider the response variable people in the dataset **rangitikei** for time of the day (<span class="in">`time`</span>) temperature groupings (<span class="in">`temp`</span>). @fig-diabox shows the boxplots of price for the combinations of <span class="in">`time`</span> and <span class="in">`temp`</span> factors. Also compare the position of the medians in the boxplots.</span>
<span id="cb68-884"><a href="#cb68-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-887"><a href="#cb68-887" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-888"><a href="#cb68-888" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-889"><a href="#cb68-889" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-diabox</span></span>
<span id="cb68-890"><a href="#cb68-890" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Boxplots of People vs. temp*time'</span></span>
<span id="cb68-891"><a href="#cb68-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-892"><a href="#cb68-892" aria-hidden="true" tabindex="-1"></a>rangitikei <span class="sc">|&gt;</span> </span>
<span id="cb68-893"><a href="#cb68-893" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">temp =</span> <span class="fu">paste</span>(<span class="st">"Temperature"</span>, temp)) <span class="sc">|&gt;</span></span>
<span id="cb68-894"><a href="#cb68-894" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb68-895"><a href="#cb68-895" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y =</span> people, </span>
<span id="cb68-896"><a href="#cb68-896" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> time,</span>
<span id="cb68-897"><a href="#cb68-897" aria-hidden="true" tabindex="-1"></a>      <span class="at">colour =</span> time</span>
<span id="cb68-898"><a href="#cb68-898" aria-hidden="true" tabindex="-1"></a>      ) <span class="sc">+</span></span>
<span id="cb68-899"><a href="#cb68-899" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb68-900"><a href="#cb68-900" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>temp) <span class="sc">+</span></span>
<span id="cb68-901"><a href="#cb68-901" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"People by time and temperature"</span>)</span>
<span id="cb68-902"><a href="#cb68-902" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-903"><a href="#cb68-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-904"><a href="#cb68-904" aria-hidden="true" tabindex="-1"></a>The assumption of a Normal distribution with constant variance may become crucial for certain confirmatory analysis. Hence the constancy of variance for each of the 24 batches is an important aspect we must look into. The boxplots are not supportive of the constant variance in the number of admissions. The relationship between spread and location issue can be explored using the medians and ranges of these batches.</span>
<span id="cb68-905"><a href="#cb68-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-908"><a href="#cb68-908" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-909"><a href="#cb68-909" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-910"><a href="#cb68-910" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb68-911"><a href="#cb68-911" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-diatrans</span></span>
<span id="cb68-912"><a href="#cb68-912" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Ranges vs. Medians'</span></span>
<span id="cb68-913"><a href="#cb68-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-914"><a href="#cb68-914" aria-hidden="true" tabindex="-1"></a>rangitikei <span class="sc">|&gt;</span> </span>
<span id="cb68-915"><a href="#cb68-915" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(time,temp) <span class="sc">|&gt;</span> </span>
<span id="cb68-916"><a href="#cb68-916" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb68-917"><a href="#cb68-917" aria-hidden="true" tabindex="-1"></a>    <span class="at">medians =</span> <span class="fu">median</span>(people), </span>
<span id="cb68-918"><a href="#cb68-918" aria-hidden="true" tabindex="-1"></a>    <span class="at">ranges =</span> <span class="fu">max</span>(people) <span class="sc">-</span> <span class="fu">min</span>(people)</span>
<span id="cb68-919"><a href="#cb68-919" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">|&gt;</span> </span>
<span id="cb68-920"><a href="#cb68-920" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb68-921"><a href="#cb68-921" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y =</span> ranges, <span class="at">x =</span> medians) <span class="sc">+</span></span>
<span id="cb68-922"><a href="#cb68-922" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span>
<span id="cb68-923"><a href="#cb68-923" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-924"><a href="#cb68-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-925"><a href="#cb68-925" aria-hidden="true" tabindex="-1"></a>In @fig-diatrans the ranges are is plotted against the medians. It is clear that there is a positive trend in the data (larger ranges being associated with larger medians). We might compare this Figure with the ideal situation in which the points would fall in a horizontal band. So we conclude that the variability is not constant across time $\times$ temp combinations.</span>
<span id="cb68-926"><a href="#cb68-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-927"><a href="#cb68-927" aria-hidden="true" tabindex="-1"></a>In the absence of outliers, we can also plot standard deviations against means as shown in @fig-diatrans1.</span>
<span id="cb68-928"><a href="#cb68-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-931"><a href="#cb68-931" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-932"><a href="#cb68-932" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-933"><a href="#cb68-933" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb68-934"><a href="#cb68-934" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-diatrans1</span></span>
<span id="cb68-935"><a href="#cb68-935" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'SDs vs. means'</span></span>
<span id="cb68-936"><a href="#cb68-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-937"><a href="#cb68-937" aria-hidden="true" tabindex="-1"></a>rangitikei <span class="sc">|&gt;</span> </span>
<span id="cb68-938"><a href="#cb68-938" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(time,temp) <span class="sc">|&gt;</span> </span>
<span id="cb68-939"><a href="#cb68-939" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb68-940"><a href="#cb68-940" aria-hidden="true" tabindex="-1"></a>    <span class="at">means =</span> <span class="fu">mean</span>(people), </span>
<span id="cb68-941"><a href="#cb68-941" aria-hidden="true" tabindex="-1"></a>    <span class="at">sds=</span><span class="fu">sd</span>(people)</span>
<span id="cb68-942"><a href="#cb68-942" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">|&gt;</span> </span>
<span id="cb68-943"><a href="#cb68-943" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb68-944"><a href="#cb68-944" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>sds, <span class="at">x=</span>means) <span class="sc">+</span></span>
<span id="cb68-945"><a href="#cb68-945" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span>
<span id="cb68-946"><a href="#cb68-946" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-947"><a href="#cb68-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-948"><a href="#cb68-948" aria-hidden="true" tabindex="-1"></a>A common transformation can be found if ranges (SDs) and medians (means) are related somewhat strongly. For example, the log transformed people data shows a random scatter of ranges vs medians in @fig-diatrans2. This means that some improvement in the constancy of variance for subgroups is achieved by the chosen log transformation.</span>
<span id="cb68-949"><a href="#cb68-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-952"><a href="#cb68-952" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-953"><a href="#cb68-953" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-954"><a href="#cb68-954" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb68-955"><a href="#cb68-955" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-diatrans2</span></span>
<span id="cb68-956"><a href="#cb68-956" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Ranges vs. Medians with log transformed data'</span></span>
<span id="cb68-957"><a href="#cb68-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-958"><a href="#cb68-958" aria-hidden="true" tabindex="-1"></a>rangitikei <span class="sc">|&gt;</span> </span>
<span id="cb68-959"><a href="#cb68-959" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trans.ppl =</span> <span class="fu">log</span>(people)) <span class="sc">|&gt;</span> </span>
<span id="cb68-960"><a href="#cb68-960" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(time, temp) <span class="sc">|&gt;</span> </span>
<span id="cb68-961"><a href="#cb68-961" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb68-962"><a href="#cb68-962" aria-hidden="true" tabindex="-1"></a>    <span class="at">medians =</span> <span class="fu">median</span>(trans.ppl),</span>
<span id="cb68-963"><a href="#cb68-963" aria-hidden="true" tabindex="-1"></a>    <span class="at">ranges =</span> <span class="fu">max</span>(trans.ppl) <span class="sc">-</span> <span class="fu">min</span>(trans.ppl)</span>
<span id="cb68-964"><a href="#cb68-964" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">|&gt;</span> </span>
<span id="cb68-965"><a href="#cb68-965" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb68-966"><a href="#cb68-966" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">y=</span>ranges, <span class="at">x=</span>medians) <span class="sc">+</span></span>
<span id="cb68-967"><a href="#cb68-967" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span>
<span id="cb68-968"><a href="#cb68-968" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-969"><a href="#cb68-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-970"><a href="#cb68-970" aria-hidden="true" tabindex="-1"></a>If groups are skewed in different directions, we may not be able to find a common variance stabilizing transformation.</span>
<span id="cb68-971"><a href="#cb68-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-972"><a href="#cb68-972" aria-hidden="true" tabindex="-1"></a><span class="fu"># Nonparametric Methods</span></span>
<span id="cb68-973"><a href="#cb68-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-974"><a href="#cb68-974" aria-hidden="true" tabindex="-1"></a>In this section we outline an alternative to using power transformations as a method of preparing the data for hypothesis testing. The alternative approach usually relies on replacing the actual observed data values by their **ranks**. That is, the smallest data value is replaced by '1', the second smallest by '2', and so on up to the largest data value replaced by '$n$'. If there are several data with the same value (known as *ties*) then they are all assigned the average rank they would have gotten if they had received a tiny random increment before being placed in order: for example if the $3^{\text{rd}}$, $4^{\text{th}}$,${5^\text{th}}$ and ${6^\text{th}}$ smallest data values are all the same, then the four corresponding points are all assigned rank 4.5, which is the average of 3, 4, 5 and 6. We then analyse the ranks as if they are the original data.</span>
<span id="cb68-975"><a href="#cb68-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-976"><a href="#cb68-976" aria-hidden="true" tabindex="-1"></a>The rank approach in some ways seems like a bad idea, as we are throwing away information - the actual data - and only analysing a lesser amount of information which is the ranks. However the approach can be justified in two ways.</span>
<span id="cb68-977"><a href="#cb68-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-978"><a href="#cb68-978" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>One is the fact that mathematical and simulation studies have shown that hypothesis tests based on ranked data have very good power compared to tests based on the Normal distribution - even when the data are *truly* Normal. That is, we don't lose much by using a **nonparametric method** even if the Normal assumptions are perfectly true.</span>
<span id="cb68-979"><a href="#cb68-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-980"><a href="#cb68-980" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>On the other hand if the data are not normally distributed, then the nonparametric tests are still powerful but the normal-theory methods can go wrong. So we are safer using a nonparametric method. The second justification, on a bit more of a philosophical level, is that if we honestly do not know what the distribution is, then we are probably wise not to pretend that any transformation is going to make it Normal. After all some data are collected on very odd scales indeed (e.g. optometry refers to '20/20 vision' etc.), so that the data are just ordinal rather than numerical. In such a context rank methods may be appropriate as they pick up on ordinal difference rather than exact numerical difference.</span>
<span id="cb68-981"><a href="#cb68-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-982"><a href="#cb68-982" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ranking and rank Correlation</span></span>
<span id="cb68-983"><a href="#cb68-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-984"><a href="#cb68-984" aria-hidden="true" tabindex="-1"></a>A nonparametric approach used very frequently, especially in the social sciences, is the **Spearman's Rank Correlation**. To calculate it, first rank the $X$ and $Y$ variable, and then obtain usual correlation (the so-called Pearson correlation) coefficient. If there are a great many ties in the ranks then various corrections or modifications to the Spearman method have been suggested, but these are beyond the scope of this course. In principle then, one could simply apply the usual data analysis techniques to the $W= \text {rank}(Y)$ data and quote the $p$-values accordingly. We don't usually do this in simple analyses, for reasons outlined below, but let's explore this idea for a moment. @fig-spear shows the usual Pearson (lower diagonal) and Spearman rank correlations (upper diagonal) for the *trees* default <span class="in">`R`</span> dataset.</span>
<span id="cb68-985"><a href="#cb68-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-988"><a href="#cb68-988" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-989"><a href="#cb68-989" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-990"><a href="#cb68-990" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb68-991"><a href="#cb68-991" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb68-992"><a href="#cb68-992" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "fig-spear"</span></span>
<span id="cb68-993"><a href="#cb68-993" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Comparison of Pearson and Spearman rank correlations"</span></span>
<span id="cb68-994"><a href="#cb68-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-995"><a href="#cb68-995" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(</span>
<span id="cb68-996"><a href="#cb68-996" aria-hidden="true" tabindex="-1"></a>  trees, </span>
<span id="cb68-997"><a href="#cb68-997" aria-hidden="true" tabindex="-1"></a>  <span class="at">upper =</span> <span class="fu">list</span>(<span class="at">continuous =</span> <span class="fu">wrap</span>(<span class="st">'cor'</span>, <span class="at">method =</span> <span class="st">"spearman"</span>)), </span>
<span id="cb68-998"><a href="#cb68-998" aria-hidden="true" tabindex="-1"></a>  <span class="at">lower =</span> <span class="fu">list</span>(<span class="at">continuous =</span> <span class="st">'cor'</span>)</span>
<span id="cb68-999"><a href="#cb68-999" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb68-1000"><a href="#cb68-1000" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1001"><a href="#cb68-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1002"><a href="#cb68-1002" aria-hidden="true" tabindex="-1"></a>It can be noted that the size of the estimates differ depending on the skew and relationship between the variables. For large samples it would be quite a reasonable approach, as the distribution of $W$ values is symmetrical and therefore the usual data analysis methods - relying on $W$ being Normally distributed - will work pretty well. The main difference to standard hypothesis tests would be that they would need to be expressed in terms of medians, say, rather than means.</span>
<span id="cb68-1003"><a href="#cb68-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1004"><a href="#cb68-1004" aria-hidden="true" tabindex="-1"></a>For example, a two-group hypothesis test would be based on computing $W$ for all the $n$ data values together, and then comparing the mean of the $n_1$ ranks in the first group of observations to the mean of the $n_2$ ranks for the second group of observations. If there is no difference in population medians for the two groups, then we should not be able to reject the hypothesis that the mean $W$ values are the same.</span>
<span id="cb68-1005"><a href="#cb68-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1006"><a href="#cb68-1006" aria-hidden="true" tabindex="-1"></a>In practice there are a couple of complications. One is that for small samples the distribution of $W=\text {rank}(Y)$ is not Normal because it is discrete, taking only integers or averages of integers. But fortunately mathematical statisticians have long since worked out the exact distribution of $W$ for many simple situations including two-sample tests, one-way ANOVA, two-way ANOVA with balanced numbers, correlation coefficients (e.g. the correlation between $\text {rank}(Y)$ and $\text {rank}(X)$) and some others. So these exact distributions can be used for hypothesis tests, and are available. The second complication is that these exact distributions for $W= \text {rank}(Y)$ usually depend on the assumption of no ties, i.e. no equal ranks. Since ties do often occur in practice (if only because data are not measured exactly enough) then we need to use methods that are modified or corrected to handle ties. Fortunately the use of a software handles such issues as a matter of course in many cases, so it doesn't take any extra time or effort on our behalf.</span>
<span id="cb68-1007"><a href="#cb68-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1008"><a href="#cb68-1008" aria-hidden="true" tabindex="-1"></a><span class="fu">## Wilcoxon signed rank test</span></span>
<span id="cb68-1009"><a href="#cb68-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1010"><a href="#cb68-1010" aria-hidden="true" tabindex="-1"></a>For the nonparametric equivalent of a one-sample $t$-test for $H_0:\mu= \mu_0$, we use the **Wilcoxon signed rank test** for $H_0: \eta=\eta_0$ where $\eta$ (Greek letter 'eta') is the population median. Effectively this test is based on rank $(|Y-\eta_0|)$, where the ranks for data with $Y&lt;\eta_0$ are compared to the ranks for data with $Y&gt;\eta_0$. If the $\eta_0$ is in about the right place, then the distances to points above $\eta_0$ will tend to rank approximately the same as the distances to points below $\eta_0$. But if median is assumed too low, say, then the distances above $\eta_0$ will tend to be bigger (ranked higher) than the distances to points below $\eta_0$. A statistical test (Wilcoxon test) and the associated $p$-value follow. In theory, this test assumes a continuous symmetric distribution, but a correction is available in the case of ties. The following output shows the two-sample $t$ test and Wilcoxon test results for testing the equality of median number of people for the time of day groups (morning &amp; afternoon).</span>
<span id="cb68-1011"><a href="#cb68-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1014"><a href="#cb68-1014" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-1015"><a href="#cb68-1015" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(rangitikei<span class="sc">$</span>people <span class="sc">~</span> rangitikei<span class="sc">$</span>time, <span class="at">conf.int=</span>T)</span>
<span id="cb68-1016"><a href="#cb68-1016" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1017"><a href="#cb68-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1018"><a href="#cb68-1018" aria-hidden="true" tabindex="-1"></a>The <span class="in">`t.test(rangitikei$people~rangitikei$time)`</span> test also gives the same conclusion for the equality of means.</span>
<span id="cb68-1019"><a href="#cb68-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1020"><a href="#cb68-1020" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sign test</span></span>
<span id="cb68-1021"><a href="#cb68-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1022"><a href="#cb68-1022" aria-hidden="true" tabindex="-1"></a>There is an additional one-sample test available called the one-sample **sign test**, which is based on replacing $Y$ not by $\text {rank}(Y)$ but simply by the *sign* of $Y-\eta_0$, i.e. whether it is positive or negative. This replacement represents an additional loss of detail in the data, but also requires no assumptions. The resulting test is based on a binomial distribution. For example, consider the television viewing time data. Suppose we wish to test the hypothesis that children watch 4 hours of television per day on average (1680 minutes per week). The one-sample sign test output follows:</span>
<span id="cb68-1023"><a href="#cb68-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1026"><a href="#cb68-1026" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-1027"><a href="#cb68-1027" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(tv<span class="sc">$</span>TELETIME, <span class="at">mu=</span><span class="dv">1680</span>, <span class="at">conf.int=</span>T)</span>
<span id="cb68-1028"><a href="#cb68-1028" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1029"><a href="#cb68-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1030"><a href="#cb68-1030" aria-hidden="true" tabindex="-1"></a>Notice the Wilcoxon test has about the same $p$-value as the Normal-based $t$-test. However as it assumes symmetry the estimated median is the same as the sample mean. The $p$-value for the sign test is similar, but not the same as the others, and the estimated median is the same. The Wilcoxon and Sign test procedure can also be used to generate approximate 95% confidence intervals for the median. Note that these are based on the sorted sample data, and so are discrete, so it is usually not possible to get exact 95% confidence intervals. Both intervals are wider than the confidence interval based on the mean. The loss of precision (longer interval) is reasonable as we are making much weaker assumptions.</span>
<span id="cb68-1031"><a href="#cb68-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1032"><a href="#cb68-1032" aria-hidden="true" tabindex="-1"></a><span class="fu">## Wilcoxon Rank-Sum or Mann-Whitney test</span></span>
<span id="cb68-1033"><a href="#cb68-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1034"><a href="#cb68-1034" aria-hidden="true" tabindex="-1"></a>As an alternative to the two-sample $t$-test is the **Wilcoxon Rank-Sum test** (also mathematically equivalent to a test known as the **Mann-Whitney test**). The assumptions of the test are that the data are continuous (or at least ordinal) from populations that have the same shape (e.g. same skewness and same variance) but just (possibly) different medians. For this test, the entire set of responses is ranked together and then the ranks for the first group are compared to the ranks for the second group. The null hypothesis is that the two group medians are the same: $H_0: \eta_1=\eta_2$. The following <span class="in">`R`</span> output shows the Wilcoxon Rank-Sum test results.</span>
<span id="cb68-1035"><a href="#cb68-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1038"><a href="#cb68-1038" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-1039"><a href="#cb68-1039" aria-hidden="true" tabindex="-1"></a><span class="fu">kruskal.test</span>(tv<span class="sc">$</span>TELETIME <span class="sc">~</span> <span class="fu">factor</span>(tv<span class="sc">$</span>SCHOOL))</span>
<span id="cb68-1040"><a href="#cb68-1040" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1041"><a href="#cb68-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1042"><a href="#cb68-1042" aria-hidden="true" tabindex="-1"></a>Again since our data are normally distributed we expect to get a similar result for the Mann-Whitney test as for the two-sample $t$-test of equal means $\mu_1=\mu_2$. This is indeed the case.</span>
<span id="cb68-1043"><a href="#cb68-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1044"><a href="#cb68-1044" aria-hidden="true" tabindex="-1"></a><span class="fu"># Permutation and bootstrap tests</span></span>
<span id="cb68-1045"><a href="#cb68-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1046"><a href="#cb68-1046" aria-hidden="true" tabindex="-1"></a>The computationally intensive alternatives to non-parametric methods are also available. Such tests are popular in certain application areas. We will cover them at a basic level only but these are not too hard to perform using <span class="in">`R`</span> packages.</span>
<span id="cb68-1047"><a href="#cb68-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1048"><a href="#cb68-1048" aria-hidden="true" tabindex="-1"></a>A permutation (or randomisation) test is based on the idea of randomly permuting the observed data and then answering whether a hypothesis is negated or not. For example, consider the two-sample t-test example on testing whether the mean TV viewing times are the same for boys and girls. We can pool the all of the data and then randomly distribute the observed data into two groups and compute the difference in the means for the two groups (maintaining the group sizes of course). This process of randomly permuting data can be done for a large number of times and then the empirical (permutation) distribution of the differences can be obtained. We can then obtain a P value as a proportion of the permuted differences that are as extreme as the actual mean difference. Many <span class="in">`R`</span> packages are available to do this test, often with a single command. There are asymptotic forms of the permutation distribution for the statistic and hence computations can be done fairly quickly. </span>
<span id="cb68-1049"><a href="#cb68-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1050"><a href="#cb68-1050" aria-hidden="true" tabindex="-1"></a>Consider the one-sample t-test example for $H_0:\mu = 1500$ for the *tv* dataset.</span>
<span id="cb68-1051"><a href="#cb68-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1052"><a href="#cb68-1052" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=TRUE}</span></span>
<span id="cb68-1053"><a href="#cb68-1053" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(tv<span class="sc">$</span>TELETIME, <span class="at">mu =</span> <span class="dv">1500</span>)</span>
<span id="cb68-1054"><a href="#cb68-1054" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1055"><a href="#cb68-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1056"><a href="#cb68-1056" aria-hidden="true" tabindex="-1"></a>For two-sample test example based on the *tv* dataset, we obtain the following output using the <span class="in">`coin`</span> package. The P-values under this test is very similar to the P-value under the Welch t-test (0.471).</span>
<span id="cb68-1057"><a href="#cb68-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1058"><a href="#cb68-1058" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=TRUE}</span></span>
<span id="cb68-1059"><a href="#cb68-1059" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(coin)</span>
<span id="cb68-1060"><a href="#cb68-1060" aria-hidden="true" tabindex="-1"></a><span class="fu">independence_test</span>(TELETIME <span class="sc">~</span> SEX, <span class="at">distribution =</span> <span class="st">'exact'</span>, <span class="at">data =</span> tv)</span>
<span id="cb68-1061"><a href="#cb68-1061" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1062"><a href="#cb68-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1063"><a href="#cb68-1063" aria-hidden="true" tabindex="-1"></a>The symmetry in the distribution of the TV times can also be tested using the function <span class="in">`symmetry_test()`</span>. The significance of the Pearson correlation coefficient can be tested using <span class="in">`spearman_test()`</span>. Pairwise permutation test across groups can also be done; see the output shown below:</span>
<span id="cb68-1064"><a href="#cb68-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1065"><a href="#cb68-1065" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=TRUE}</span></span>
<span id="cb68-1066"><a href="#cb68-1066" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(<span class="at">x =</span> testmarks<span class="sc">$</span>Maths, </span>
<span id="cb68-1067"><a href="#cb68-1067" aria-hidden="true" tabindex="-1"></a>            <span class="at">y =</span> testmarks<span class="sc">$</span>English, </span>
<span id="cb68-1068"><a href="#cb68-1068" aria-hidden="true" tabindex="-1"></a>            <span class="at">paired=</span><span class="cn">TRUE</span>)</span>
<span id="cb68-1069"><a href="#cb68-1069" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1070"><a href="#cb68-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1071"><a href="#cb68-1071" aria-hidden="true" tabindex="-1"></a>There is a close theoretical connection between randomisation type and nonparametric tests. So the results are often similar when the same size is large.</span>
<span id="cb68-1072"><a href="#cb68-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1073"><a href="#cb68-1073" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bootstrap methods</span></span>
<span id="cb68-1074"><a href="#cb68-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1075"><a href="#cb68-1075" aria-hidden="true" tabindex="-1"></a>When a random sample is taken from a population, the expectation is that it is representative. So why not sample from the sample (i.e. resample) so that the quality of how well the sample is representative can be examined. We cannot gain extra information hugely by ordinary resampling and it is more like moving forward pulling the bootstrap! The methodology bootstrapping or resampling was introduced by @efron. This computational intensive procedure can be implemented very mechanically and simpler. By computing the sampling distribution of a statistic of interest, issues such as its bias and the standard error can be addressed.</span>
<span id="cb68-1076"><a href="#cb68-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1077"><a href="#cb68-1077" aria-hidden="true" tabindex="-1"></a><span class="in">`R`</span> package *boot* has many features and several variations (types) of the bootstrap resampling method but harder to use. We will use the *resample* package instead because it is simpler and also includes simple permutation tests.</span>
<span id="cb68-1078"><a href="#cb68-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1079"><a href="#cb68-1079" aria-hidden="true" tabindex="-1"></a>Under the simple bootstrap method, observations are resampled with replacement from the original sample to create a bootstrap sample. We can then compute a statistic such as the sample mean for this resample. This process can be repeated many times, say 10000, and we form the bootstrap distribution of the statistic. Consider the *tv* dataset. If we resample TELETIME, and compute the mean television viewing time for each sample, we construct the bootstrap distribution of mean. Using the <span class="in">`resample`</span> package, we get-</span>
<span id="cb68-1080"><a href="#cb68-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1083"><a href="#cb68-1083" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-1084"><a href="#cb68-1084" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(resample)</span>
<span id="cb68-1085"><a href="#cb68-1085" aria-hidden="true" tabindex="-1"></a>bootC <span class="ot">&lt;-</span> <span class="fu">bootstrap</span>(tv<span class="sc">$</span>TELETIME, mean)</span>
<span id="cb68-1086"><a href="#cb68-1086" aria-hidden="true" tabindex="-1"></a>bootC</span>
<span id="cb68-1087"><a href="#cb68-1087" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1088"><a href="#cb68-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1089"><a href="#cb68-1089" aria-hidden="true" tabindex="-1"></a>This output shows the observed sample mean 1729.283 and the mean of all bootstrap means which is 1729.114. The bias is the difference, which is -0.168. The main advantage of the bootstrap method is that it can quantify the bias that can occur due to sampling. @fig-bootc and @fig-bootqq, respectively, show the histogram and the normal quantile plots for the bootstrap means. Obviously the bootstrap means follow normal (due to CLT).</span>
<span id="cb68-1090"><a href="#cb68-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1093"><a href="#cb68-1093" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-1094"><a href="#cb68-1094" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-1095"><a href="#cb68-1095" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bootc</span></span>
<span id="cb68-1096"><a href="#cb68-1096" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Histogram of the bootstrap means of TELETIME"</span></span>
<span id="cb68-1097"><a href="#cb68-1097" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb68-1098"><a href="#cb68-1098" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb68-1099"><a href="#cb68-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1100"><a href="#cb68-1100" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(bootC)</span>
<span id="cb68-1101"><a href="#cb68-1101" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1102"><a href="#cb68-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1103"><a href="#cb68-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1106"><a href="#cb68-1106" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-1107"><a href="#cb68-1107" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb68-1108"><a href="#cb68-1108" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bootqq</span></span>
<span id="cb68-1109"><a href="#cb68-1109" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "QQ plot of the bootstrap means of TELETIME"</span></span>
<span id="cb68-1110"><a href="#cb68-1110" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb68-1111"><a href="#cb68-1111" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb68-1112"><a href="#cb68-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1113"><a href="#cb68-1113" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(bootC)</span>
<span id="cb68-1114"><a href="#cb68-1114" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1115"><a href="#cb68-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1116"><a href="#cb68-1116" aria-hidden="true" tabindex="-1"></a>There are many versions of bootstrap confidence intervals depending on the way bootstrapping is done. Without going into details, the 95% confidence interval for the true mean viewing time is obtained as follows:</span>
<span id="cb68-1117"><a href="#cb68-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1120"><a href="#cb68-1120" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-1121"><a href="#cb68-1121" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 7</span></span>
<span id="cb68-1122"><a href="#cb68-1122" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb68-1123"><a href="#cb68-1123" aria-hidden="true" tabindex="-1"></a><span class="fu">CI.t</span>(bootC)</span>
<span id="cb68-1124"><a href="#cb68-1124" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1125"><a href="#cb68-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1126"><a href="#cb68-1126" aria-hidden="true" tabindex="-1"></a>This interval compares well with the confidence interval using t-distribution found earlier namely (1560.633, 1897.932). The same approach can be taken to construct a confidence interval for the mean of the paired differences and thereby perform a test analogous to paired t-test. See the *testsmarks* data example given below:</span>
<span id="cb68-1127"><a href="#cb68-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1128"><a href="#cb68-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1131"><a href="#cb68-1131" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-1132"><a href="#cb68-1132" aria-hidden="true" tabindex="-1"></a>differ <span class="ot">&lt;-</span> testmarks<span class="sc">$</span>Maths<span class="sc">-</span>testmarks<span class="sc">$</span>English</span>
<span id="cb68-1133"><a href="#cb68-1133" aria-hidden="true" tabindex="-1"></a>bootC <span class="ot">&lt;-</span> <span class="fu">bootstrap</span>(differ, mean)</span>
<span id="cb68-1134"><a href="#cb68-1134" aria-hidden="true" tabindex="-1"></a><span class="fu">CI.t</span>(bootC)</span>
<span id="cb68-1135"><a href="#cb68-1135" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1136"><a href="#cb68-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1137"><a href="#cb68-1137" aria-hidden="true" tabindex="-1"></a>The parametric (i.e. t-test based) and bootstrap results are very similar. The bootstrap method can obtain better confidence intervals for the mean when the population is skewed because resampling tends to adjust for the skew in the population when captured by the sample well. The bootstrap approach will work well only when reasonably large sample sizes are available because of the inherent uncertainty in the tails of the underlying distribution. Small samples are not sufficient to identify capture the tails of the distribution and hence certain types of inferences involving tail part of distribution will not work well. Parametric assumptions can be made to improve the bootstrap method and this approach is known as parametric bootstrapping. We will not study such methods in this course.</span>
<span id="cb68-1138"><a href="#cb68-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1139"><a href="#cb68-1139" aria-hidden="true" tabindex="-1"></a>The <span class="in">`resample`</span> package also has options to bootstrap from two vectors. Consider the television viewing times for the boys and girls groupings. We resample from the two groups to perform the two sample test.</span>
<span id="cb68-1140"><a href="#cb68-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1143"><a href="#cb68-1143" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb68-1144"><a href="#cb68-1144" aria-hidden="true" tabindex="-1"></a>bootC <span class="ot">&lt;-</span> <span class="fu">bootstrap2</span>(tv<span class="sc">$</span>TELETIME, <span class="at">statistic=</span>mean, <span class="at">treatment=</span>tv<span class="sc">$</span>SEX)</span>
<span id="cb68-1145"><a href="#cb68-1145" aria-hidden="true" tabindex="-1"></a><span class="fu">CI.t</span>(bootC)</span>
<span id="cb68-1146"><a href="#cb68-1146" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb68-1147"><a href="#cb68-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1148"><a href="#cb68-1148" aria-hidden="true" tabindex="-1"></a>The bootstrap test conclusion again agrees with the Welch two-sample t-test conclusion.</span>
<span id="cb68-1149"><a href="#cb68-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1150"><a href="#cb68-1150" aria-hidden="true" tabindex="-1"></a>You will not be examined on the use of permutation and bootstrap tests in the final exam. We may occasionally use this approach for assignments.</span>
<span id="cb68-1151"><a href="#cb68-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1152"><a href="#cb68-1152" aria-hidden="true" tabindex="-1"></a><span class="fu"># Summary</span></span>
<span id="cb68-1153"><a href="#cb68-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1154"><a href="#cb68-1154" aria-hidden="true" tabindex="-1"></a>The Normal distribution is important not only in its own right as a model for data, but because it plays a basic part in confirmatory statistics. Indeed the $t$, $F$ and $\chi^2$ statistics we will use in the rest of this course all assume that the populations are Normal. The sample under study will not follow exactly a Normal distribution but we will often require it to be at least approximately symmetric.</span>
<span id="cb68-1155"><a href="#cb68-1155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1156"><a href="#cb68-1156" aria-hidden="true" tabindex="-1"></a>A standard Normal, $Z$, can be obtained as follows:</span>
<span id="cb68-1157"><a href="#cb68-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1158"><a href="#cb68-1158" aria-hidden="true" tabindex="-1"></a>$Z$ = (statistic-expected)/standard error.</span>
<span id="cb68-1159"><a href="#cb68-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1160"><a href="#cb68-1160" aria-hidden="true" tabindex="-1"></a>The statistic may be the sample mean, $\bar{y}$, or it may be the coefficient of a linear model.</span>
<span id="cb68-1161"><a href="#cb68-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1162"><a href="#cb68-1162" aria-hidden="true" tabindex="-1"></a>If the standard error is not known but must be estimated from the sample, the standard variable follows a $t$ rather than a $Z$ distribution. If the statistic is the mean, $\bar{y}$, the standard error is $S/\sqrt n$, here $S=\sqrt{{\frac{1}{n-1}} \sum _{i=1}^{n}(y_{i} -\bar{y})^{2} }$.</span>
<span id="cb68-1163"><a href="#cb68-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1164"><a href="#cb68-1164" aria-hidden="true" tabindex="-1"></a>For other statistics, the standard error may be more complicated and we shall rely on a computer program to calculate this.</span>
<span id="cb68-1165"><a href="#cb68-1165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1166"><a href="#cb68-1166" aria-hidden="true" tabindex="-1"></a>Usually a confidence interval for the expected value has the form</span>
<span id="cb68-1167"><a href="#cb68-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1168"><a href="#cb68-1168" aria-hidden="true" tabindex="-1"></a>statistic $\pm$ margin of error</span>
<span id="cb68-1169"><a href="#cb68-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1170"><a href="#cb68-1170" aria-hidden="true" tabindex="-1"></a>where margin of error = $t$ $\times$ estimated standard error.</span>
<span id="cb68-1171"><a href="#cb68-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1172"><a href="#cb68-1172" aria-hidden="true" tabindex="-1"></a>Notice that:</span>
<span id="cb68-1173"><a href="#cb68-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1174"><a href="#cb68-1174" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>For a 95% confidence interval of population parameter, we would expect 95% of such intervals to include the true value of the parameter (such as the population mean).</span>
<span id="cb68-1175"><a href="#cb68-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1176"><a href="#cb68-1176" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>To increase the confidence level from (say) 95% to 99%, the tabulated $t$-value will increase, yielding a wider confidence interval.</span>
<span id="cb68-1177"><a href="#cb68-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1178"><a href="#cb68-1178" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>If the sample size was increased, we would expect the estimated standard error to decrease so that the confidence interval would become shorter.</span>
<span id="cb68-1179"><a href="#cb68-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1180"><a href="#cb68-1180" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>We have considered symmetric confidence intervals. If data is skewed (say to the right) a symmetric confidence interval may not be appropriate. In such a case we may use a transformation to make the data symmetric, find our confidence interval, and convert back to the original scale.</span>
<span id="cb68-1181"><a href="#cb68-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1182"><a href="#cb68-1182" aria-hidden="true" tabindex="-1"></a>Hypothesis testing and confidence intervals for the mean difference of two batches of data usually assume that the two populations are normally distributed and the variances of the two populations are the same. Of course the variances of the samples will differ but, hopefully, not by too much for this would suggest that a transformation may be desirable. If the means of the two populations are equal, we would expect the difference of the sample means, $(\bar{y}_{1} -\bar{y}_{2})$, to be close to zero and, conversely, we would expect the confidence interval for the difference in means to include zero.</span>
<span id="cb68-1183"><a href="#cb68-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1184"><a href="#cb68-1184" aria-hidden="true" tabindex="-1"></a>If the population means are different, we would expect the difference in the sample means to be a value different from zero. So, if the value of zero does *not* fall in the confidence interval, the hypothesis that the means are equal will be *rejected*. We make our decision to accept or reject the hypothesis based on the P-value. We reject $H_0: \mu_1 = \mu_2$ only if the $p$-value $&lt;$ the set significance level, usually 5%.</span>
<span id="cb68-1185"><a href="#cb68-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1186"><a href="#cb68-1186" aria-hidden="true" tabindex="-1"></a>There are many ways of transforming data. Simple transformations can be used such as multiplying each value by 100 to remove decimals or to convert to percentages, or a ratio transformation, such as petrol consumption expressed as litres per 100 km. In this course, we concentrate on power transformations where each observation, $y$, is raised to the power of $\lambda$. For negative values of $\lambda$, it is advisable to multiply transformed values by $-1$ as this preserves the original order in the data. In this chapter, we have considered two reasons for transformations.</span>
<span id="cb68-1187"><a href="#cb68-1187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1188"><a href="#cb68-1188" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>The first reason was to obtain symmetry in a single batch of data. Such transformations are sometimes required when we decide to carry out hypothesis tests or compute confidence intervals for some variable.</span>
<span id="cb68-1189"><a href="#cb68-1189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1190"><a href="#cb68-1190" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>With more than two batches of data, we have the additional reason of the need for common variance across batches. To compare means using Normal-based methods, it is advisable for the measurements in each batch to be both distributed symmetrically and with the same spread and variation as those in other batches. It may not be possible to find a single transformation, which will bring about these two ideal characteristics but a transformation may be found to bring the measurements closer to these ideals. Note that, to compare groups, or the means of these groups, the same transformation must be applied to each group. If groups are skewed in different directions (positive and negative), no transformation would be appropriate.</span>
<span id="cb68-1191"><a href="#cb68-1191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-1192"><a href="#cb68-1192" aria-hidden="true" tabindex="-1"></a>Nonparametric and other computationally intensive methods such as the regular bootstrap methods can be useful when normal and other distributional assumptions are grossly violated. These methods are also useful for validating Normal-based results.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>