---
title: "Chapter 5:<br>Tabulated Counts"
image: img/mendel1.png
format: 
  revealjs:
    width: 1050
    height:	700
    scrollable: true
    transition: fade
    theme: [default, myquarto.scss]
    slide-number: c/t  
    logo: img/L_Color.png
    footer: "[161250 Data Analysis](https://anhsmith.quarto.pub/161250-data-analysis/slides.html)"
    styles:
      - revealjs.dark:
        background-color: #222
        color: #fff
    
execute:
  echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      echo = FALSE, 
                      include=TRUE, 
                      message=FALSE, 
                      comment = NA, 
                      warn=-1, 
                      warn.conflicts = FALSE, 
                      quietly=TRUE, 
                      fig.align="center")
```

## Tables and frequencies

-   Main question

    -   How do the observed counts compare with known (assumed or
        expected) counts?

-   Model

    ```         
    `data = fit + error`  (or)

    `observed = expected + residual`
    ```

-   `Observed` comes from sample data

-   `Expected` comes from model or from assumption about the population

## Example

-   Does the distribution of rejects of metal castings by causes in a
    particular week vary from the long term average counts?

| Causes of rejection | Rejects during the week | long term |
|---------------------|-------------------------|-----------|
| sand                | 90                      | 82        |
| misrun              | 8                       | 4         |
| shift               | 16                      | 10        |
| drop-               | 8                       | 6         |
| corebreak           | 23                      | 21        |
| broken              | 21                      | 20        |
| other               | 5                       | 8         |

-   Long term average counts are the `expected` counts.
-   How to compare `observed` with `expected`?

## Goodness of fit test

-   Chi-squared test statistic ($\chi ^ 2$ = Ki Sq.)

$$
\chi ^{2} =\sum _{1}^{c}\frac{\left({\rm Observed-Expected}\right)^{{\rm 2}} }{{\rm Expected}}  =\sum _{1}^{c}\frac{\left( O-E\right)^{2}}{E}
$$

-   If the number of categories is $c$, then the degrees of freedom is
    $c-1$
    -   A single df is lost due to $\sum E= \sum O$
-   $\chi ^ 2$ distribution is a continuous distribution. When used for
    discrete data, Chi-squared approximation is very good for $E>5$ and
    often good if some expected values are as small as 1.
    -   Merge/combine categories in case of small expected counts.

## Example

A survey of voters included 550 males and 450 females. Will you call
this survey as a biased one?

| Gender | $O$  | $E$  | $(O-E)^2/E$          |
|--------|------|------|----------------------|
| male   | 550  | 500  | $(550-500)^2/ 500=5$ |
| female | 450  | 500  | $(450-500)^2/ 500=5$ |
| sum    | 1000 | 1000 | 10                   |

-   df= (2-1)=1
-   At 5% level, the critical value is only 3.84. So the sample is a
    biased one.

## Mendel's experiment (Read SG)

::: left-narrow
-   Mendel discovered the principles of heredity by breeding garden
    peas. In one Mendel's trials ratios of various types of peas
    (dihybrid-crosses) were 9:3:3:1

-   The observed results are very close to expected results. This
    results in a small chi squared value. Were experimental results
    fudged or was there a *confirmation bias*?
:::

::: right-wide
![](img/mendel1.png)
:::

## Goodness of fit for distributions

-   Treat class intervals as *categories* and obtain the actual counts
    (O) 
-   The assumed distribution gives the expected counts (E)
-   Perform a goodness of fit and validate the assumed theoretical
    distribution
-   Adjust the degrees of freedom (df) for the number of estimated
    parameters of the theoretical distribution
    -   For example, assume that you have 10 class intervals and test
        for normal distribution, which has 2 parameters.\
    -   So the df for this test will be 10-1-2=7.

## Contingency table

-   Given a two way table of frequency counts, **we test whether the row
    and column variables are independent**

-   The expected count for cell $(i,j)$ is given by $E_{ij}$ =
    $(T_i \times T_j)/n$ where\
    $~~~~~T_i$, the total for row $i$;\
    $~~~~~T_j$, the total for column $j$\
    $~~~~~n$, the overall total count

-   Test statistic :
    $\chi ^{2} =\sum _{{i=1}}^{r}\sum _{{j=1}}^{{\rm c}}\frac{\left({ O}_{{ ij}} { -E}_{{ij}} \right)^{{ 2}} }{{ E}_{{ij}} }.$

-   degrees of freedom: $(r-1)(c-1)$

## Example

`Context`: Porcine Stress Syndrome (PSS) result in pale, soft meat in
pigs and under conditions of stress- death.

-   Presence of PPS is a positive reaction to breathing halothane.
    -   Selective breeding for reducing incidence of PSS

```{r}
col.lbl = c("Halothane.positive", "Halothane.negative")
row.lbl = c("Large White", "Hampshire", "Landrace(B)", "Landrace(S)")
col1 = c(2, 3, 11, 16)
col2 = c(76, 86, 73, 76)
tabledata = matrix(cbind(col1, col2), nrow = 4, ncol=2, byrow=F,
                   dimnames = list(row.lbl, col.lbl))
dt <- as.data.frame(tabledata)
dt <- data.frame(tabledata)
dt$Totals <- rowSums(dt)
dt["Totals",] <- colSums(dt)
library(tidyverse)
library(kableExtra)
kable(dt, caption="Porcine Stress Syndrome (PSS) data") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

```{r, echo=TRUE}
chisq.test(dt)
```

## Computations

![](img/halothane1.png)

## Inference

-   The tabulated Chisq with (4-1)x(2-1) = 3 d.f are
    -   7.81 at 5% level; 16.27 at 1% level.

-   ::: red
    Conclusion: There is a statistical evidence that the breed is not
    independent of the result of the Halothane test.
    :::

    -   Note that large counts in the second column (Halothane negative)
        lead to large expected values but low contributions to
        chi-squared statistic.
    -   This is because of the division by the appropriate expected
        value. On the other hand, the small observations of first column
        lead to small expected values but large contributions to the
        $\chi ^{2}$.

## Significant cell contribution

-   Counts follow Poisson distribution for which *mean = variance*

-   Hence \[\chi \^{2}
    =\sum \frac{\left({\rm residual}\right)^{{\rm 2}} }{{\rm variance}}
    =\sum \left(\frac{{\rm residual}}{{\rm std\; dev}} \right)\^{2}. \]

-   Individual cell contribution is similar to standardized residual.
    Any standardized residual greater than 2 is regarded as significant.

-   So $2^2 = 4$ is treated as a significant contribution to the
    $\chi ^{2}$ statistic.

## Warnings

-   Use only frequency counts. Use of percentages in place of counts may
    lead to incorrect conclusions.

-   Check for small expected values. An expected value of less than 5
    may lead to concern and a very small value of less than 1 is a
    warning.

-   If the chi-squared statistic is small enough to be not significant,
    there is no problem.

-   If chi-squared statistic is significant, check the contributions to
    each cell. If cells with large expected value (\>5) contribute a
    large amount to chi-squared statistic, again there is no problem.

-   If cells with expected values less than 5 lead large contributions
    to chi-squared statistic, the significance of the chi-squared
    statistic should be treated with caution.

## Simpson's paradox

Group 1

```{r}
group1 <- matrix(c(80,120,30,80), nrow=2, ncol=2, byrow=T)
group1
chisq.test(group1)
```

Group 2

```{r}
group2 <- matrix(c(20,75,25,20), nrow=2, ncol=2, byrow=T)
group2
chisq.test(group2)
```

After amalgamation of both groups

```{r}
all <- matrix(c(80,120,30,80)+c(20,75,25,20), nrow=2, ncol=2, byrow=T)
all
chisq.test(all)
```

## Permutation test

This test is done maintaining the marginal totals.

```         
Data: Smoking Status vs. Staff  Groupings
```

```{r, echo=FALSE}
col.lbl = c("None","Moderate", "Heavy")
row.lbl = c("Junior employees", "Junior managers", "Secretaries", "Senior employees", "Senior managers")
col1 = c(18, 4, 10, 25, 4)
col2 = c(57, 10, 13, 22, 5)
col3 = c(13, 4, 2, 4, 2)
tabledata = matrix(cbind(col1, col2, col3), 
                   nrow = 5, ncol=3, byrow=F,
                   dimnames = list(row.lbl, col.lbl))

dt <- data.frame(tabledata)
dt$Totals <- rowSums(dt)
dt["Totals",] <- colSums(dt)
library(kableExtra)
library(tidyverse)
kable(dt, caption=" Smoking Status vis-a-vis Staff Groupings") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Permutation test

```{r, echo=TRUE, warning=FALSE}
set.seed(12321)
chisq.test(tabledata, simulate.p.value = TRUE)
```

Regular Chi-square test

```{r, echo=TRUE, warning=FALSE}
chisq.test(tabledata)
```

## Log-linear model

This model is built based on the assumption that the log of the count,
which is predicted or modelled by categorical factors, is normally
distributed.

Easy to fit using the MASS package but the data must be in the tall
format.

```{r, echo=FALSE}
data.frame(tabledata) %>%
  pivot_longer(cols = 1:3, 
               values_to = "Counts",
               names_to = "Smoke_status") ->smk
smk %>% 
  mutate(Employ=rep(row.lbl,each=3)) %>% 
  relocate(Smoke_status,Employ, Counts)->smk
glimpse(smk)
```

```{r, echo=TRUE}
library(MASS)
MASS::loglm(Counts~ Smoke_status+Employ, smk)
```

-   In addition to the usual Pearson Chi-square statistic, a different
    test statistic based on the likelihood principle is also given.

-   Useful to model multi-way tables with more than two factors.

-   Generalised linear models are preferred to log-linear models where
    the count response can be assumed to follow Poisson. Logistic models
    can be built to binomial counts. These models are covered in the
    *applied linear models* course.

## Correspondence Analysis \| (not examined in depth)

-   CA is for assessing interdependence of categorical variables
    presented in the form of a contingency table

    Data: Smoking Status vs. Staff Groupings

\tiny

```{r, echo=FALSE, warning=FALSE, comment=""}
col.lbl <-  c("None","Moderate", "Heavy")
row.lbl <- c("Junior employees", "Junior managers", "Secretaries", "Senior employees", "Senior managers")
col1 <- c(18, 4, 10, 25, 4)
col2 <- c(57, 10, 13, 22, 5)
col3 <- c(13, 4, 2, 4, 2)
tabledata <- matrix(cbind(col1, col2, col3), 
                   nrow = 5, ncol=3, byrow=F,
                   dimnames = list(row.lbl, col.lbl))
tabledata
chisq.test(tabledata)
```

## Row mass & row profiles

-   row profiles are found dividing the cell counts by the corresponding
    row total

-   row mass is found dividing the row totals by the grand total

```{r, , comment=""}
#tabledata
#formatC(chisq.test(tabledata)$exp)
#chisq.test(tabledata)
#chisq.test(tabledata, simulate.p.value = T, B=5000)
rowtotal <- rowSums(tabledata)
rowprofile <- tabledata/rowtotal
coltotal <- colSums(tabledata)
colprofile <- tabledata/coltotal
rowmass <- coltotal/sum(coltotal)
colmass <- rowtotal/sum(rowtotal)
options(digits=3)
rbind(rowprofile, rowmass)
```

-   Similarly column profiles & column masses can be found

## Graphical display of row profiles

```{r, fig.height=5.5, fig.width=8}
matplot(rowprofile, 
        ylab ="Row Profiles",
        xlab ="Staff Grouping", 
        pch=1:3, col=1,
        xaxt = "n")
axis(1, at = c(1:5), labels = row.lbl)
legend("topright", col.lbl, pch=1:3, col=1, title = "Smoking Status")   
```

-   No clear patterns seen

## Symmetric plots to find subgrouping {.scrollable}

```{r, fig.height=5, fig.width=7}
#| echo: true
#| fig-pos: false
library("FactoMineR")
CA(tabledata) |> summary()
```

## Symmetric plots to find subgrouping

![](img/CA.png)

## Summary

-   Goodness of fit is for testing whether the observed counts are from
    the hypothesised population groups.

    -   For $c$ categories (groups), the test involves $c-1$ df.

-   Contingency Table ($r$ rows and $c$ columns) data are tested for the
    independence.

    -   For $r\times c$ cells the test involves $(r-1)(c-1)$ df
    -   shiny app
        <http://shiny.massey.ac.nz/anhsmith/demos/explore.counts.of.factors/>

-   $\chi ^{2}$ test works well if $E> 5$. Some could be as low as 1.

-   Do correspondence analysis (symmetric plots) when independence is
    rejected.

<!-- ## Exercises -->

<!-- download.file("http://www.massey.ac.nz/~kgovinda/220exer/Chap7moreexamples.R", destfile="Chap7moreexamples.R") -->

<!-- download.file("https://www.massey.ac.nz/~kgovinda/220exer/chapter-7-exercises.html", destfile="chapter-7-exercises.html") -->

<!-- install.packages("remotes") -->

<!-- remotes::install_github("ricompute/ricomisc") -->

<!-- ricomisc::rstudio_viewer("chapter-7-exercises.html", file_path = NULL) -->
