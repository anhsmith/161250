---
title: "Chapter 5:<br>Tabulated Counts"
image: img/mendel1.png
format: 
  revealjs:
    width: 1050
    height:	700
    scrollable: true
    transition: fade
    theme: [default, myquarto.scss]
    slide-number: c/t  
    logo: img/L_Color.png
    footer: "[161250 Data Analysis](https://anhsmith.github.io/161250/slides.html)"
    styles:
      - revealjs.dark:
        background-color: #222
        color: #fff
    
execute:
  echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      echo = FALSE, 
                      include=TRUE, 
                      message=FALSE, 
                      comment = NA, 
                      warn=-1, 
                      warn.conflicts = FALSE, 
                      quietly=TRUE, 
                      fig.align="center")
```

## Analysing frequencies

- This chapter focuses on data that consist of frequencies or counts of occurrences of some events

- Wish to compare *observed* with what we would have *expected*

- We will cover the $\chi ^ 2$ distribution, "Goodness-of-fit" tests, and test of independence

## Chi squared distribution
::: left-narrow
- Continuous 0 to infinity; starts at 0 because it is a square (a square number can't be negative)

- The mean of this distribution is its degrees of freedom (k)

- It is right skewed, mean greater than median and mode; variance is $2k$

- Shape of the $\chi ^ 2$ distribution is determined by the degrees of freedom (k), at very high k (90 or greater) $\chi ^ 2$ distribution resembles the normal distribution

- Main purpose is hypothesis testing, not describing real-world distributions
:::

::: right-wide
![](img/chi-square-distribution-k-example.png)
:::

## Tables and frequencies

-   We often have hypotheses regarding the frequencies of levels of a factor or group of factors.

    - For a single two-level factor (e.g., male vs female, survived vs died), we might wish to know how likely the data are to have come from a population with equal proportions (or some other specified proportion). 

    - For two factors, we might wish to know whether they are independent. 

- In either case, we can specify a null hypothesis and test it using data. 

- To do so, compare *observed* counts with *expected* counts, where the *expected* counts are derived from our null model about the population.

- We are employing the Chi-squared statistic with data which consists of integers. That is, the data are discrete rather than continuous. 

## Examples of data with one factor 

#### Example 1

- A dataset contains 40 males and 50 females.
- How plausible is the null model of these counts coming from a population with 50% males and 50% females?
- The expected counts in this case would be 45 males and 45 females.

#### Example 2

::::{.columns}
:::{.column}

-   Does the distribution of rejects of metal castings by causes in a particular week vary from the long-term average counts?
-   Treat the long-term average counts as the *expected* counts.
-   Compare *observed* counts with *expected* counts.

:::
:::{.column style="font-size: 85%;"}

| Causes of rejection | Rejects during the week | Long-term average |
|---------------------|-------------------------|-----------|
| sand                | 90                      | 82        |
| misrun              | 8                       | 4         |
| shift               | 16                      | 10        |
| drop-               | 8                       | 6         |
| corebreak           | 23                      | 21        |
| broken              | 21                      | 20        |
| other               | 5                       | 8         |

:::
::::

## Chi-squared test statistic

$$
\chi ^{2} =\sum _{1}^{c}\frac{\left({\rm Observed-Expected}\right)^{{\rm 2}} }{{\rm Expected}}  =\sum _{1}^{c}\frac{\left( O-E\right)^{2}}{E}
$$

If the number of categories is $c$, then the degrees of freedom is $c-1$. 

## Chi-squared test statistic

If the null hypothesis were true, then the value of $\chi ^{2}$ calculated from our data is a random value from a Chi-squared distribuion: 
$$
\chi_{0} ^{2} = \chi_{c-1} ^{2}
$$
Once we calculate the test statistic, we can compare our observed value with its distribution under $H_{0}$ to calculate a p-value.

## Assumptions

-   The classification of observations into groups must be independent

-   No more than 20% of categories should have expected counts less than 5

## Warnings

-   Use only frequency counts. Use of percentages in place of counts may
    lead to incorrect conclusions.

-   Check for small expected values. An expected value of less than 5
    may lead to concern and a very small value of less than 1 is a
    warning. Sometimes, you can merge/combine categories in case of small
    expected counts.

-   If the chi-squared statistic is small enough to be not significant,
    there is no problem.

-   If chi-squared statistic is significant, check the contributions to
    each cell. If cells with large expected value (\>5) contribute a
    large amount to chi-squared statistic, again there is no problem.

-   If cells with expected values less than 5 lead large contributions
    to chi-squared statistic, the significance of the chi-squared
    statistic should be treated with caution.

## Goodness of fit test

- Compare *observed* frequencies with *expected* frequencies under some specified null hypothesis.

- Make a hypothesis about the population, what would we expect the frequency to be under that hypothesis?

- For example:

    - Wish to compare the frequency of occurence of different phenotypes in an organism with the frequencies we would expect under Mendel's laws of inheritance.
    - Wish to compare the distribution of a observation with expected count we would obtain from a Poisson distribution. 
    
Hypotheses of this sort can be tested using the **Chi-squared test statistic** ($\chi ^ 2$ = Ki Sq.)


## Example

A survey of voters included 550 males and 450 females. Will you call
this survey as a biased one?

Here the null hypothesis is that the ratio of males to females is 1:1.

Equivalent: the proportion of males is equal to the proportion of female in the population

| Gender | $O$  | $E$  | $(O-E)^2/E$          |
|--------|------|------|----------------------|
| male   | 550  | 500  | $(550-500)^2/ 500=5$ |
| female | 450  | 500  | $(450-500)^2/ 500=5$ |
| sum    | 1000 | 1000 | 10                   |

$$
\chi ^{2} = \sum _{1}^{c}\frac{\left( O-E\right)^{2}}{E} = 
\frac{(550-500)^2}{500} + \frac{(450-500)^2}{500} = 10
$$
## Example
![](img/chisq_ex.png)

-   df= (2-1)=1
-   At 5% level ($\alpha =0.05$), the critical value is only 3.84. So the sample is a biased one.

## Mendel's experiment (see Study Guide)

::: left-narrow
-   Mendel discovered the principles of heredity by breeding garden
    peas. In one Mendel's trials ratios of various types of peas
    (dihybrid-crosses) were 9:3:3:1

-   The observed results are very close to expected results. This
    results in a small chi squared value. Were experimental results
    fudged or was there a *confirmation bias*?
:::

::: right-wide
![](img/mendel1.png)
:::

## Goodness of fit for distributions

-   Treat class intervals as *categories* and obtain the actual counts
    (O) 
-   The assumed distribution gives the expected counts (E)
-   Perform a goodness of fit and validate the assumed theoretical
    distribution
-   Adjust the degrees of freedom (df) for the number of estimated
    parameters of the theoretical distribution
    -   For example, assume that you have 10 class intervals and test
        for normal distribution, which has 2 parameters.\
    -   So the df for this test will be 10-1-2=7.

## Goodness of fit for distributions example
A safety inspector monitors car accidents at a bustling intersection. The inspector enters the counts of monthly accidents.

Null: The sample data follow the Poisson distribution.
Alternative: The sample data do not follow the Poisson distribution.


*Note* The Poisson distribution is a discrete probability distribution (integers) that can model counts of events or attributes in a fixed observation space. Many but not all count processes follow this distribution. 
## Goodness of fit for distributions example

| Accidents| $O$ | $E$  | $(O-E)^2/E$         |
|---------|-----|------|----------------------|
| 0       | 7   |      |                      |
| 1       | 8   |      |                      |
| 2       | 13  |      |                      |
| 3       | 10  |      |                      |
| >=4     | 12  |      |                      |
|---------|-----|------|----------------------|
| Sum     | 40  |      |                      |

Conclusion:

## Tests of Independence

- In some cases, we have counts of observations cross-classified in terms of two factors.

- We are generally interested in determining whether or not the two factors are independent.

- If we consider the observations falling into each category for factor 1, is this distribution consistent across all levels of factor 2? (or vice versa)

## Contingency table

-   Given a two way table of frequency counts, **we test whether the row
    and column variables are independent**

-   Hypotheses:

    - Null: the two factors are independent
    - written another way: the row (or column) distributions are the same

-   The expected count for cell $(i,j)$ is given by $E_{ij}$ =
    $(T_i \times T_j)/n$ where\
    $~~~~~T_i$, the total for row $i$;\
    $~~~~~T_j$, the total for column $j$\
    $~~~~~n$, the overall total count

-   Test statistic :
    $\chi ^{2} =\sum _{{i=1}}^{r}\sum _{{j=1}}^{{\rm c}}\frac{\left({ O}_{{ ij}} { -E}_{{ij}} \right)^{{ 2}} }{{ E}_{{ij}} }.$

-   degrees of freedom: $(r-1)(c-1)$

## Example

`Context`: Porcine Stress Syndrome (PSS) result in pale, soft meat in
pigs and under conditions of stress- death.

-   Presence of PPS is a positive reaction to breathing halothane.
    -   Selective breeding for reducing incidence of PSS

```{r}
col.lbl = c("Halothane.positive", "Halothane.negative")
row.lbl = c("Large White", "Hampshire", "Landrace(B)", "Landrace(S)")
col1 = c(2, 3, 11, 16)
col2 = c(76, 86, 73, 76)
tabledata = matrix(cbind(col1, col2), nrow = 4, ncol=2, byrow=F,
                   dimnames = list(row.lbl, col.lbl))
dt <- as.data.frame(tabledata)
dt <- data.frame(tabledata)
dt$Totals <- rowSums(dt)
dt["Totals",] <- colSums(dt)
library(tidyverse)
library(kableExtra)
kable(dt, caption="Porcine Stress Syndrome (PSS) data") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

```{r, echo=TRUE}
chisq.test(dt)
```

## Computations

![](img/halothane1.png)

## Inference

-   The tabulated Chisq with (4-1)x(2-1) = 3 d.f are
    -   7.81 at 5% level; 16.27 at 1% level.

-   ::: red
    Conclusion: There is a statistical evidence that the breed is not
    independent of the result of the Halothane test.
    :::

    -   Note that large counts in the second column (Halothane negative)
        lead to large expected values but low contributions to
        chi-squared statistic.
    -   This is because of the division by the appropriate expected
        value. On the other hand, the small observations of first column
        lead to small expected values but large contributions to the
        $\chi ^{2}$.

## Significant cell contribution

-   Counts follow Poisson distribution for which *mean = variance*

-   Hence \[\chi \^{2}
    =\sum \frac{\left({\rm residual}\right)^{{\rm 2}} }{{\rm variance}}
    =\sum \left(\frac{{\rm residual}}{{\rm std\; dev}} \right)\^{2}. \]

-   Individual cell contribution is similar to standardized residual.
    Any standardized residual greater than 2 is regarded as significant.

-   So $2^2 = 4$ is treated as a significant contribution to the
    $\chi ^{2}$ statistic.


## Simpson's paradox

Group 1

```{r}
group1 <- matrix(c(80,120,30,80), nrow=2, ncol=2, byrow=T)
group1
chisq.test(group1)
```

Group 2

```{r}
group2 <- matrix(c(20,75,25,20), nrow=2, ncol=2, byrow=T)
group2
chisq.test(group2)
```

After amalgamation of both groups

```{r}
all <- matrix(c(80,120,30,80)+c(20,75,25,20), nrow=2, ncol=2, byrow=T)
all
chisq.test(all)
```

## Permutation test

This test is done maintaining the marginal totals.

```         
Data: Smoking Status vs. Staff  Groupings
```

```{r, echo=FALSE}
col.lbl = c("None","Moderate", "Heavy")
row.lbl = c("Junior employees", "Junior managers", "Secretaries", "Senior employees", "Senior managers")
col1 = c(18, 4, 10, 25, 4)
col2 = c(57, 10, 13, 22, 5)
col3 = c(13, 4, 2, 4, 2)
tabledata = matrix(cbind(col1, col2, col3), 
                   nrow = 5, ncol=3, byrow=F,
                   dimnames = list(row.lbl, col.lbl))

dt <- data.frame(tabledata)
dt$Totals <- rowSums(dt)
dt["Totals",] <- colSums(dt)
library(kableExtra)
library(tidyverse)
kable(dt, caption=" Smoking Status vis-a-vis Staff Groupings") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Permutation test

```{r, echo=TRUE, warning=FALSE}
set.seed(12321)
chisq.test(tabledata, simulate.p.value = TRUE)
```

Regular Chi-square test

```{r, echo=TRUE, warning=FALSE}
chisq.test(tabledata)
```


## Correspondence Analysis 
Correspondence Analysis is an exploratory statistical technique for assessing the interdependence of categorical variables whose data are presented primarily in the form of a two-way table of frequencies


    Data: Smoking Status vs. Staff Groupings

\tiny

```{r, echo=FALSE, warning=FALSE, comment=""}
col.lbl <-  c("None","Moderate", "Heavy")
row.lbl <- c("Junior employees", "Junior managers", "Secretaries", "Senior employees", "Senior managers")
col1 <- c(18, 4, 10, 25, 4)
col2 <- c(57, 10, 13, 22, 5)
col3 <- c(13, 4, 2, 4, 2)
tabledata <- matrix(cbind(col1, col2, col3), 
                   nrow = 5, ncol=3, byrow=F,
                   dimnames = list(row.lbl, col.lbl))
tabledata
chisq.test(tabledata)
```

## Row mass & row profiles

-   row profiles are found dividing the cell counts by the corresponding
    row total

-   row mass is found dividing the row totals by the grand total

```{r, , comment=""}
#tabledata
#formatC(chisq.test(tabledata)$exp)
#chisq.test(tabledata)
#chisq.test(tabledata, simulate.p.value = T, B=5000)
rowtotal <- rowSums(tabledata)
rowprofile <- tabledata/rowtotal
coltotal <- colSums(tabledata)
colprofile <- tabledata/coltotal
rowmass <- coltotal/sum(coltotal)
colmass <- rowtotal/sum(rowtotal)
options(digits=3)
rbind(rowprofile, rowmass)
```

-   Similarly column profiles & column masses can be found

## Graphical display of row profiles

```{r, fig.height=5.5, fig.width=8}
matplot(rowprofile, 
        ylab ="Row Profiles",
        xlab ="Staff Grouping", 
        pch=1:3, col=1,
        xaxt = "n")
axis(1, at = c(1:5), labels = row.lbl)
legend("topright", col.lbl, pch=1:3, col=1, title = "Smoking Status")   
```

-   No clear patterns seen

## Symmetric plots to find subgrouping {.scrollable}

```{r, fig.height=5, fig.width=7}
#| echo: true
#| fig-pos: false
library("FactoMineR")
CA(tabledata) |> summary()
```

## Symmetric plots to find subgrouping

![](img/CA.png)

## Summary

-   Goodness of fit is for testing whether the observed counts are from
    the hypothesised population groups.

    -   For $c$ categories (groups), the test involves $c-1$ df.

-   Contingency Table ($r$ rows and $c$ columns) data are tested for the
    independence.

    -   For $r\times c$ cells the test involves $(r-1)(c-1)$ df
    -   shiny app
        <http://shiny.massey.ac.nz/anhsmith/demos/explore.counts.of.factors/>

-   $\chi ^{2}$ test works well if $E> 5$. Some could be as low as 1.

-   Do correspondence analysis (symmetric plots) when independence is
    rejected.

<!-- ## Exercises -->

<!-- download.file("http://www.massey.ac.nz/~kgovinda/220exer/Chap7moreexamples.R", destfile="Chap7moreexamples.R") -->

<!-- download.file("https://www.massey.ac.nz/~kgovinda/220exer/chapter-7-exercises.html", destfile="chapter-7-exercises.html") -->

<!-- install.packages("remotes") -->

<!-- remotes::install_github("ricompute/ricomisc") -->

<!-- ricomisc::rstudio_viewer("chapter-7-exercises.html", file_path = NULL) -->
